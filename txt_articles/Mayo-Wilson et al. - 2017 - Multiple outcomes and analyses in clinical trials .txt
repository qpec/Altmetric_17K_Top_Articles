 Multiple outcomes and analyses in clinical trials create challenges for
interpretation and research synthesis
Evan Mayo-Wilsona,*, Nicole Fuscoa, Tianjing Lia, Hwanhee Hongb, Joseph K. Cannerc,
Kay Dickersina, for the MUDS investigators
aDepartment of Epidemiology, Johns Hopkins University Bloomberg School of Public Health, 615 North Wolfe Street, Baltimore, MD 21205, USA
bDepartment of Mental Health, Johns Hopkins University Bloomberg School of Public Health, 624 N Broadway, Hampton House, Baltimore, MD 21205, USA
cDepartment of Surgery, Johns Hopkins University School of Medicine, 600 North Wolfe Street, Blalock Building, Baltimore, MD 21287, USA
Accepted 9 May 2017; Published online 18 May 2017
Abstract
Objective: To identify variations in outcomes and results across reports of randomized clinical trials (RCTs).
Study Design and Setting: Eligible RCTs examined gabapentin for neuropathic pain and quetiapine for bipolar depression, reported in
public (e.g., journal articles) and nonpublic (e.g., clinical study reports) sources by 2015. We prespecified outcome domains. From each
source, we collected ‘‘outcomes’’ (i.e., domain, measure, metric, method of aggregation, and time point); ‘‘treatment effect’’ (i.e., outcome
plus the methods of analysis [e.g., how missing data were handled]); and results (i.e., numerical contrasts of treatment and comparison
groups). We assessed whether results included sufficient information for meta-analysis.
Results: We found 21 gabapentin (68 public, 6 nonpublic reports) and seven quetiapine RCTs (46 public, 4 nonpublic reports). For four
(gabapentin) and seven (quetiapine) prespecified outcome domains, RCTs reported 214 and 81 outcomes by varying four elements. RCTs
assessed 605 and 188 treatment effects by varying the analysis of those outcomes. RCTs reported 1,230 and 661 meta-analyzable results,
305 (25%) and 109 (16%) in public reports.
Conclusion: RCTs included hundreds of outcomes and results; a small proportion were in public reports. Trialists and meta-analysts
may cherry-pick what they report from multiple sources of RCT information.
� 2017 The Authors. Published by Elsevier Inc. This is an
open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Keywords: Clinical trials; Systematic reviews; Meta-analysis; Outcomes; Selective outcome reporting
1. Introduction
Although randomized clinical trials (RCTs) are consid-
ered the reference standard for examining effectiveness
and safety of treatments, it is rare that a single RCT provides
sufficient evidence to merit adoption of a treatment for any
given condition. Furthermore, clinicians and others can no
longer stay abreast of rapidly growing knowledge, including
the findings of all RCTs pertinent to their treatment deci-
sions. Accordingly, they look to summaries of knowledge,
such as clinical practice guidelines, that depend in part on ev-
idence syntheses (e.g., systematic reviews, meta-analyses);
evidence syntheses combine information from similar
studies, often focusing on RCTs for treatment decisions.
In many systematic reviews, ‘‘outcomes’’ are not well
defined. Although ‘‘outcomes’’ are often described by a
‘‘name’’ such as ‘‘pain intensity,’’ this name is actually
the ‘‘outcome domain,’’ one of five elements comprising
an outcome [1]. The five elements are as follows: (1)
outcome domain; (2) measure (e.g., McGill Pain Question-
naire, Montgomery �
Asberg Depression Rating Scale); (3)
metric (e.g., value at a time point, change from baseline);
(4) method of aggregation (e.g., mean value for continuous
data, percent with an outcome for categorical data); and (5)
time point at which the assessment was made (e.g., 8 weeks
after starting treatment). Thus, for a single outcome
domain, one RCT may include many defined outcomes
The MUDS investigators include Lorenzo Bertizzolo, Terrie Cowley,
Peter Doshi, Jeffrey Ehmsen, Gillian Gresham, Nan Guo, Jennifer Hay-
thornthwaite, James Heyward, Diana Lock, Jennifer Payne, Lori Rosman,
Elizabeth Stuart, Catalina Suarez-Cuervo, Elizabeth Tolbert, Claire Twose,
and Swaroop Vedula.
Ethics approval: The study received an exemption from the Johns Hop-
kins Bloomberg School of Public Health Institutional Review Board (IRB
no: 00006324).
Funding: Supported by contract ME 1303 5785 from the Patient-Cen-
tered Outcomes Research Institute (PCORI) and a fund established at
Johns Hopkins for scholarly research on reporting biases by Greene
LLP. The funders were not involved in the design or conduct of the study,
article preparation, or the decision to submit the article for publication.
* Corresponding author. Tel.: 443-287-5042.
E-mail address: evan.mayo-wilson@jhu.edu (E. Mayo-Wilson).
http://dx.doi.org/10.1016/j.jclinepi.2017.05.007
0895-4356/� 2017 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/
licenses/by-nc-nd/4.0/).
Journal of Clinical Epidemiology 86 (2017) 39e50
 What is new?
Key findings
� Trials of the same intervention and condition
included hundreds of different outcomes and re-
sults, and much of this information was available
only using nonpublic sources.
What this adds to what was known?
� Multiple outcome definitions and multiple methods
of analysis lead to challenges for interpreting clin-
ical trials, particularly because they create opportu-
nities for cherry-picking by both clinical trialists
and systematic reviewers.
� Variation in outcomes, and incomplete results re-
porting, makes it difficult to compare clinical trials
and to translate knowledge into practice.
What is the implication and what should change
now?
� Clinical trials and systematic reviews should define
their outcomes and methods of analysis completely
and report their results transparently.
� Guidance is needed for using multiple outcomes
and results in systematic reviews.
because different measures, metrics, time points, and
methods of aggregation were used (Fig. 1).
Investigators
performing
evidence
syntheses
usually
prespecify eligibility criteria for including RCTs and
outcome domains that will be examined. It is not unusual
for investigators to find, however, that even when many trials
are eligible for a systematic review, only a few trials can be
combined using meta-analysis [2,3]. Consequently, many tri-
als that are eligible for systematic reviews are not included in
the meta-analyses they contain; those trials thus contribute
little information to the overall conclusions of systematic re-
views. This may occur because the included RCTs did not
assess the same outcome domains because different out-
comes within the same domains could not be combined in
meta-analysis (i.e., one or more elements differed), because
the included RCTs assessed but did not report the same out-
comes, or because RCTs reported the same outcomes but did
not report sufficient statistical information to allow combina-
tion of the numerical results [4,5]. Furthermore, if systematic
reviewers assume that outcomes within RCTs can be used
interchangeably, even if those outcomes are not defined us-
ing all elements, reviewers may be making assumptions that
lead to errors when synthesizing overall results [6,7].
The fact that RCTs may assess multiple outcomes for
the same domain leads to challenges for systematic re-
viewers,
regardless
of
whether
they
conduct
meta-
analyses [8,9]. First, if an RCT reports multiple outcomes,
which outcome should be used to determine whether the
intervention ‘‘works’’? Second, a single RCT might report
different results for the same outcome by using multiple
methods of analyses (e.g., methods for handling missing
data) [10e14]. If there are multiple results for an outcome,
which estimate should the meta-analyst use? Third, even
when it is possible to combine multiple RCTs, synthesized
results (e.g., the combined standardized mean difference
[SMD]) may be difficult to interpret if studies used
different outcome definitions or different methods of anal-
ysis [12]. All of these situations pose challenges to the
proper interpretation of RCTs and evidence syntheses,
and they may lead to innocent errors.
Defining multiple outcomes under the same domain may
also be associated with deliberate efforts (e.g., by trialists or
systematic reviewers) to conceal findings and to mislead
readers. For example, in RCTs that include many outcomes,
trialists might report statistically significant results selec-
tively [14e16]. In systematic reviews, investigators might
cherry-pick results to include in meta-analyses [17,18].
Furthermore, when only some outcomes are reported pub-
licly, it is impossible for the systematic reviewer or other
interpreter of the trial findings to know for sure whether
there has been selective reporting.
Few studies have explored the number of results that
investigators could select to include in meta-analyses
[7,13,19]. We know of no studies that have used both public
and nonpublic data sources for RCTs to quantify the num-
ber of outcomes and results reported across RCTs, the num-
ber of reported outcomes that are defined, or the number of
results that are meta-analyzable.
Fig. 1. The number of outcomes in a trial is a function of the number
of definitions of each of the five elements. In this hypothetical
example, the number of treatment effects for a single trial is the prod-
uct of definitions for each element. In a trial with four outcome do-
mains, introducing two definitions for each of the other elements
will result in 64 unique defined outcomes.
40
E. Mayo-Wilson et al. / Journal of Clinical Epidemiology 86 (2017) 39e50
 2. Objective
Starting with prespecified outcome domains, we sought to
identify all defined outcomes and treatment effects available
from reports of eligible RCTs. We also sought to determine
whether results for those outcomes were meta-analyzable
and whether the information was publicly available.
3. Methods
This study was part of an investigation into multiple re-
ports of RCTs and the effect of disagreement among reports
of RCTs. Inclusion criteria, methods for the study, and re-
sults of the searches have been described elsewhere
[20,21]. Definitions of terms are provided in the text and
summarized in Box 1.
Briefly, we identified multiple reports emanating from
RCTs of (1) gabapentin treatment for neuropathic pain
and (2) quetiapine treatment for bipolar depression. We
did not include individual participant data (IPD) in this sub-
study because information required for our analysis is not
usually specified in IPD (e.g., information about metrics
and methods of analysis might be included in study proto-
cols rather than the IPD data sets). We identified public
(e.g., journal articles, Food and Drug Administration
[FDA] reviews, short reports) and nonpublic reports (clin-
ical study reports [CSRs], CSR-synopses) available for
these trials by 2015. We searched electronic databases for
published and unpublished reports, searched for trial regis-
trations, requested reports and data sets from manufac-
turers, searched the FDA web site, and checked reference
lists of systematic reviews. For gabapentin-neuropathic
pain trials, we hand-searched conference proceedings to
identify abstracts. For quetiapine, we did not conduct hand
searches because of limitations in funding. Two authors ex-
tracted data independently from each report using the open
access Systematic Review Data Repository [23] and recon-
ciled differences through discussion.
From each eligible report, we identified outcomes and
methods of analysis related to outcome domains specified
a priori. All five prespecified outcome domains for
gabapentin-neuropathic pain related to the potential effec-
tiveness of treatment; two of the eight outcome domains
for quetiapine-bipolar depression related to potential safety
(i.e., suicide and weight).
3.1. Identifying outcomes in trial reports
We defined each ‘‘outcome’’ using the five elements
described in the Section 1 and Table 1 [1]: the (1) outcome
domain, (2) measure, (3) metric, (4) method of aggregation;
and (5) time point (e.g., 8 weeks after starting treatment).
When all five elements were defined, we describe an
outcome as ‘‘defined’’; when some but not all elements of
an outcome were defined, we refer to an outcome as ‘‘not
defined’’ (Supplement A at www.jclinepi.com).
3.2. Identifying treatment effects in trial reports
We use the term ‘‘treatment effect’’ to describe the
outcome (when all five elements of an outcome are
described the outcome is ‘‘defined’’) plus the ‘‘method of
analysis.’’ We use the term ‘‘method of analysis’’ to include
the elements: ‘‘analysis population,’’ ‘‘method for handling
missing data,’’ and ‘‘method of adjustment’’ (Table 1)
[26,27]. We use the term ‘‘analysis population’’ to describe
participants eligible for analysis [28]. For example, the ef-
fect of a drug might be assessed for all randomized partic-
ipants or for people who took a minimum dose of the
Box 1. Definitions used in this study for key terms
Term
Definition used in our study
Outcome
An event in a person, used to assess a treatment’s effect [22]. May be defined using
all elements or not defined.
Defined outcome
Includes all five elements of an outcome: (1) outcome domain, (2) specific measure,
(3) specific metric, (4) method of aggregation, and (5) time point. For example,
‘‘proportion of participants with 50% change from baseline to 8 weeks on the
MADRS total score.’’
Not defined outcome
Includes the name of an outcome domain but does not include one or more of the
other four elements; for example, ‘‘symptoms of depression at 8 weeks.’’
Defined treatment effect
A defined outcome plus all three methods of analysis (i.e., the analysis population,
method for handling missing data, and method of adjustment).
Not defined treatment effect
Includes the name of an outcome domain but does not include one or more
elements of an outcome or methods of analysis.
Result
A numerical contrast between a treatment and comparison group (e.g., relative risk,
mean difference).
Meta-analyzable results
A result for which sufficient information was provided to calculate the between
group effect (e.g., a point estimate and a measure of precision).
Unique (outcome, treatment effect, or result)
Defined outcome, treatment effect, or result counted only once, regardless of how
many times it appeared in all reports.
Nonunique (outcome, treatment effect, or result)
Defined outcome, treatment effect, or result counted each time it appears in reports.
Abbreviation: MADRS, Montgomery �
Asberg Depression Rating Scale.
41
E. Mayo-Wilson et al. / Journal of Clinical Epidemiology 86 (2017) 39e50
 assigned treatment, two different analysis populations. We
did not count subgroup analyses (e.g., men vs. women) as
separate analysis populations. We refer to ‘‘method for
handling missing data’’ as the manner in which data for
participants who missed an assessment or dropped out of
the trial completely were considered in the analysis. We
used the term ‘‘method of adjustment’’ to describe whether
each analysis was adjusted or unadjusted for covariates and
the method used. We assumed that the analysis was unad-
justed unless otherwise specified, and when the analysis
was adjusted did not record which specific covariates were
used for adjustment [29].
3.3. Identifying results in trial reports
We use the term ‘‘result’’ to refer to estimates of treat-
ment effects; that is, numerical contrasts between the test
intervention and comparison groups (e.g., relative risk,
mean difference) (Box 1). We extracted results for defined
outcomes and for not defined outcomes (see Box 1 for def-
initions). We refer to ‘‘meta-analyzable results’’ as those re-
sults for which sufficient information was provided to
include them in a mathematical synthesis with other studies
(e.g., a point estimate and a measure of precision for
continuous
results,
numerator
and
denominator
for
Table 1. Information needed to define a treatment effect
Element
Definition used in our study
Example
A. Outcome (adapted from Zarin et al. 2011) [1]
1. Domain
Title [24] or concept [25] to describe one or more
outcomes.
Depression
2. Specific measure
Instrument used to assess the outcome domain,
including the following:
a. Name of the instrument or questionnaire.
Montgomery �
Asberg Depression Rating Scale (MADRS).
b. The total score or the subscales that will be analyzed.
MADRS total score.
3. Specific metric
Unit of measurement (e.g., value at a time point,
change from baseline, time-to-event).
Change from baseline.
4. Method of aggregation
The procedure for estimating the treatment effect,
including the following:
a. If the outcome will be treated as a continuous,
categorical, or time-to-event variable.
Categorical variable.
b. For continuous variables, a measure of central
tendency (e.g., mean value). For categorical and
time-to-event data variables, proportion with an event
and, if relevant, the specific cutoff or categories
compared.
Proportion of participants with �50% decrease.
5. Time point
The period of follow-up, including the following:
a. When outcome measurements will be obtained
b. Which of the outcome measurements will be analyzed
Analyzed at 8 weeks after starting treatment
B. Method of analysis
6. Analysis population
Participants eligible to be included in the analysis.
All participants taking at least one dose of study
medication or placebo.
7. Method for handling
missing data
a. Procedure(s) used to account for participants who
withdrew from the trial, did not complete an
assessment, or otherwise did not provide data.
a. Last observation carried forward.
b. Procedures used for missing data items (e.g.,
questions that were not completed on a
questionnaire).a
b. If at least 80% of the questionnaire is complete at
8 weeks, the average score will be used for the
missing item(s). If the questionnaire is less than 80%
complete, the questionnaire will be treated as
missing.
8. Method of adjustment
Specific analytic procedures used, if any, including the
following:
a. Methods to adjust the data for covariates.
a. Not adjusted.
b. Methods to transform the data (e.g., log
transformed).b
b. Not transformed.
To conduct an analysis, each element of an outcome must be defined. In addition, the analyst must select a method of analysis. The right col-
umn illustrates the definition of an outcome and additional elements to define the treatment effect. This hypothetical example of a defined treat-
ment effect in the ‘‘Example’’ column might be described as follows: ‘‘Depression was measured using the Montgomery �
Asberg Depression Rating
Scale (MADRS) total score. We compared the proportion of participants with �50% decrease between baseline and 8 weeks after starting treat-
ment, adjusting for baseline covariates. If a participant completed at least 80% of the MADRS, we used the average score; if the MADRS was less
than 80% complete, we treated it as missing. All participants taking at least one dose of study medication or placebo were included in the analysis,
and we imputed missing cases using last observation carried forward (LOCF).
a Methods for handling missing items are often not described in journal articles and other public reports, and we did not record this information
in this study.
b We assumed that data were not transformed or adjusted unless otherwise specified. Thus, we recorded that this element was always defined
for the purpose of our study.
42
E. Mayo-Wilson et al. / Journal of Clinical Epidemiology 86 (2017) 39e50
 categorical results). The number of meta-analyzable results
could be larger than the number of treatment effects
because, for multiple contrasts across trials, we expected
to find multiple results corresponding to the same treatment
effect (for a single contrast in a trial, we expected to find
one result corresponding to each treatment effect).
Fig. 2. The number of outcomes observed increased as a consequence of multiple definitions of the elements. The points nearest to the center of
the circle represent the prespecified outcome domains for which we found any information in eligible trials (i.e., four outcome domains for ga-
bapentin and seven outcome domains for quetiapine); these are represented by different colors. The branches represent variations in outcome
definitions associated with those domains (i.e., different measures, metrics, methods of aggregation, or time points). Each point around the outer
edge of the circle thus represents a unique outcome described in one or more of the trial reports we identified. (A) Gabapentin for neuropathic
pain. Specific measure (name of instrument): BPI, Brief Pain Inventory; CGIC, Clinician Global Impression of Change; ECOG, Eastern Cooperative
Oncology Group; GIC, Global Impression of Change; GSS, Global Symptom Score; HADS, Hospital Anxiety and Depression Scale; HRQoLQ, Health
Related Quality of Life Questionnaire; NPS, Neuropathic Pain Scale; Pain (11 pt), pain score (11-point scale); Pain (4 pt), pain score (4-point
scale); PGIC, Patient Global Impression of Change; POMS, Profile of Mood States; SF-36, Short Form 36 item; SFMPQ, Short Form McGill Pain
Questionnaire; Sleep score, sleep score (11-point scale); TOPS, Treatment Outcomes in Pain Survey. Specific measure (total or subscale): % pain
relief, percentage pain relief; Ave. pain, average pain score; Care sat, health care satisfaction; Current pain, current pain score; Fear/Avoid, fear
avoidance; Gen health, general health; Globe intensity, global intensity; Globe unpleas, global unpleasantness; Lower body, lower body functional
limitations; Ment health, mental health; Obj. social, objective family/social disability; Obj. work, objective work disability; Pain sympt, pain symp-
tom; Passive cope, passive coping; Patient sat, patient satisfaction with outcomes; Phys funct, physical functioning; PPI, present pain intensity;
Role emot, role emotional; Role phys, role physical; Social disab, perceived family/social disability; Social funct, social functioning; Solicitous,
solicitous responses; Total pain, total pain experience; Total score, total; Upper body, upper body functional limitations; VAS, visual analog scale;
Work limit, work limitations. Metric: Change, change from baseline; Value, value at a time point. Method of aggregation: % change mean, percent
change mean; % change median, percent change median; 2 decrease, at least 2-point decrease; 3 decrease, at least 3-point decrease; 4
decrease, at least 4-point decrease; M or VM improved, much or very much improved; M or VM worse, much worse or very much worse; Min
or No change, minimally improved or no change; Min improved, minimally improved; Min worse, minimally worse; Mod improved, moderately
improved; Mod or much imp, moderately or much improved; Mod worse, moderately worse; No change, no change (0%); Pain free, totally pain
free; Response (ND), response (not defined); V much improved, very much improved; V much worse, very much worse. (B) Quetiapine for bipolar
depression Specific measure (name of instrument): BMI, body mass index; CGI-I, Clinical Global ImpressioneImprovement; CGI-S, Clinical
Global ImpressioneSeverity; HAM-A, Hamilton Anxiety Rating Scale; HAM-D, Hamilton Depression Rating Scale; MADRS, Montgomery �
Asberg
Depression Rating Scale; MADRS/YMRS, Montgomery �
Asberg Depression Rating Scale & Young Mania Rating Scale; MOS Cog, Medical Outcomes
Study Cognitive Scale; PSQI, Pittsburgh Sleep Quality Assessment; Q-LES-Q, Quality of Life Enjoyment and Satisfaction Questionnaire; QIDS-SR-
16, Quick Inventory of Depressive Symptomology Self-Report 16; SDS, Sheehan Disability Scale; Ideation, Suicidal ideation (self-reported).
Metric: Change, change from baseline; Value, value at a time point. Method of aggregation: 5 10, less than or equal to 10; 5 12, less than
or equal to 12; 5 7, less than or equal to 7; 5 8, less than or equal to 8; 15% increase, at least 15% increase; 25% increase, at least 25%
increase; 30% decrease, at least 30% decrease; 40% decrease, at least 40% decrease; 50% decrease, at least 50% decrease; 60% decrease,
at least 60% decrease; 7% increase, at least 7% increase; 70% decrease, at least 70% decrease; extremely ill, among the most extremely ill; M
or VM improved, much or very much improved; MADRS 5 12 and YMRS 5 12, MADRS less than or equal to 12 and YMRS less than or equal to
12; MADRS 5 12 and YMRS 5 8, MADRS less than or equal to 12 and YMRS less than or equal to 8; Mean % Max, mean percent of the
maximum; Min improved, minimally improved; Min worse, minimally worse; Normal or Borderline, normal, not at all ill or borderline ill; Normal,
not ill, Normal, not at all ill; Proportion, proportion of participants who experienced the event; Remission (ND), remission (not defined); Response
(ND), response (not defined); V Much improved, very much improved; V Much worse, very much worse.
43
E. Mayo-Wilson et al. / Journal of Clinical Epidemiology 86 (2017) 39e50
 3.4. Comparing reports
We counted the number of defined outcomes and the
number of defined treatment effects in each report and
across all reports for each RCT. For each meta-analyzable
result, we recorded the number of participants included in
the analysis, and we compared the number of people that
would be included in the meta-analyses that included (1)
the most participants possible and (2) the fewest partici-
pants possible.
We use the term ‘‘unique’’ when we counted an
outcome, treatment effect, or result only once, regardless
of how many times the outcome appeared across all RCTs
and reports. For example, if two reports included the pain
intensity domain measured using the McGill Pain Ques-
tionnaire and assessed as the mean difference between
groups in their change in pain between baseline and the
8-week assessment, we counted that as one unique defined
outcome. We use the term ‘‘nonunique’’ when we counted
an outcome, treatment effect, or result each time it ap-
peared across reports (e.g., in both a journal article and a
conference abstract about the same trial).
We used Stata 14 (StataCorp, College Station, Texas) and R
3.3.1 (R Core Team, Vienna, Austria) to analyze the results and
D3.js JavaScript Library version 3 (www.D3js.org) to draw
Fig. 2.
4. Results
4.1. Results of the search
As described elsewhere [21], in 2015, we identified 21
RCTs of gabapentin for neuropathic pain (68 public and
6 nonpublic reports) and seven RCTs of quetiapine for bi-
polar depression (46 public and 4 nonpublic reports)
meeting our eligibility criteria (Table 2).
4.2. Trials assessed many outcomes for a few outcome
domains
In the reports we identified, investigators reported 4/5
(gabapentin-neuropathic pain) and 7/8 (quetiapine-bipolar
depression) of our prespecified outcome domains [21].
We identified 214 and 81 unique defined outcomes for
the four and seven prespecified domains that were reported,
respectively; the proliferation of defined outcomes is shown
in Fig. 2, the details of which may be easier to view on
screen rather than in print.
The number of unique defined outcomes differed across
outcome domains. For pain intensity and depression, the
most
common
outcome
domains
for
gabapentin-
neuropathic pain and quetiapine-bipolar depression, respec-
tively, we found 119 and 44 unique defined outcomes. For
Table 2. Outcomes and treatment effects estimated across clinical trial reports
Gabapentin-neuropathic pain (21 RCTs)
Quetiapine-bipolar depression (7 RCTs)
All reports
Public reports
Nonpublic reports
All reports
Public reports
Nonpublic reports
Number of reports
74
68 (92%)
6 (8%)
50
46 (92%)
4 (8%)
Number of outcomes
Prespecified outcome domains
4
4 (100%)
4 (100%)
7
7 (100%)
7 (100%)
Unique outcomes (defined and not
defined)
250
140 (56%)
187 (75%)
107
63 (59%)
66 (62%)
Unique defined outcomes
214
104 (49%)
187 (87%)
81
37 (46%)
65 (80%)
Number of treatment effects
Nonunique treatment effects (defined
and not defined)a
1,746
699 (40%)
1,047 (60%)
955
355 (37%)
600 (63%)
Nonunique defined treatment effects
1,122
234 (21%)
888 (79%)
739
141 (19%)
598 (81%)
Unique defined treatment effects
605
107 (18%)
553 (91%)
188
51 (27%)
159 (85%)
Number of results
Nonunique meta-analyzable resultsa
1,364
342 (25%)
1,022 (75%)
685
113 (16%)
572 (84%)
Unique meta-analyzable results
1,230
305 (25%)
1,017 (83%)
661
109 (16%)
571 (86%)
Unique meta-analyzable results for
which all elements were defined
1,032
192 (19%)
870 (84%)
630
78 (12%)
571 (91%)
A defined outcome includes five elements: domain, measure, metric, method of aggregation, and time point of assessment. A defined treatment
effect includes all items required for a defined outcome, and it also includes the analysis population, method of handling missing data, and method
of analysis. Unique outcomes and treatment effects were counted only once, regardless of how many times they appeared across all trials and re-
ports. Nonunique outcomes and treatment effects were counted each time they appeared across trials and reports. We were able to count the num-
ber of unique and nonunique outcomes and treatment effects if they were defined; we were able to count only the number of nonunique outcomes
and treatment effects if they were not defined. We considered an outcome or treatment effect to have occurred in most trials if it occurred in any
report of at least 11 (gabapentin-neuropathic pain) or 4 (bipolar depression) trials. Public reports 5 journal articles, short reports (conference ab-
stracts, letters to the editor, posters, press releases, reports in trade publications), FDA reviews, trial registrations. Nonpublic reports 5 Clinical
Study Reports (CSRs), CSR-Synopses.
a It was not necessary for an outcome to be defined for a result to be meta-analyzable.
44
E. Mayo-Wilson et al. / Journal of Clinical Epidemiology 86 (2017) 39e50
 the
other
three
gabapentin-neuropathic
pain
and
six
quetiapine-bipolar depression outcome domains, we identi-
fied an average of 32 (range 7e76) and six (range 4e11)
unique defined outcomes (Supplement A at www.jclinepi.
com).
4.3. Trials used multiple methods of analysis
For gabapentin-neuropathic pain and quetiapine-bipolar
depression RCTs, respectively, we identified four and seven
unique analysis populations and five and three unique
methods of handling missing data. For both topics, we
identified adjusted and unadjusted analyses.
Differences in analysis populations and methods of
handling missing data affected the number of people
included in each analysis. Meta-analyses including the
same RCTs could have included 2,424 to 3,239 participants
for the outcome domain ‘‘pain intensity’’ and 840 to 1,721
participants for the outcome domain ‘‘depression’’ [21],
differences of 34% and 105%.
4.4. Defined outcomes were analyzed using multiple
methods
For the prespecified outcome domains, the total number
of treatment effects that were assessed increased with the
addition of each analysis population, method of handling
missing data, and method of adjustment (Table 1). In total,
21 gabapentin-neuropathic pain and 7 quetiapine-bipolar
depression RCTs evaluated 605 and 188 unique defined
treatment effects, respectively, for the seven and four pre-
specified domains that were assessed in these RCTs
(Fig. 3). There was more variation in the assessment and re-
porting of pain intensity and depression compared with
other outcome domains (Supplement B at www.jclinepi.
com).
4.5. Many treatment effects were not described in public
reports
Including all RCTs, public reports described 699/1,746
(40% gabapentin-neuropathic pain) and 355/955 (37%
quetiapine-bipolar depression) of the nonunique treatment
effects available from all reports (Table 2).
For the RCTs in which we had nonpublic reports (six
gabapentin-neuropathic pain and four quetiapine-bipolar
depression RCTs), we found that public reports described
510/1,557 (33%) and 305/905 (34%) of the nonunique
treatment effects available from all reports of these trials.
One gabapentin-neuropathic pain trial had only a nonpublic
report (i.e., there was no associated journal article or other
Fig. 3. Usable and reproducible results in public reports are a small subset of all clinical trial results. The top row represents the number of pre-
specified outcome domains. Subsequent rows show the number of defined outcomes and treatment effects, and annotations to the right of the
flowchart identify sources of variation in outcome definitions and treatment effects. The number of nonreproducible outcomes and treatment ef-
fects likely overestimates the true number of outcomes and treatment effects that were assessed; unless all elements were defined, we could not
tell if two treatment effects were the same or different, and some of these outcomes and treatment effects may have been completely defined in
other sources. Results could have been associated with outcomes that were defined or not defined. Of all results we identified, some were not meta-
analyzable, and some meta-analyzable results were not included in public reports. The final row represents the number of meta-analyzable results
available from public reports. (A) Gabapentin for neuropathic pain. (B) Quetiapine for bipolar depression.
45
E. Mayo-Wilson et al. / Journal of Clinical Epidemiology 86 (2017) 39e50
 public report) and the nonpublic report included 305 treat-
ment effects.
Some metrics and methods of aggregation described in
public reports were not described in nonpublic reports.
For example, one gabapentin-neuropathic pain report [30]
described categories for change in pain intensity (e.g., �4
point change, �3 point change) that were not found in
the corresponding nonpublic reports (Supplement C at
www.jclinepi.com). All measures and time points in public
reports were included in nonpublic sources.
4.6. Reports included insufficient data for meta-analysis
Of all nonunique results in public and nonpublic reports,
1,364/1,746
(78%)
and
685/955
(72%)
were
meta-
analyzable (Table 2).
Of the treatment effects reported publicly, we found a
meta-analyzable result for 342/699 (49% gabapentin-
neuropathic pain) and 113/355 (32% quetiapine-bipolar
depression; Table 2). The most common reasons that results
were not meta-analyzable were the failure to report the de-
nominator for dichotomous outcomes and the failure to
report a measure of precision for continuous outcomes.
4.7. Some meta-analyzable results were not
reproducible
Reports sometimes included meta-analyzable data (e.g.,
mean, standard error, number of participants) but did not
define all elements of the outcome or treatment effect, mak-
ing it impossible for another investigator to reproduce the
results. In particular, methods of analysis were better
defined in nonpublic reports compared with public reports.
At least one element was not defined for 294/1,364 (22%)
(gabapentin-neuropathic pain) and 33/685 (5%) (quetia-
pine-bipolar depression) nonunique meta-analyzable results
(Supplement D at www.jclinepi.com). Nonpublic reports
typically defined these elements completely, and confer-
ence abstracts were the type of report least likely to define
all elements of included outcomes and treatment effects
(Supplement E at www.jclinepi.com).
4.8. Outcomes and treatment effects differed across
trials
Because of different outcome definitions and because of
incomplete reporting, no meta-analysis could have included
all of the eligible RCTs for either gabapentin-neuropathic
pain or quetiapine-bipolar depression [21]. Indeed, few out-
comes appeared in most trials; 2/214 (1%) and 12/81
(15%), respectively, of the unique defined outcomes were
assessed in a majority of RCTs (i.e., at least 11/21
gabapentin-neuropathic
pain
RCTs
and
at
least
4/7
quetiapine-bipolar depression RCTs). Many outcomes ap-
peared in only one trial; 116/214 (54%) and 23/81 (28%),
respectively, were assessed in only one RCT. This was
similar to what we observed for treatment effects, with 0/
605 (0%) and 8/188 (4%), respectively, unique defined
treatment effects assessed in a majority of RCTs while
508/605 (84%) and 64/188 (34%) were assessed in only
one RCT (Table 3).
5. Discussion
We examined two commonly prescribed drugs recom-
mended in clinical practice guidelines, gabapentin for neuro-
pathic pain [31] and quetiapine for bipolar depression [32].
These are among a small number of drugs and indications
for which nonpublic sources are available; our findings were
very similar for both cases and they were consistent with
previous studies, suggesting that the areas of concern we
identified are applicable to the wider situation for clinical tri-
als (Box 2). We found multiple reports of RCTs, some public
and some nonpublic [21]. Within these reports, we found
multiple outcomes reported for each trial and across all trials
(Table 3). The variety of outcome definitions and methods of
analysis led to the large number of results reported for these
RCTs. Many outcomes were not defined, even for results that
could be included in meta-analyses (Table 2). Because we
restricted our study to prespecified outcome domains, we re-
corded only one time point within each prespecified window,
and we did not count, subgroup populations as separate anal-
ysis populations, our findings are likely to understate the to-
tal number of results for these trials. The fact that many
results exist for every research question creates important
challenges for interpreting the findings of RCTs and for ev-
idence synthesis.
Although we found an excessive number of outcomes and
treatment effects in these reports, much important informa-
tion was not publicly available [33]. Fewer than half of the
unique outcomes appeared in public reports (Table 2). Even
when information was publicly available, many results were
not meta-analyzable, and thus the information could not
inform practice [34]. Furthermore, some meta-analyzable re-
sults were nonreproducible [35] because there was insuffi-
cient information to determine which outcome had been
assessed or which methods of analyses had been used. For
example, it was often unclear how the authors handled
missing data. Systematic reviewers who use outcomes that
are not defined might naively compare dissimilar informa-
tion and reach incorrect conclusions. This could be a partic-
ular problem for meta-analyses that compare different
measures on a uniform scale (e.g., risk ratio, SMD) [7].
For this reason, statistical methods that allow the combina-
tion of results across trials with disparate outcomes might
also disguise important differences among trials.
In addition to the problems we identified within trials, we
found that differences across trials make it very difficult to
compare RCTs (e.g., in systematic reviews and clinical
guidelines), and these differences pose important obstacles
to knowledge translation. Although we specified only a
few outcome domains, RCTs assessed hundreds of different
treatment effects within them [36]. Previous authors have
46
E. Mayo-Wilson et al. / Journal of Clinical Epidemiology 86 (2017) 39e50
 argued that core outcome sets, which list the minimum out-
comes to assess in research, could improve the comparability
of trials [37,38] and improve the synthesis of trials for knowl-
edge translation [39]. Although we agree, our results also
show that efforts to develop core outcome sets might have lit-
tle impact unless they define outcomes completely. That is,
eligible trials assessed many of the same domains, yet no
defined outcome was reported in all trials of gabapentin-
neuropathic pain and only one defined outcome appeared
in all trials of quetiapine-bipolar depression (Table 3).
Worse, few outcomes for gabapentin-neuropathic pain and
quetiapine-bipolar depression appeared in most trials; conse-
quently, most meta-analyses could have included only a
small proportion of the eligible RCTs and only a small pro-
portion of the outcomes they assessed (Table 3). Of all out-
comes, almost half were reported in exactly one trial; these
outcomes could not be combined with the same outcome
from any other trial except by using statistical procedures
that obscure differences in measurement (e.g., SMD).
Considering differences in analyses, the situation is even
worse; 1% of unique treatment effects appeared in most tri-
als, whereas a majority appeared in exactly one trial. In these
ways, our findings indicate that the state of evidence is the
opposite of what would be best for knowledge translation
and for patient care; that is, it would be desirable for all trials
to include a few common outcomes that are defined and
analyzed in the same way.
Understanding the plethora of outcomes in RCTs could
identify ways to improve trials and evidence synthesis. On
one hand, it is efficient to collect multiple outcomes, and it
is good practice for trialists to conduct sensitivity analyses
to evaluate whether their findings are robust under different
assumptions [40,41]. On the other hand, multiple outcomes
and analyses lead to different results and, thus, allow tria-
lists [15,16] and systematic reviewers to cherry-pick
[17,19,21,38]. We are concerned that the sources of varia-
tion we identified are not always described in protocols
and registrations for trials [42,43] or systematic reviews.
For example, protocols for systematic reviews usually
specify the outcome domains and the time points of interest
[24]; however, few define their methods for handling mul-
tiple measures or multiple analyses [7,13,38]. For these rea-
sons, independent investigators have been unable to
reproduce the data selected for systematic reviews or to
replicate the meta-analyses included in reviews [44]. At a
minimum, systematic reviewers should expect clinical trials
to include many definitions of outcomes within the domains
of interest, and reviewers should prespecify plans for
handling many different outcomes and results across multi-
ple reports of eligible trials.
Our study adds to previous research by using both public
and nonpublic reports [7,13]. Because there is strong evi-
dence that trialists purposively select outcomes to include
Table 3. Occurrence of treatment effects across clinical trial reports
Gabapentin-neuropathic
pain (21 RCTs)
Quetiapine-bipolar
depression (7 RCTs)
Outcomes
Mean unique outcomes per trial (range)
26 (0e95)
27 (6e65)
Unique outcomes occurring in all trials
0
1
Unique outcomes occurring in exactly one trial
116
23
Unique outcomes occurring in the majority of trialsa
2
12
Treatment effects
Mean unique treatment effects per trial (range)
38 (0e305)
48 (0e148)
Unique treatment effects occurring in all trials
0
0
Unique treatment effects occurring in exactly one trial
508
64
Unique treatment effects occurring in the majority of trialsa
0
8
Results
Mean unique meta-analyzable results per trial (range)
59 (0e308)
94 (0e296)
Number of participants who could be included in the analysis by
selecting one result from each trial (range)b
2,424e3,239
840e1,721
a We considered an outcome or treatment effect to have appeared in most trials if it was present in any report of at least 11 of the 21
gabapentin-neuropathic pain RCTs or at least 4 of 7 quetiapine-bipolar depression RCTs.
b For gabapentin-neuropathic pain, we used the most common domain, ‘‘pain intensity.’’ For quetiapine-bipolar depression, we used the most
common domain, ‘‘depression.’’
Box 2. Key findings and implications of this study
Key findings from our study
Many reports are available for individual RCTs
Some RCT reports are public, but many are nonpublic
Multiple outcomes are described across RCT, even within 11
prespecified outcome domains
Some outcomes are defined in RCT reports, and some are not defined
Many treatment effects are described in individual trials and across all
trials
Many results are described in individual trials
Not all results are meta-analyzable
Nonpublic reports include more information than public reports about
outcomes, treatment effects, and results
Implications of our findings
Many reports do not include enough information to reproduce RCT
results
Clinical decision making may not be well informed
Clinical decision making may not be accurately informed
Much of the available research may be wasted
Abbreviation: RCT, randomized clinical trial.
47
E. Mayo-Wilson et al. / Journal of Clinical Epidemiology 86 (2017) 39e50
 in public reports [15,16], it is not surprising that previous
studies limited to public reports suggest that systematic re-
views are not affected by which results are selected for
analysis [38]. By comparison, our study suggests that sys-
tematic reviewers using nonpublic reports have consider-
able opportunities for cherry-picking and that meta-
analyses that include nonpublic sources may be affected
by reviewers’ selection of outcomes [21].
In response to growing mandates for ‘‘open science’’
[45e47], trialists may begin to share previously nonpublic
reports and to provide access to IPD. Consequently, it is
increasingly important for systematic reviewers to include
plans in their protocols for handling the sources of variation
we identified.
Our study focused on two research questions, and
eligible RCTs were reported between 1997 and 2013 (gaba-
pentin-neuropathic pain RCTs) and 2003 and 2014 (quetia-
pine-bipolar depression RCTs). Although the included
trials might not reflect current practice, we are not aware
of any evidence that suggests the number of outcomes
and analyses have decreased over time. Indeed, the opposite
might be true. We had access to nonpublic reports for only
one-third of RCTs, so the total number of outcomes and
treatment effects in these RCTs might have been greater
than our findings suggest.
In the short term, adherence to existing guidance could
improve current practice. Journals should adopt standards
that promote transparency in all the research they publish
[48]. Trialists should register their studies [49e51] and
consult the EQUATOR Network [52] for relevant report-
ing guidelines for protocols [28] and research reports
[26]. Systematic reviewers should use reporting guidelines
when publishing protocols [53] and final reports using
aggregate data [54] or IPD [55]. Our results demonstrate
that such efforts could reduce opportunities for cherry-
picking. When used systematically, our findings support
the theory that nonpublic sources may improve the accu-
racy and quality of evidence syntheses [56]. However,
our results also suggest that nonpublic data could increase
burden on reviewers and introduce new opportunities for
cherry-picking. Additional guidance for using multiple
outcomes and analyses from multiple reports of clinical
trials is needed.
Acknowledgments
The authors used the open access database, Systematic
Review Data Repository (SRDR), and are grateful to Jens
Jap, Bryant Smith, and Joseph Lau for making this possible
without charge and for the support they provided. The au-
thors are grateful to �
Elise Diard (Centre de Recherche
�
Epid�
emiologie et Statistique Sorbonne Paris Cit�
e) for
contributing substantially to the figures.
Authors’ contributions: Study conception and design:
The study design was first described in the application to
the Patient-Centered Outcomes Research Institute (PCORI)
in 2013 (Kay Dickersin, principal investigator), to which
Peter Doshi, Tianjing Li, and Swaroop Vedula contributed.
E.M.-W. drafted the protocol with contributions from other
authors. Acquisition of data: Lori Rosman and Claire Tay-
lor designed and ran the electronic searches. Terrie Cowley,
Kay Dickersin, Nicole Fusco, Gillian Gresham, Jennifer
Haythornthwaite, James Heyward, Tianjing Li, Diana
Lock, Evan Mayo-Wilson, Jennifer Payne, and Elizabeth
Tolbert contributed to drafting and finalizing the data
extraction forms. Kay Dickersin, Nicole Fusco, Gillian
Gresham, James Heyward, Susan Hutfless, Tianjing Li,
Evan Mayo-Wilson, and Swaroop Vedula screened studies
for inclusion. Lorenzo Bertizzolo, Joseph K. Canner, Jef-
frey Ehmsen, Nicole Fusco, Gillian Gresham, James Hey-
ward, Diana Lock, Evan Mayo-Wilson, and Catalina
Suarez-Cuervo extracted the data. Analysis and interpreta-
tion of data: Joseph K. Canner, Nicole Fusco, Hwanhee
Hong, and Evan Mayo-Wilson managed the data. Joseph
K. Canner, Nicole Fusco, and Hwanhee Hong analyzed
the data. Joseph K. Canner, Kay Dickersin, Nicole Fusco,
Hwanhee Hong, Tianjing Li, and Evan Mayo-Wilson
contributed to the interpretation and data presentation.
Drafting of article: Evan Mayo-Wilson wrote the first draft,
with Kay Dickersin, Nicole Fusco, Tianjing Li, and Evan
Mayo-Wilson providing subsequent revisions. Joseph K.
Canner drew the figures. Critical revision: All authors re-
viewed, provided critical revisions, and approved the article
for publication.
E.M.-W. is the guarantor. All authors, external and inter-
nal, had full access to all of the data (including statistical re-
ports and tables) in the study and can take responsibility for
the integrity of the data and the accuracy of the data analysis.
Supplementary data
Supplementary data related to this article can be found at
http://dx.doi.org/10.1016/j.jclinepi.2017.05.007.
References
[1] Zarin DA, Tse T, Williams RJ, Califf RM, Ide NC. The Clinical-
Trials.gov results databaseeupdate and key issues. N Engl J Med
2011;364:852e60.
[2] Davey J, Turner RM, Clarke MJ, Higgins JP. Characteristics of
meta-analyses and their component studies in the Cochrane Data-
base of Systematic Reviews: a cross-sectional, descriptive analysis.
BMC Med Res Methodol 2011;11:160.
[3] Kirkham JJ, Dwan KM, Altman DG, Gamble C, Dodd S, Smyth R,
et al. The impact of outcome reporting bias in randomised
controlled trials on a cohort of systematic reviews. BMJ 2010;
340:c365.
[4] Dwan K, Altman DG, Cresswell L, Blundell M, Gamble CL,
Williamson PR. Comparison of protocols and registry entries to
published reports for randomised controlled trials. Cochrane Data-
base Syst Rev 2011;MR000031.
48
E. Mayo-Wilson et al. / Journal of Clinical Epidemiology 86 (2017) 39e50
 [5] Furukawa TA, Watanabe N, Omori IM, Montori VM, Guyatt GH.
Association between unreported outcomes and effect size estimates
in Cochrane meta-analyses. Jama 2007;297:468e70.
[6] Moayyedi P, Deeks J, Talley NJ, Delaney B, Forman D. An update
of the Cochrane systematic review of Helicobacter pylori eradica-
tion therapy in nonulcer dyspepsia: resolving the discrepancy be-
tween
systematic
reviews.
Am
J
Gastroenterol
2003;98(12):
2621e6.
[7] Tendal B, Nuesch E, Higgins JP, Juni P, Gotzsche PC. Multiplicity
of data in trial reports and the reliability of meta-analyses: empirical
study. Bmj 2011;343:d4829.
[8] Schulz KF, Grimes DA. Multiplicity in randomised trials I: end-
points and treatments. Lancet 2005;365:1591e5.
[9] Gyawali B, Prasad V. Same data; different interpretations. J Clin
Oncol 2016;. http://dx.doi.org/10.1200/JCO.2016.68.2021.
[10] Wood AM, White IR, Thompson SG. Are missing outcome data
adequately handled? A review of published randomized controlled
trials in major medical journals. Clin Trials 2004;1:368e76.
[11] Chan AW, Hrobjartsson A, Jorgensen KJ, Gotzsche PC, Altman DG.
Discrepancies in sample size calculations and data analyses reported
in randomised trials: comparison of publications with protocols.
Bmj 2008;337:a2299.
[12] Vedula SS, Li T, Dickersin K. Differences in reporting of analyses in
internal company documents versus published trial reports: compar-
isons in industry-sponsored trials in off-label uses of gabapentin.
Plos Med 2013;10(1):e1001378.
[13] Page MJ, McKenzie JE, Chau M, Green SE, Forbes A. Methods to
select results to include in meta-analyses deserve more consider-
ation in systematic reviews. J Clin Epidemiol 2015;68:1282e91.
[14] Dwan K, Altman DG, Clarke M, Gamble C, Higgins JP, Sterne JA,
et al. Evidence for the selective reporting of analyses and discrep-
ancies in clinical trials: a systematic review of cohort studies of clin-
ical trials. Plos Med 2014;11(6):e1001666.
[15] Dwan K, Gamble C, Williamson PR, Kirkham JJ. Reporting Bias
Group. Systematic review of the empirical evidence of study publi-
cation bias and outcome reporting bias - an updated review. PLoS
One 2013;8:e66844.
[16] Chan AW, Hrobjartsson A, Haahr MT, Gotzsche PC, Altman DG.
Empirical evidence for selective reporting of outcomes in random-
ized trials: comparison of protocols to published articles. JAMA
2004;291:2457e65.
[17] Chalmers TC, Frank CS, Reitman D. Minimizing the three stages of
publication bias. JAMA 1990;263:1392e5.
[18] Page MJ, McKenzie JE, Forbes A. Many scenarios exist for selec-
tive inclusion and reporting of results in randomized trials and sys-
tematic reviews. J Clin Epidemiol 2013;66:524e37.
[19] Page MJ, McKenzie JE, Kirkham J, Dwan K, Kramer S, Green S,
et al. Bias due to selective inclusion and reporting of outcomes
and analyses in systematic reviews of randomised trials of health-
care interventions. Cochrane Database Syst Rev 2014;MR000035.
[20] Mayo-Wilson E, Hutfless S, Li T, Gresham G, Fusco N, Ehmsen J,
et al. Integrating multiple data sources (MUDS) for meta-analysis to
improve patient-centered outcomes research: a protocol for a sys-
tematic review. Syst Rev 2015;4:143.
[21] Mayo-Wilson E, Li T, Fusco N, Bertizzolo L, Canner J, Cowley T
et al. Disagreements across multiple reports of clinical trials: impli-
cations for interpretation of trial quality and results. Under Rev.
[22] Meinert CL. Clinical Trials Dictionary: Terminology and Usage
Recommendations. 2nd ed. Hoboken, NJ: John Wiley & Sons; 2012.
[23] Li T, Vedula SS, Hadar N, Parkin C, Lau J, Dickersin K. Innovations
in data collection, management, and archiving for systematic re-
views. Ann Intern Med 2015;162:287e94.
[24] Saldanha IJ, Dickersin K, Wang X, Li T. Outcomes in Cochrane sys-
tematic reviews addressing four common eye conditions: an evalua-
tion of completeness and comparability. PLoS One 2014;9:e109400.
[25] Shamseer
L,
Moher
D,
Clarke
M,
Ghersi
D,
Liberati
A,
Petticrew M, et al. Preferred reporting items for systematic review
and meta-analysis protocols (PRISMA-P) 2015: elaboration and
explanation. BMJ 2015;349:g7647.
[26] Schulz KF, Altman DG, Moher D, Group C. CONSORT 2010 state-
ment: updated guidelines for reporting parallel group randomised
trials. Plos Med 2010;7(3):e1000251.
[27] Chan AW, Tetzlaff JM, Altman DG, Dickersin K, Moher D. SPIRIT
2013: new guidance for content of clinical trial protocols. Lancet
2013;381:91e2.
[28] Chan AW, Tetzlaff JM, Altman DG, Laupacis A, Gotzsche PC,
Krleza-Jeric K, et al. SPIRIT 2013 statement: defining standard pro-
tocol items for clinical trials. Ann Intern Med 2013;158:200e7.
[29] Patel CJ, Burford B, Ioannidis JP. Assessment of vibration of effects
due to model specification can demonstrate the instability of obser-
vational associations. J Clin Epidemiol 2015;68:1046e58.
[30] Backonja M. Gabapentin for painful diabetic neuropathy (in reply).
JAMA 1999;282:133e4.
[31] Kendall T, Morriss R, Mayo-Wilson E, Marcus E. Guideline devel-
opment group of the National Institute for Health and Care Excel-
lence. assessment and management of bipolar disorder: summary
of updated NICE guidance. BMJ 2014;349:g5673.
[32] Dworkin RH, O’Connor AB, Audette J, Baron R, Gourlay GK,
Haanpaa ML, et al. Recommendations for the pharmacological
management of neuropathic pain: an overview and literature update.
Mayo Clin Proc 2010;85(3 Suppl):S3e14.
[33] Chan
AW,
Song
F,
Vickers
A,
Jefferson
T,
Dickersin
K,
Gotzsche PC, et al. Increasing value and reducing waste: addressing
inaccessible research. Lancet 2014;383:257e66.
[34] Glasziou P, Altman DG, Bossuyt P, Boutron I, Clarke M, Julious S,
et al. Reducing waste from incomplete or unusable reports of
biomedical research. Lancet 2014;383:267e76.
[35] Goodman S, Fanelli D, Ioannidis JP. What does research reproduc-
ibility mean? Sci Trans Med 2016;8(341):341ps12.
[36] Kaufman J, Ryan R, Bosch-Capblanch X, Cartier Y, Cliff J,
Glenton C, et al. Outcomes mapping study for childhood vaccina-
tion communication: too few concepts were measured in too many
ways. J Clin Epidemiol 2016;72:33e44.
[37] Williamson P, Clarke M. The COMET (Core outcome measures in
effectiveness trials) initiative: its Role in improving Cochrane re-
views. Cochrane Database Syst Rev 2012;ED000041.
[38] Page MJ, Forbes A, Chau M, Green SE, McKenzie JE. Investigation
of bias in meta-analyses due to selective inclusion of trial effect es-
timates: empirical study. BMJ Open 2016;6(4):e011863.
[39] Saldanha IJ, Li T, Yang C, Owczarzak J, Williamson PR,
Dickersin K. Clinical trials and systematic reviews addressing
similar interventions for the same condition do not consider similar
outcomes to be important: a case study in HIV/AIDS. J Clin Epide-
miol 2017;84:85e94.
[40] Altman DG. Missing outcomes in randomized trials: addressing the
dilemma. Open Med 2009;3(2):e51e3.
[41] Li T, Hutfless S, Scharfstein D, Daniels M, Hogan J, Little R, et al.
Standards should be applied in the prevention and handling of
missing data for patient-centered outcomes research: a systematic
review and expert consensus. J Clin Epidemiol 2014;67:15e32.
[42] Mathieu S, Boutron I, Moher D, Altman DG, Ravaud P. Comparison
of registered and published primary outcomes in randomized
controlled trials. JAMA 2009;302:977e84.
[43] Perlmutter A, Tran VT, Dechartres A, Ravaud P. Comparison of pri-
mary outcomes in protocols, public clinical-trial registries and pub-
lications: the example of oncology trials. Ann Oncol 2017;28:
688e95.
[44] Tendal B, Higgins JP, Juni P, Hrobjartsson A, Trelle S, Nuesch E, et al.
Disagreements in meta-analyses using outcomes measured on contin-
uous orratingscales:observeragreement study. BMJ 2009;339:b3128.
[45] Taichman DB, Backus J, Baethge C, Bauchner H, de Leeuw PW,
Drazen JM, et al. Sharing clinical trial data: a proposal from the in-
ternational committee of medical journal editors. PLoS Med 2016;
13(1):e1001950.
49
E. Mayo-Wilson et al. / Journal of Clinical Epidemiology 86 (2017) 39e50
 [46] Moorthy VS, Karam G, Vannice KS, Kieny MP. Rationale for WHO’s
new position calling for prompt reporting and public disclosure of in-
terventional clinical trial results. PLoS Med 2015;12(4):e1001819.
[47] Doshi P, Dickersin K, Healy D, Vedula SS, Jefferson T. Restoring
invisible and abandoned trials: a call for people to publish the find-
ings. BMJ 2013;346:f2865.
[48] Nosek BA, Alter G, Banks GC, Borsboom D, Bowman SD,
Breckler SJ, et al. Scientific standards. Promoting an open research
culture. Science 2015;348:1422e5.
[49] Weber WE, Merino JG, Loder E. Trial registration 10 years on. BMJ
2015;351:h3572.
[50] Zarin DA, Tse T, Williams RJ, Carr S. Trial reporting in
ClinicalTrials.govdthe final rule. N Engl J Med 2016;375:
1998e2004.
[51] Hudson KL, Lauer MS, Collins FS. Toward a new era of trust and
transparency in clinical trials. JAMA 2016;316:1353e4.
[52] Simera I, Altman DG, Moher D, Schulz KF, Hoey J. Guidelines for
reporting health research: the EQUATOR network’s survey of
guideline authors. PLoS Med 2008;5(6):e139.
[53] Moher D, Shamseer L, Clarke M, Ghersi D, Liberati A, Petticrew M,
et al. Preferred reporting items for systematic review and meta-
analysis protocols (PRISMA-P) 2015 statement. Syst Rev 2015;4:1.
[54] Moher D, Liberati A, Tetzlaff J, Altman DG. Preferred reporting
items for systematic reviews and meta-analyses: the PRISMA state-
ment. Plos Med 2009;6(7):e1000097.
[55] Stewart LA, Clarke M, Rovers M, Riley RD, Simmonds M,
Stewart G, et al. Preferred reporting items for systematic review
and meta-analyses of individual participant data: the PRISMA-
IPD Statement. JAMA 2015;313:1657e65.
[56] Doshi P, Jefferson T, Del Mar C. The imperative to share clinical
study reports: recommendations from the Tamiflu experience. PLoS
Med 2012;9(4):e1001201.
50
E. Mayo-Wilson et al. / Journal of Clinical Epidemiology 86 (2017) 39e50
