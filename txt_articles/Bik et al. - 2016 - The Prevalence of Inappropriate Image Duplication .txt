 The Prevalence of Inappropriate Image Duplication in Biomedical
Research Publications
Elisabeth M. Bik,a Arturo Casadevall,b,c Ferric C. Fangd
Department of Medicine, Division of Infectious Diseases, Stanford School of Medicine, Stanford, California, USAa; Department of Molecular Microbiology and
Immunology, Johns Hopkins Bloomberg School of Public Health, Baltimore, Maryland, USAb; Department of Medicine, Johns Hopkins School of Medicine, Baltimore,
Maryland, USAc; Departments of Laboratory Medicine and Microbiology, University of Washington School of Medicine, Seattle, Washington, USAd
ABSTRACT
Inaccurate data in scientific papers can result from honest error or intentional falsification. This study attempted to
determine the percentage of published papers that contain inappropriate image duplication, a specific type of inaccurate data.
The images from a total of 20,621 papers published in 40 scientific journals from 1995 to 2014 were visually screened. Overall,
3.8% of published papers contained problematic figures, with at least half exhibiting features suggestive of deliberate manipula-
tion. The prevalence of papers with problematic images has risen markedly during the past decade. Additional papers written by
authors of papers with problematic images had an increased likelihood of containing problematic images as well. As this analysis
focused only on one type of data, it is likely that the actual prevalence of inaccurate data in the published literature is higher. The
marked variation in the frequency of problematic images among journals suggests that journal practices, such as prepublication
image screening, influence the quality of the scientific literature.
I
naccuracies in scientific papers have many causes. Some result
from honest mistakes, such as incorrect calculations, use of the
wrong reagent, or improper methodology (1). Others are inten-
tional and constitute research misconduct, including situations in
which data are altered, omitted, manufactured, or misrepresented
in a way that fits a desired outcome. The prevalence rates of honest
error and misconduct in the scientific literature are unknown.
One review estimated the overall frequency of serious research
misconduct, including plagiarism, to be 1% (2). A meta-analysis
by Fanelli, combining the results of 18 published surveys, found
that 1.9% of researchers have admitted to modification, falsifica-
tion, or fabrication of data (3).
There is also little firm information on temporal trends regard-
ing the prevalence of errors and misconduct. Research errors and
misconduct have probably always existed. Even scientific lumi-
naries such as Darwin, Mendel, and Pasteur were accused of ma-
nipulating or misreporting their data (4, 5). However, the percep-
tion of error and misconduct in science has been recently
magnified by high-profile cases and a sharp rise in the number of
retracted manuscripts (6). In recent years, retractions have in-
creased at a rate that is disproportionately greater than the growth
of the scientific literature (7). Although this could be interpreted
as an increase in problematic papers, the actual causes may be
more complex and could include a greater inclination by journals
and authors to retract flawed work (7). Retractions are a poor
indicator of error, because most retractions result from miscon-
duct (8), and many erroneous studies are never retracted (1). In
fact, only a very small fraction of the scientific literature has been
retracted. As of May 2016, the PubMed bibliographic database
listed 4,160 retracted publications among more than 24 million
articles (0.017%).
Concerns about misconduct have been accompanied by in-
creasing concerns about the reproducibility of the scientific liter-
ature. An analysis of 53 landmark papers in oncology reported
that only 6 could be reproduced (9), and other pharmaceutical
industry scientists have also reported low rates of reproducibility
of published findings, which in some cases led to the termination
of drug development projects (10). In the field of psychology, less
than half of experimental and correlational studies are reportedly
reproducible (11). Inaccurate data can result in societal injury. For
example, a now-retracted study associating measles vaccination
with autism continues to resonate and may be contributing to low
vaccination rates (12). Corrosion of the literature, whether by er-
ror or misconduct, may also impede the progress of science and
medicine. For example, false leads may be contributing to increas-
ing disparities between scientific investment and measurable out-
comes, such as the discovery of new pharmacological agents (13).
In this study, we sought to estimate the prevalence of a specific
type of inaccurate data that can be readily observed in the pub-
lished literature, namely, inappropriate image duplication. The
results demonstrate that problematic images are disturbingly
common in the biomedical literature and may be found in ap-
proximately 1 out of every 25 published articles containing pho-
tographic image data, in particular Western blotting images.
Papers containing inappropriately duplicated images. A
total of 20,621 research papers containing the search term “West-
ern blot” from 40 different journals and 14 publishers were exam-
ined for inappropriate duplications of photographic images, with
or without repositioning or evidence of alteration (see Table S1 in
the supplemental material). Of these, 8,138 (39.8%) were pub-
lished by a single journal (PLoS One) in 2013 and 2014; the other
12,483 (60.5%) papers were published in 39 journals spanning the
years 1995 to 2014 (Fig. 1). Overall, 782 (3.8%) of these papers
were found to include at least one figure containing inappropriate
duplications.
Published 7 June 2016
Citation Bik EM, Casadevall A, Fang FC. 2016. The prevalence of inappropriate image
duplication in biomedical research publications. mBio 7(3):e00809-16.
doi:10.1128/mBio.00809-16.
Editor L. David Sibley, Washington University School of Medicine
Copyright © 2016 Bik et al. This is an open-access article distributed under the terms of
the Creative Commons Attribution 4.0 International license.
Address correspondence to Elisabeth M. Bik, eliesbik@stanford.edu.
A.C. and F.C.F. contributed equally to this work
PERSPECTIVE
crossmark
May/June 2016
Volume 7
Issue 3
e00809-16
®
mbio.asm.org
1
 on June 4, 2019 by guest
http://mbio.asm.org/
Downloaded from 
 Classification of inappropriately duplicated images. Prob-
lematic images were classified into three major categories: simple
duplications, duplications with repositioning, and duplications
with alteration.
● Category I: simple duplications. Figures containing two or
more identical panels, either within the same figure or be-
tween different figures within the same paper, purporting to
represent different experimental conditions, were classified
as simple duplications. The most common examples in this
category were beta-actin loading controls that were used
multiple times to represent different experiments or identi-
cal microscopy images purporting to be obtained from dif-
ferent experiments. For papers containing such figures, the
methods and results were reviewed to establish that the du-
plicated figures were indeed reused for different experi-
ments. The reuse of loading controls in different figures ob-
tained from the same experiment was not considered to be a
problem. Examples of simple duplication are shown in
Fig. 2.
● Category II: duplication with repositioning. This category
included microscopic or blot images with a clear region of
overlap, where one image had been shifted, rotated, or re-
versed with respect to the other. Figure 3 shows examples of
duplicated figures with repositioning.
● Category III: duplication with alteration. This category con-
sisted of images that were altered with complete or partial
duplication of lanes, bands, or groups of cells, sometimes
with rotation or reversal with respect to each other, within
the same image panel or between panels or figures. This
category also included figures containing evidence of (i)
“stamping,” in which a defined area was duplicated multiple
times within the same image, (ii) “patching,” in which part
of an image was obscured by a rectangular area with a dif-
ferent background, and (iii) fluorescence-activated cell sort-
ing (FACS) images sharing conserved regions and other re-
gions in which some data points had been added or
0 
1000 
2000 
3000 
4000 
5000 
6000 
7000 
8000 
1995 
1996 
1997 
1998 
1999 
2000 
2001 
2002 
2003 
2004 
2005 
2006 
2007 
2008 
2009 
2010 
2011 
2012 
2013 
2014 
Number of papers investigated 
Year 
 Other journals
 PLOS ONE 
FIG 1 Publications investigated by year of publication. The majority of screened papers were published in 2013 and 2014, due to the large proportion of PLoS
One papers (39.5%) in the data set. The lowest number of papers (n � 151) screened in this study was published in 1996.
FIG 2
Examples of simple duplications (category I). (A) The beta-actin
control panel in the top left is identical to the panel in the bottom right (green
boxes), although each panel represents a different experimental condition.
This figure appeared in reference 27 and was corrected in reference 28. (Re-
produced with permission from the publisher.) (B) The panels shown here
were derived from two different figures within the same paper (reference 29;
corrected in reference 30). Two of the top panels appear identical to two of the
bottom panels, but they represent different experimental conditions (red and
blue boxes). (Figure reproduced under the Creative Commons [CC BY] li-
cense.) All duplications might have been caused by honest errors during as-
sembly of the figures.
Perspective
2
®
mbio.asm.org
May/June 2016
Volume 7
Issue 3
e00809-16
 on June 4, 2019 by guest
http://mbio.asm.org/
Downloaded from 
 removed. Examples of duplicated images with alteration are
shown in Fig. 4.
Two additional types of image modification were not scored as
problematic, although they may represent questionable research
practices by current standards and would not be accepted by cer-
tain journals (i.e., Journal of Cell Biology):
● Cuts. Abrupt vertical changes in the background signal be-
tween adjacent lanes in a blot or gel suggest that the lanes
were not next to each other in the original gel. These splices
are of potential concern but do not necessarily indicate in-
accurate data representation.
● Beautification. Part of the background of a blot or gel where
no band of interest is expected may show signs of patching,
perhaps to remove a smudge or stain. This is not considered
to represent best practice according to contemporary guide-
lines for data presentation (14) but does not necessarily in-
dicate inaccurate data representation.
Although researcher intent could not be definitively deter-
mined in this study, the three categories of duplicated images were
felt to have different implications with regard to the likelihood of
scientific misconduct. Category I (simple duplication) images are
most likely to result from honest errors, in which an author(s)
intended to insert two similar images but mistakenly inserted the
same image twice. Alternatively, simple duplications may result
from misconduct, for example, if an author(s) intentionally recy-
cled a control panel from a different experiment because the actual
control was not performed. Category II (duplication with reposi-
tioning) and category III (duplication with alteration) may be
somewhat more likely to result from misconduct, as conscious
effort would be required for these actions.
In our study, a paper was classified as containing an inappro-
priate duplication when at least one category I, II, or III problem
was identified. Papers were classified according to the highest cat-
egory of duplicated image, e.g., a paper containing both category I
and category II images was classified as a category II paper.
Among the 782 problematic papers found in this study, 230
(29.4%) contained simple duplications and 356 (45.5%) con-
tained duplicated images with repositioning, while the remaining
196 (25.1%) contained duplicated figures with alteration.
Temporal trends in image duplication. To investigate the
prevalence of image duplications and alterations over time, we
plotted the percentage of papers containing inappropriate im-
age duplication as a function of publication year (Fig. 5). The
percentage of papers with image duplications appeared to be
relatively low (�2%) from 1995 to 2002, with no problematic
images found among the 194 papers screened from 1995. How-
ever, a sharp increase in the percentage of papers with dupli-
cated images was observed in the year 2003 (3.6%), after which
the percentages remained close to or above 4%. This pattern
remained very similar when only a subset of 16 journals for
which papers were scanned from all 20 years was considered,
except for a decline in the duplications found in 2014, the last
year of our screen (2.2%) (Fig. 5).
Correlation of impact factor with image duplication. Sub-
stantial variation in the prevalence of papers with image dupli-
cation was observed among the 40 journals investigated. In
PLoS One, from which the largest number of papers was
screened, 4.3% of the papers were found to contain inappro-
priately duplicated images, whereas the percentage of papers
FIG 3 Examples of duplication with repositioning (category II). (A) Al-
though the panels represent four different experimental conditions, three of
the four panels appear to show a region of overlap (green and blue boxes),
suggesting that these photographs were actually obtained from the same spec-
imen. These panels originally appeared in reference 31 and were corrected in
reference 32. (B) Western blot panels that purportedly depict different pro-
teins and cellular fractions, but the blots appear very similar, albeit shifted by
two lanes (red boxes). Panels originally appeared in reference 33, and were
corrected in reference 34. (Figures in both panels were reproduced under the
Creative Commons [CC BY] license.)
FIG 4 Examples of duplication with alteration (category III). (A) The left and
right FACS panels represent different experimental conditions and show dif-
ferent percentages of cell subsets, but regions of identity (colored boxes) be-
tween the panels suggest that the images have been altered. (This illustration
originally appeared in reference 35; the article was retracted in reference 36.)
(Reproduced with permission from the publisher.) (B) The figure shown here
displays Western blotting results for 10 different protein fractions isolated
from a density gradient. The figure appears to show a single blot, but the last
two lanes (red circles) appear to contain an identical band. Exposure was
altered to bring out details in reference 37; the figure was corrected in reference
38. (Figure reproduced under the Creative Commons [CC BY] license.)
Perspective
May/June 2016
Volume 7
Issue 3
e00809-16
®
mbio.asm.org
3
 on June 4, 2019 by guest
http://mbio.asm.org/
Downloaded from 
 with image duplication ranged from 0.3% (Journal of Cell Bi-
ology) to 12.4% (International Journal of Oncology) among the
other journals, with a mean of 4.4% in the last decade. Hence,
even though PLoS One was the journal from which the largest
set of papers were evaluated in this study, it is not an outlier
with regard to inappropriately duplicated images relative to the
other journals examined. To assess the possibility that journals
with higher impact factors might be better at detecting prob-
lematic images and/or authors might be more careful in pre-
paring images for publication in such journals, the relationship
between the prevalence of image duplication and journal im-
pact factor was examined (Fig. 6). For this analysis, only papers
published between 2005 and 2014 were included, because the
prevalence of problematic images was lower in older publica-
tions, and older papers were only evaluated for selected jour-
nals. A negative correlation between image duplication and
journal impact factor was observed (Pearson’s correlation, P �
0.019), with the lowest percentage of problematic images found
in journals with high impact factors. The prevalence of image
duplication in 12 open-access journals was not significantly
different from that in 28 non-open-access journals (P � 0.38,
chi-square test).
Countries of origin of papers containing image duplication.
To determine whether inappropriate image duplication was more
frequent in papers from some countries than others, the country
of origin for each of the 348 papers from PLoS One containing
duplicated images was compared to the country of origin for all
papers published by that journal during the same time interval
that were included in our search. In cases where the authors of a
paper were affiliated with institutions in multiple countries, all
countries were taken into account. A majority of the 8,138
screened papers published in PLoS One during the 16-month
study period from 2013 to 2014 were affiliated with China (26.2%)
and the United States (40.9%) (Fig. 7). However, papers from
China had a 1.89-fold-higher probability of containing problem-
atic images than would have been predicted from the frequency of
publication (chi-square test, P � 0.001), while papers from the
United States had a lower probability (0.60-fold; chi-square test,
P � 0.001). Other countries with a higher-than-predicted ratio of
papers containing image duplication were India (1.93) and Tai-
wan (1.20), whereas the prevalence rates of image duplication
were lower than predicted in papers from the United Kingdom
(0.47), Japan (0.26), and Germany (0.34).
Number of authors and image duplication. Errors or miscon-
duct might be predicted to be more frequent in papers with fewer
authors, due to reduced scrutiny. The mean number of authors
per paper for the 781 papers containing inappropriate image du-
plication was 7.28. No significant difference between the mean
number of authors for papers with versus without image duplica-
tion was found (P � 0.1).
Problematic images in multiple papers by the same author.
Our data set of 782 problematic papers contained 28 papers (i.e.,
14 pairs) of papers with a common first author. To determine
whether authors of papers containing inappropriate image dupli-
cation were more likely to have published additional papers con-
taining image duplication, we screened other papers written by
the first and last authors of 559 papers (all from unique first au-
thors) identified during our initial screening. This analysis en-
compassed 2,425 papers, or a mean of 4.3 additional papers for
each primary paper. In 217 cases (38.8%), at least one additional
paper containing duplicated images was identified. In total, 269
additional papers containing duplicated images (11.1%) were
found among the 2,425 papers in the secondary data set. The per-
centage of papers with duplicated images in the secondary data set
was significantly higher than that of the first data set (11.1% versus
3.8%; chi-square test, P � 0.001), indicating that other papers by
first or last authors of papers with duplicated images have an in-
creased probability of also containing duplicated images.
0 
1 
2 
3 
4 
5 
6 
1995 
1996
1997
1998 
1999 
2000 
2001
2002 
2003 
2004
2005
2006 
2007 
2008 
2009 
2010 
2011 
2012 
2013 
2014 
Year
Papers with problematic Images (%) 
All 40 journals 
Subset of 16 journals
FIG 5 Percentage of papers containing inappropriate image duplications by year of publication. No papers with duplications were found in 1995. The dark gray
bars show the data for all 40 journals. The light gray bars show a subset of 16 journals for which papers spanning the complete timespan of 20 years were scanned.
The total numbers of papers screened in each year are shown in Fig. 1.
Perspective
4
®
mbio.asm.org
May/June 2016
Volume 7
Issue 3
e00809-16
 on June 4, 2019 by guest
http://mbio.asm.org/
Downloaded from 
 DISCUSSION
The quality and integrity of the scientific literature has been in-
creasingly questioned (8, 9, 15, 16). The present study attempted
to empirically determine the prevalence of one type of problem-
atic data, inappropriate image duplication, by visual inspection of
electrophoretic, microscopic, and flow cytometric data from more
than 20,000 recent papers in 40 primary research journals. The
major findings of this study were the following: (i) figures contain-
ing inappropriately duplicated images can be readily identified in
published papers through visual inspection without the need for
special forensic software methods or tools; (ii) approximately 1 of
every 25 published papers contains inappropriately duplicated
images; (iii) the prevalence of papers with inappropriate image
duplication rose sharply after 2002 and has since remained at in-
creased levels; (iv) the prevalence of inappropriate image duplica-
tion varies among journals and correlates inversely with journal
impact factor; (v) papers containing inappropriately duplicated
images originated more frequently from China and India and less
frequently from the United States, United Kingdom, Germany,
Japan, or Australia; and (vi) other papers by authors of papers
containing inappropriately duplicated images often contained
duplicated images as well. These findings have important impli-
cations for the biomedical research enterprise and suggest a need
to improve the literature though greater vigilance and education.
The finding that those figures with inappropriate duplications
can be readily identified by simple inspection suggests that greater
scrutiny of publications by authors, reviewers, and editors might
be able to identify problematic figures prior to publication. The
Journal of Cell Biology was among the first to call attention to the
problem of figure alteration in manuscripts (17), and this journal
instituted a policy to carefully inspect all manuscripts for image
manipulation prior to publication (14). The low prevalence of
problematic images in this journal (0.3%) suggests that these mea-
sures have been effective. EMBO Journal, which was not part of the
present study, has also instituted a manual screening process for
aberrant images (18). Our findings are consistent with the notion
that greater scrutiny by journals can reduce the prevalence of
problematic images. However, this is likely to require a concerted
effort by all journals, so that authors of papers with problematic
data do not simply avoid publication in venues that employ rig-
orous screening procedures.
The prevalence of papers containing inappropriate image du-
plication increased markedly in 2003 and has remained high in
subsequent years. This coincides with the observed increase in
retracted publications (8) and provides empirical evidence that
the increased prevalence of problematic data is not simply a result
of increased detection, as has been suggested (19). Although the
causes of the increased frequency of image duplication since 2003
are not known, we have considered several possible explanations.
First, older papers often contained figures with lower resolution,
which may have obscured evidence of manipulation. Second, the
widespread availability and usage of digital image modification
Papers with problematic images (%)
Impact factor 
0
4
8
12
1
2
3
4
5 6 7 8 910
20
30
40 50
Publisher
ASM
BioMedCentral
Elsevier
Hindawi
Informa
PLOS
Spandidos
Springer
Wiley
Other
FIG 6 Correlation between journal impact factor and percentage of papers with image duplication. Only papers from 2005 to 2014 (n � 17,816) were included
in this analysis. Each data point represents a journal included in this study (n � 40), with data points color-coded according to the publisher (n � 14; journals
published by AAAS, Nature, Cell Press, the National Academy of Sciences, and the Rockefeller University Press are grouped under “other.”) The x axis is shown
on a logarithmic scale due to the small number of journals with a high impact factor included in this study. The blue line shows a linear regression model. The
gray zone depicts the 95% confidence interval.
Perspective
May/June 2016
Volume 7
Issue 3
e00809-16
®
mbio.asm.org
5
 on June 4, 2019 by guest
http://mbio.asm.org/
Downloaded from 
 software in recent years may have provided greater opportunity
for both error and intentional manipulation. Third, the increasing
tendency for images to be directly prepared by authors instead of
by professional photographers working in consultation with au-
thors has removed a potential mechanism of quality control. One
possible mechanism to reduce errors at the laboratory level would
be to involve multiple individuals in the preparation of figures for
publication. The lack of correlation between author number and
the frequency of image duplication suggests that the roles of most
authors are compartmentalized or diluted, such that errors or
misconduct are not readily detected. A fourth consideration is that
increasing competition and career-related pressures may be en-
couraging careless or dishonest research practices (20). Finally,
electronic manuscript submission, as implemented by many jour-
nals in the early 2000s, facilitated submissions from researchers in
some countries who were previously discouraged to submit be-
cause of high postal costs.
A large variation in the prevalence of papers containing inap-
propriately duplicated images was observed among journals rang-
ing from the Journal of Cell Biology (0.3%) to the International
Journal of Oncology (12.4%), a difference of more than 40-fold.
The differences among journals are important because, as noted
above, these findings suggest that journal editorial policies can
have a substantial impact on this problem. Alternatively, the vari-
able prevalence of duplication could be partly accounted for by
variations in the average number of figures and the number of
panels per figure, which is likely to differ per journal but was not
determined in our study. The inverse correlation between the
prevalence of problematic papers and journal impact factor con-
trasts with the positive correlation observed for research miscon-
duct resulting in retraction (8, 21–23). Although the association
was weak, it may suggest that higher impact journals are better
able to detect anomalous images prior to publication. Alterna-
tively, authors submitting to such journals may be more careful
with figure preparation. Nevertheless, we note that even the most
highly selective journals contained some papers with figures of
concern.
China and the United States were responsible for the majority
of papers containing inappropriately duplicated images, which is
not surprising given the large research output of these countries.
However, it is noteworthy that the proportions of PLoS One pa-
pers from China and India that were found to contain problematic
images were higher than would be predicted from their overall
representation, whereas the opposite was true for papers from the
United States, United Kingdom, Germany, Japan, and Australia.
This suggests that ongoing efforts at scientific ethics reform in
China and India should pay particular attention to issues relating
to figure preparation (24, 25). The analysis of geographic origin
was limited to papers published in PLoS One, because this journal
offered an online tool to search for this information. The geo-
graphic distribution of papers with problematic images may be
different in other journals.
In nearly 40% of the instances in which a problematic paper
was identified, screening of other papers from the same authors
revealed additional problematic papers in the literature. This sug-
gests that image duplication results from systematic problems in
figure preparation by individual researchers, which tend to recur.
Our findings suggest that as many as 1 out of every 25 pub-
lished papers containing Western blotting results or other photo-
graphic images could contain data anomalies. This is likely to be
an underestimate of the extent of problematic data in the litera-
ture for several reasons. First, only image data were analyzed; thus,
errors or manipulation involving numerical data in graphs or ta-
bles would not have been detected. Second, only duplicated im-
ages within the same paper were examined; thus, the reuse of
images in other papers by the same author(s) would not have been
detected. Third, since problematic images were detected by visual
inspection, the false-negative rate could not be determined; we
readily acknowledge that our screen may have missed problematic
papers. It should be noted that our findings contrast with a recent
small study by Oksvold that examined 120 papers from three dif-
ferent cancer research journals, which reported duplicated images
in 24.2% of the papers examined (26). Many of the reported image
duplications in the Oksvold study involved representation of
identical experiments, which we do not regard as necessarily in-
appropriate, as this form of duplication does not alter the research
results. For comparison, we screened 427 papers from the same
three journals examined by Oksvold and found the average per-
centage of problematic papers in these journals to be 6.8%, which
is closer to our findings for other journals. Moreover, our study
included more than 20,000 papers from 40 journals; in addition to
more rigorous inclusion criteria, we required consensus between
three independent examiners for an image to be classified as con-
taining inappropriate duplication, ensuring a low false-positive
rate.
USA
China
UK
Japan
Germany
France
Canada
Italy
Taiwan
Korea
Spain
Australia
India
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.10
0.20
0.30
0.40
0.50
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.10
0.20
0.30
0.40
0.50
Fraction of published papers
Fraction of papers with problematic images
FIG 7 Proportion of papers with image duplications by country. The pro-
portion of papers affiliated with specific countries submitted to PLoS One
during a 16-month period in the years 2013 and 2014 (n � 8,138) plotted
versus the proportion of PLoS One papers from that same period containing
inappropriate image duplication, affiliated with specific countries (n � 348).
Each data point represents a country for which 100 or more papers were
screened. Some papers were affiliated with more than one country. The blue
line represents where data points are expected to fall if problematic papers are
distributed as expected according to their representation in the journal. Coun-
tries plotted above the blue line had a higher-than-expected proportion of
problematic papers; countries plotted below the line had a lower-than-
expected ratio.
Perspective
6
®
mbio.asm.org
May/June 2016
Volume 7
Issue 3
e00809-16
 on June 4, 2019 by guest
http://mbio.asm.org/
Downloaded from 
 The high prevalence of inaccurate data in the literature should
be a finding of tremendous concern to the scientific community,
since the literature is the record of scientific output upon which
future research progress depends. Papers containing inaccurate
data can reduce the efficiency of the scientific enterprise by direct-
ing investigators to pursue false leads or construct unsupportable
hypotheses. Although our findings are disturbing, they also sug-
gest specific actions that can be taken to improve the literature.
Increased awareness of recurring problems with figure prepara-
tion, such as control band duplication, can lead to the reform of
laboratory procedures to detect and correct such issues prior to
manuscript submission. The variation among journals in the
prevalence of problematic papers suggests that individual journal
practices, such as image screening, can reduce the prevalence of
problematic images (17, 18). The problems identified in this study
provide further evidence for the scientific establishment that cur-
rent standards are insufficient to prevent flawed papers from be-
ing published. Our findings call for the need of greater efforts to
ensure the reliability and integrity of the research literature.
OVERVIEW OF OUR RESEARCH APPROACH
Selection strategy. A total of 20,621 papers were selected from 40
different scientific journals in the fields of microbiology and im-
munology, cancer biology, and general biology. These journals
were published by 14 organizations (average of 2.9 journals per
publisher; range, 1 to 6) (see Table S1 in the supplemental mate-
rial). All journals included in the search were indexed in PubMed,
with a mean impact factor of 6.9 (range, 1.3 to 42.4 [Thomson
Reuters 2013]). Papers were examined if they contained the search
term “Western blot” and were identified by using the search tool
provided at the journal’s website. Only original research papers
containing figures were included; retracted papers, review papers,
and conference abstracts were excluded. Corrected papers were
included only if the correction involved issues other than the
problems identified by the present analysis (e.g., an incorrect
grant statement). From a single journal (PLoS One), 8,138 papers
published in 2013 and 2014 were included in the study, compris-
ing 39.5% of the data set. From the remaining journals, a mean of
320.1 (range, 77 to 1,070) papers per journal were included. For
most of these journals, if more than 50 papers were found in a
given year, screening was limited to the first 40 to 50 papers that
were shown in the search field. The selected papers spanned the
years 1995 to 2014, with most papers published in 2013 and 2014,
primarily as a result of the large contribution of papers from PLoS
One (Fig. 1). The large number of PLoS One papers analyzed re-
flects both the journal format, which facilitates image analysis, and
the fact that PLoS One is currently the world’s largest scientific
journal, with approximately 30,000 new articles per year (https://
en.wikipedia.org/wiki/PLOS_ONE).
Visual screening. All papers were screened by examining im-
ages at the publisher’s website. Although papers were selected us-
ing the search term “Western blot,” all types of photographic im-
ages were examined, including protein and nucleic acid
electrophoretic gels and blots, histology or microscopy images,
and photos of culture plates, plants, and animals. FACS plots were
included as well, since these, like photographic images, purport-
edly represent raw data. Figure panels containing line art, such as
bar graphs or line graphs, were not included in the study. Images
within the same paper were visually inspected for inappropriate
duplications, repositioning, or possible manipulation (e.g., dupli-
cations of bands within the same blot). All papers were initially
screened by one of the authors (E.M.B.). If a possible problematic
image or set of images was detected, figures were further examined
for evidence of image duplication or manipulation by using the
Adjust Color tool in Preview software on an Apple iMac com-
puter. No additional special imaging software was used. Supple-
mentary figures were not part of the initial search but were exam-
ined in papers in which problems were found in images in the
primary manuscript. All figures found by the screening author
(E.M.B.) to contain possible duplications were independently re-
viewed by the two coauthors (F.C.F. and A.C.). Consensus had to
be reached among all three authors for a paper to be considered to
contain unequivocal evidence of inappropriate figure duplication.
Consensus among all three authors was reached in 90.4% of the
papers selected during primary screening.
Statistical analysis. The R software package was used to plot
and analyze data. The stat_smooth method implemented from the
R ggplot2 library was applied to find the best fit for a linear regres-
sion model on semi-log-transformed data examining the relation-
ship between the 2013 Thomson Reuters impact factor and per-
centage of papers with inappropriately duplicated images for the
40 journals included in this study. Pearson’s correlation and Pear-
son’s chi-square tests with Yates’ continuity correction were per-
formed in basic R. Details on the R code are available in Data
Set S1 in the supplemental material.
SUPPLEMENTAL MATERIAL
Supplemental material for this article may be found at http://mbio.asm.org/
lookup/suppl/doi:10.1128/mBio.00809-16/-/DCSupplemental.
Table S1, XLSX file, 0.1 MB.
Data Set S1, DOCX file, 0.1MB.
FUNDING INFORMATION
This research received no specific grant from any funding agency in the
public, commercial, or not-for-profit sectors.
REFERENCES
1. Casadevall A, Steen RG, Fang FC. 2014. Sources of error in the retracted
scientific literature. FASEB J 28:3847–3855. http://dx.doi.org/10.1096/
fj.14-256735.
2. Steneck NH. 2006. Fostering integrity in research: definitions, current
knowledge, and future directions. Sci Eng Ethics 12:53–74. http://
dx.doi.org/10.1007/s11948-006-0006-y.
3. Fanelli D. 2009. How many scientists fabricate and falsify research? A
systematic review and meta-analysis of survey data. PLoS One 4:e5738.
http://dx.doi.org/10.1371/journal.pone.0005738.
4. Lüscher TF. 2013. The codex of science: honesty, precision, and truth—
and its violations. Eur Heart J 34:1018–1023. http://dx.doi.org/10.1093/
eurheartj/eht063.
5. Geison GL. 1996. The private science of Louis Pasteur illustrated, revised.
Princeton University Press, Princeton, NJ.
6. Alberts B, Cicerone RJ, Fienberg SE, Kamb A, McNutt M, Nerem RM,
Schekman R, Shiffrin R, Stodden V, Suresh S, Zuber MT, Pope BK,
Jamieson KH. 2015. Scientific integrity. Self-correction in science at work.
Science 348:1420–1422. http://dx.doi.org/10.1126/science.aab3847.
7. Steen RG, Casadevall A, Fang FC. 2013. Why has the number of scientific
retractions increased? PLoS One 8:e68397. http://dx.doi.org/10.1371/
journal.pone.0068397.
8. Fang FC, Steen RG, Casadevall A. 2012. Misconduct accounts for the
majority of retracted scientific publications. Proc Natl Acad Sci U S A
109:17028–17033. http://dx.doi.org/10.1073/pnas.1212247109.
9. Begley CG, Ellis LM. 2012. Drug development: raise standards for pre-
clinical cancer research. Nature 483:531–533. http://dx.doi.org/10.1038/
483531a.
10. Prinz F, Schlange T, Asadullah K. 2011. Believe it or not: how much can
Perspective
May/June 2016
Volume 7
Issue 3
e00809-16
®
mbio.asm.org
7
 on June 4, 2019 by guest
http://mbio.asm.org/
Downloaded from 
 we rely on published data on potential drug targets? Nat Rev Drug Discov
10:712. http://dx.doi.org/10.1038/nrd3439-c1.
11. Open Science Collaboration. 2015. Psychology. Estimating the reproduc-
ibility of psychological science. Science 349:aac4716. http://dx.doi.org/
10.1126/science.aac4716.
12. Flaherty DK. 2011. The vaccine-autism connection: a public health crisis
caused by unethical medical practices and fraudulent science. Ann Phar-
macother 45:1302–1304. http://dx.doi.org/10.1345/aph.1Q318.
13. Bowen A, Casadevall A. 2015. Increasing disparities between resource
inputs and outcomes, as measured by certain health deliverables, in bio-
medical research. Proc Natl Acad Sci U S A 112:11335–11340. http://
dx.doi.org/10.1073/pnas.1504955112.
14. Rossner M, Yamada KM. 2004. What’s in a picture? The temptation of
image manipulation. J Cell Biol 166:11–15. http://dx.doi.org/10.1083/
jcb.200406019.
15. Ioannidis JP. 2005. Why most published research findings are false. PLoS
Med 2:e124. http://dx.doi.org/10.1371/journal.pmed.0020124.
16. Collins FS, Tabak LA. 2014. Policy: NIH plans to enhance reproducibility.
Nature 505:612–613. http://dx.doi.org/10.1038/505612a.
17. Rossner M. 2002. Figure manipulation: assessing what is acceptable. J Cell
Biol 158:1151. http://dx.doi.org/10.1083/jcb.200209084.
18. Pulverer B. 2014. EMBO Press—a new way to publish. EMBO J 33:1–2.
http://dx.doi.org/10.1002/embj.201387566.
19. Fanelli D, Costas R, Larivière V. 2015. Misconduct policies, academic
culture and career stage, not gender or pressures to publish, affect scien-
tific integrity. PLoS One 10:e0127556. http://dx.doi.org/10.1371/
journal.pone.0127556.
20. Anderson MS, Ronning EA, De Vries R, Martinson BC. 2007. The
perverse effects of competition on scientists’ work and relationships. Sci
Eng Ethics 13:437–461. http://dx.doi.org/10.1007/s11948-007-9042-5.
21. Fang FC, Casadevall A. 2011. Retracted science and the retraction index.
Infect Immun 79:3855–3859. http://dx.doi.org/10.1128/IAI.05661-11.
22. Cokol M, Iossifov I, Rodriguez-Esteban R, Rzhetsky A. 2007. How many
scientific papers should be retracted? EMBO Rep 8:422–423. http://
dx.doi.org/10.1038/sj.embor.7400970.
23. Liu SV. 2006. Top journals’ top retraction rates. Sci Ethics 1:91–93. http://
im1.biz/TopRetraction.pdf.
24. Yidong G, Xin H. 2006. Research ethics. China’s Science Ministry fires a
barrage of measures at misconduct. Science 312:1728a–1729a. http://
dx.doi.org/10.1126/science.312.5781.1728a.
25. Patnaik PR. 2015. Scientific misconduct in India: causes and perpetua-
tion. Sci Eng Ethics. http://dx.doi.org/10.1007/s11948-015-9677-6.
26. Oksvold MP. 2016. Incidence of data duplications in a randomly selected
pool of life science publications. Sci Eng Ethics 22:487–496. http://
dx.doi.org/10.1007/s11948-015-9668-7.
27. Zou X, Sorenson BS, Ross KF, Herzberg MC. 2013. Augmentation of
epithelial resistance to invading bacteria by using mRNA transfections.
Infect Immun 81:3975–3983. http://dx.doi.org/10.1128/IAI.00539-13.
28. Zou X, Sorenson BS, Ross KF, Herzberg MC. 2015. Correction for Zou
et al., augmentation of epithelial resistance to invading bacteria by using
mRNA transfections. Infect Immun 83:1226–1227. http://dx.doi.org/
10.1128/IAI.02991-14.
29. Liu J, Xu P, Collins C, Liu H, Zhang J, Keblesh JP, Xiong H. 2013.
HIV-1 Tat protein increases microglial outward K� current and resultant
neurotoxic activity. PLoS One 8:e64904. http://dx.doi.org/10.1371/
journal.pone.0064904.
30. PLoS One Staff. 2014. Correction: HIV-1 Tat protein increases microglial
outward K� current and resultant neurotoxic activity. PLoS One
9:e109218. http://dx.doi.org/10.1371/journal.pone.0109218.
31. Liu Y, Liu Y, Sun C, Gan L, Zhang L, Mao A, Du Y, Zhou R, Zhang H.
2014. Carbon ion radiation inhibits glioma and endothelial cell migration
induced by secreted VEGF. PLoS One 9:e98448. http://dx.doi.org/
10.1371/journal.pone.0098448.
32. Liu Y, Liu Y, Sun C, Gan L, Zhang L, Mao A, Du Y, Zhou R, Zhang H.
2015. Correction: carbon ion radiation inhibits glioma and endothelial
cell migration induced by secreted VEGF. PLoS One 10:e0135508. http://
dx.doi.org/10.1371/journal.pone.0135508.
33. Pulloor NK, Nair S, McCaffrey K, Kostic AD, Bist P, Weaver JD, Riley
AM, Tyagi R, Uchil PD, York JD, Snyder SH, García-Sastre A, Potter
BV, Lin R, Shears SB, Xavier RJ, Krishnan MN. 2014. Human genome-
wide RNAi screen identifies an essential role for inositol pyrophosphates
in type-I interferon response. PLoS Pathog 10:e1003981. http://
dx.doi.org/10.1371/journal.ppat.1003981.
34. PLoS Pathogens Staff. 2014. Correction: human genome-wide RNAi
screen identifies an essential role for inositol pyrophosphates in type-I
interferon response. PLoS Pathog 10:e1004519. http://dx.doi.org/
10.1371/journal.ppat.1004519.
35. Xu X, Liu T, Zhang A, Huo X, Luo Q, Chen Z, Yu L, Li Q, Liu L, Lun
ZR, Shen J. 2012. Reactive oxygen species-triggered trophoblast apoptosis
is initiated by endoplasmic reticulum stress via activation of caspase-12,
CHOP, and the JNK pathway in Toxoplasma gondii infection in mice.
Infect Immun 80:2121–2132. http://dx.doi.org/10.1128/IAI.06295-11.
36. Xu X, Liu T, Zhang A, Huo X, Luo Q, Chen Z, Yu L, Li Q, Liu L, Lun
Z, Shen J. 2015. Retraction for Xu et al., reactive oxygen species-triggered
trophoblast apoptosis is initiated by endoplasmic reticulum stress via ac-
tivation of caspase-12, CHOP, and the JNK pathway in Toxoplasma gon-
dii infection in mice. Infect Immun 83:1735. http://dx.doi.org/10.1128/
IAI.00118-15.
37. Friedman-Levi Y, Mizrahi M, Frid K, Binyamin O, Gabizon R. 2013.
PrP(ST), a soluble, protease resistant and truncated PrP form features in
the pathogenesis of a genetic prion disease. PLoS One 8:e69583. http://
dx.doi.org/10.1371/journal.pone.0069583.
38. Friedman-Levi Y, Mizrahi M, Frid K, Binyamin O, Gabizon R. 2015.
Correction: PrPST, a soluble, protease resistant and truncated PrP form
features in the pathogenesis of a genetic prion disease. PLoS One 10:
e0133911. http://dx.doi.org/10.1371/journal.pone.0133911.
Perspective
8
®
mbio.asm.org
May/June 2016
Volume 7
Issue 3
e00809-16
 on June 4, 2019 by guest
http://mbio.asm.org/
Downloaded from 
