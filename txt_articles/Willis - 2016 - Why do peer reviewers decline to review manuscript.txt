 R E S E A R C H A R T I C L E
Received: 29 September 2015
|
Accepted: 9 November 2015
(wileyonlinelibrary.com) doi: 10.1002/leap.1006
Why do peer reviewers decline to review manuscripts? A study
of reviewer invitation responses
Michael Willis
Abstract
With peer review under closer scrutiny than ever before, research is needed to
investigate not only what incentives encourage researchers to review manuscripts,
but also what reasons prevent them from reviewing. We analysed responses to
reviewer invitations sent by one journal in March to July 2015. The data showed
that the overwhelming reason why reviewers decline is because they are
unavailable to do so. Although the finding may not be surprising and confirms the
findings of earlier research, the study illustrates how a journal can analyse and draw
conclusions from its own reviewer invitation data as a first step to improving the
invitation acceptance rates.
INTRODUCTION
As the published literature onpeer reviewgrows (asearchon PubMed found
that the number of articles published each year containing ‘peer review’ in
the title increased from about 50 in 1974 to around 120 in 2014), there is
an increasing focus on investigating reviewer behaviour and motivation
(for
example
the
PEERE
Action,
http://www.peere.org/peeer-in-a-
nutshell/, funded by the European Union). The 2014 Author Insights survey
by Nature Publishing Group found that 77% of researchers agreed, or
strongly agreed, that ‘traditional peer review processes could be made more
efficient’, and over 70% reported that they were frequently frustrated by the
length of time the process of peer review takes (Nature Publishing Group,
2014). In the face of such frustration, various initiatives have arisen to
provide incentives and credit to reviewers to undertake peer review, an
activity otherwise largely unacknowledged by employers, accreditation
agencies, or funding bodies, in the hope that this will lead to speedier and
more robust peer review. Some publishers offer rewards for reviewers: for
example, reviewers for journals published by Elsevier can claim discounts
on
Elsevier
services
(http://www.reviewerrecognition.elsevier.com).
Collabra (http://www.collabra.org/about/our-model/), an open access
megajournal published by the University of California Press, uses a
proportion of its article processing charges to pay reviewers. Arguing that
‘the real currency of academia is reputation’ and that perks and discounts
offered by publishers are insufficient for motivating peer reviewers
(Johnston, 2015), Publons (http://publons.com) offers official recognition
for reviewers in a format that they can use for tenure and funding
applications. ORCID, CASRAI and F1000 have partnered to develop an
industry standard for public acknowledgement of peer review, not only
in the interests of promoting trust and transparency in the research
and publication process, but also as an incentive to researchers to engage
in peer review (Haak, 2015).
In examining what types of incentive encourage reviewers to
review, however, it is also important to examine the reverse: to identify
which factors prevent reviewers from undertaking peer review. The only
published research investigating this which could be discovered by
searching in Google using the terms reasons, peer review, and decline, is
nearly 10 years old (Tite & Schroter, 2007), and although there are online
forums and blog posts describing why particular individuals have chosen
to decline to review papers (for example, Academia Stack Exchange,
2015,
http://academia.stackexchange.com/questions/657/when-is-it-
appropriate-to-decline-a-review-request), there is a scope for further
research in this area. The present study demonstrates how a single
journal generated and analysed data for its own pool of reviewers in
order to identify obstacles preventing reviewers from agreeing to review.
METHOD
The study looked at a journal published by John Wiley & Sons Ltd and
owned by a professional medical society based in the United Kingdom, with
affiliations to other international societies in the field. The journal is listed in
the second quartile within its Journal Citation Reports category and may
therefore be classed as a ‘mid-rank’ journal in its field, as measured by
Impact Factor. The journal uses Thomson Reuters’ ScholarOne Manuscripts
for its editorial office peer review management. Data for this study were
Michael Willis
Regional Manager, Peer Review, John Wiley & Sons Ltd,
Oxford, UK.
ORCID: 000-0002-3110-3796
E-mail: miwillis@wiley.com
Learned Publishing 2016; 29: 5–7
alpsp.org/Learned-Publishing
© 2016 John Wiley & Sons Ltd. Learned Publishing published by John Wiley & Sons Ltd
on behalf of ALPSP – The Association for Learned and Professional Society Publishers.
5
This is an open access article under the terms of the Creative Commons Attribution License, which permits use,
distribution and reproduction in any medium, provided the original work is properly cited.
 collected and used with the consent of the society, on the understanding
that the journal, society, and reviewer/editor names were not revealed.
The journal’s database of reviewers has been developed since the
journal’s inception nearly 20 years ago. It includes members of the
society, which own the journal, members of affiliated societies, personal
acquaintances of Editors, and members of the editorial boards, and
authors of papers previously submitted to and published in the journal.
The reviewer invitation e-mail contains the title and abstract of the
paper, followed by hyperlinks for each response option. The reviewer
clicks on the appropriate response that is then registered immediately in
the ScholarOne Manuscripts database. Prior to February 2015 the
response options were ‘Agree to review’ and ‘Decline to review’. Since
February 2015, reviewers have been offered four e-mail response options:
• Agree to review
• Decline to review – not available
• Decline to review – not my field
• Decline to review – conflict of interest
The study looked at responses selected by reviewers when they were
invited to review original research papers and review articles over a
5-month period, from 1 March to 31 July 2015 inclusive. Only invitations
for the first round of peer review were included, because for this journal
reviewers are not invited to review revised versions – the paper is sent back
to them automatically if they have previously agreed to review a revised
version. As there is an expectation that the journal’s panel of Editors (around
a dozen) will review a paper if invited to do so, responses from these
individuals were excluded from the study. Editorial Board members were
not excluded as the journal lays no obligation upon them to review if invited
to do so. As well as analysing the responses received from reviewers, the
time to respond to an invitation was also examined, calculated as the time
in calendar days from the date on which the invitation was sent to the date
on which the response was recorded. If no response was received from a
reviewer, then this was recorded manually by the editorial office staff.
RESULTS
During the period of the study 577 unique reviewers (excluding Editors)
were invited to review 221 papers, with a total of 747 invitations sent.
No response was received for 56 (7.5%) invitations. In 11 (1.5%) cases,
a reviewer invitation was not delivered because the e-mail address was
invalid. These two categories were excluded from the results, leaving
for analysis a total of 680 invitations and responses. The number of
responses recorded for each response option, and the respective
response time, are given in Table 1.
A subgroup analysis was conducted of those who were invited to
review more than once. For this group, the median response time for all
response options was 1.8 days (0–30.5), compared with 1.5 days (0–34.5)
for all response options for reviewers invited only one. Details of the
responses for this subgroup are given in Table 2. A slightly higher proportion
of these reviewers agreed to review compared with the figure for the whole
reviewer cohort (72.9% compared with 69.0%), while a lower proportion
declined on the grounds that the paper was outside their area of expertise
(2.5% compared with 4.3%). The median response to indicate unavailability
was somewhat higher in this subgroup (2.6 compared with 1.9).
Within this group, six reviewers were invited four times; of this group,
three agreed to review every time, and one never agreed to review.
Twenty-seven reviewers were invited to review three times; 59 (72.8%)
of these invitations were accepted, while in 16 (19.7%) cases, the reviewers
gave ‘not available’ as the reason for declining, significantly lower than for
the entire cohort. Ninety-eight reviewers were invited to review twice;
129 (65.8%) of these invitations were accepted, while in 49 (25.0%) cases,
the reviewers gave ‘not available’ as the reason for declining.
DISCUSSION
Analysing responses to reviewer invitations allows the editorial office to
identify and resolve weaknesses in the peer review process. The fact that
only 1.5% of all invitations were not delivered because of invalid e-mail
addresses suggests that the quality of data in the reviewer database is
fairly sound. Having said that, a higher proportion (4.3%) of reviewers
indicated that the paper they had been invited to review did not fall
within their area of expertise. An appropriate course of action would
be for the editorial office to ask these reviewers individually to provide
information about their areas of expertise, in order to provide editors
with a more useful database for finding appropriate reviewers.
TABLE 1
Reviewers’ responses to invitations to review.
Invitation response option
n (%)
Median (range)
response time in
calendar days
Agree to review
469 (69.0%)
1.1 (0.0–24.2)
Decline to review – not available
176 (25.9%)
1.9 (0.0–14.5)
Decline to review – not my field
29 (4.3%)
0.9 (0.0–7.8)
Decline to review – conflict of interest
6 (0.9%)
1.4 (0.4–2.3)
Key points
• There are little data on the reasons why reviewers decline to
review.
• This study found that lack of time was the main reason for refusal
(69%).
• Further studies may reveal geographical, demographic, or other
patterns to reviewer refusals.
TABLE 2
Responses to invitations to review by reviewers invited more than once.
Invitation response option
n (%)
Median (range)
response time in
calendar days
Agree to review
205 (72.9%)
1.4 (0.0–24.1)
Decline to review – not available
68 (24.2%)
2.6 (0.0–12.1)
Decline to review – not my field
7 (2.5%)
0.8 (0.4–7.8)
Decline to review – conflict of interest
1 (0.4%)
1.6
6
M. Willis
alpsp.org/Learned-Publishing
Learned Publishing 2016; 29: 5–7
© 2016 John Wiley & Sons Ltd. Learned Publishing
published by John Wiley & Sons Ltd on behalf of ALPSP –
The Association for Learned and Professional Society Publishers.
 In the case of the 56 reviewers who did not respond, appropriate
action might be considered such as contacting them individually to
enquire whether they wish to remain in the reviewer database, or to
ask whether they have alternative preferred methods of contact.
The main focus of this study, however, was analysis of the cases
where a response to the invitation was received from reviewers. An
agreement rate of 69% with a median response time of about 1 day
suggests that the journal has a committed and readily available pool of
reviewers at its disposal, which may be due in part to the fact that the
reviewers have close affiliations to the journal through personal
connections with Editors or Editorial Board members, or through
membership of the owning or affiliated societies.
Of those who declined to review, most indicated that they were not
available to review. This repeats the finding of Tite and Schroter (2007)
that ‘lack of time is the principal factor in the decision to decline’ and is
not surprising. One can speculate that other academic or professional
commitments formed the main competing demands on their time. As
the study period partially overlapped with the main summer holiday
period in the northern hemisphere, personal commitments may also have
contributed to these reviewers declining to review. The journal being a
mid-rank journal within its field, it is also possible that these reviewers
had competing reviewing demands and, given a choice, preferred to
review for higher-rank journals in the field. It is perhaps noteworthy that
this particular cohort typically took longer to respond to an invitation
than other groups.
Given that only a handful of reviewers indicated that they had a
conflict of interest, it is difficult to know whether the effort to avoid
inviting reviewers with such conflicts would have been worthwhile and
time-effective. Providing the option in the invitation letter does at least
give reviewers an easy opportunity to acknowledge that they have a
conflict of interest, which is a specific recommendation in the Publisher’s
guidelines on publishing ethics (John Wiley & Sons Ltd, 2014).
It is not certain that any firm conclusions can be drawn from the
analysis of the subgroup of reviewers who were invited more than once.
The proportion of various response options given, and the times to
respond, are not substantially different from those for the entire cohort.
The sample size is too small to investigate a correlation between
reviewer invitation and reviewer acceptance, seeking to establish
whether a reviewer is more likely to review the more times he or she is
invited. It would be interesting to explore this further.
Study limitations
The study has a number of limitations. Not including the names of the
paper’s authors in the invitation e-mail means that reviewers can only
identify conflicts of interest at the invitation stage by looking at the
abstract, and they are not alerted to possible conflicts of interest between
author groups. Although the reviewer scoresheet allows reviewers the
opportunity to identify such conflicts later in the review process, it would
improve the process if they were alerted at the invitation stage.
It might also be helpful to include in the e-mail guidance as to the
most appropriate option for reviewers to select. For example, defining
what constitutes a conflict of interest might guide some reviewers to
select that option instead of ‘not available’ as the most appropriate
reason for declining.
The order in which the response invitation options appear in the
invitation e-mail may influence the types of response received. A
reviewer who does not want to review for any reason may, in haste or
otherwise, click on the first ‘Decline to review’ response option in the list.
A follow-up study in which the response options are ordered differently
would be worthwhile to see if this has a significant effect.
A future study might examine the demographic profile of the invited
reviewers, including gender, geographical location, affiliation to the
journal’s owning or affiliated societies, and personal acquaintances with
the Editor or editorial board members. One might hypothesise, for
example, that members of the owning society feel an obligation to review
for this journal, although for another journal they might decline the
invitation. It would be fascinating to study regional differences, for
example, to see whether reviewers from, say, the Far East are more likely
to agree to review than reviewers from Europe. Unfortunately, the
current reviewer database does not provide any sufficiently reliable data
for us to make firm conclusions.
Finally, because the study is limited to one journal with one specific
profile, it is impossible to draw conclusions applicable to a wider group of
journals. It would be interesting to repeat the study for a ‘top tier’ journal
within its field and for a journal not owned by a learned or professional
society where there may be less journal loyalty on the part of reviewers.
Taking the study overall, it is unlikely that the proportion of reviewers
declining for any reason would be reduced by making changes to the
options available to them. It could be argued that reviewers are more likely
to decline if they are presented with several possible excuses, although
there is not, to the best of my knowledge, any evidence to suggest this.
CONCLUSION
The study illustrates a simple way of enabling an editorial office to
analyse reviewer invitation responses as a first step towards improving
invitation acceptance rates.
REFERENCES
Haak (2015, 18 May). F100 and ORCID partner launch standard citing
Peer Review activities [Web log post]. Retrieved from http://orcid.org/
blog/2015/05/15/f1000-and-orcid-partner-launch-standard-citing-
peer-review-activities
Johnston, D. (2015). Peer review incentives: a simple idea to encourage fast
and effective peer review. European Science Editing, 41(3), 70–71.
John Wiley & Sons Ltd (2014). Best practice guidelines on publishing ethics: a
publisher’s perspective. Second Edition. Retrieved September 29, 2015
from http://exchanges.wiley.com/ethicsguidelines
Nature Publishing Group (2014). Author insights. Retrieved October 29, 2015,
from http://dx.doi.org/10.6084/m9.figshare.1204999
Tite, L., & Schroter, S. (2007). Why do peer reviewers decline to review? a
survey. Journal of Epidemiology and Community Health, 61, 9–12.
doi:10.1136/jech.2006.049817
Why peer reviewers decline invitations
7
alpsp.org/Learned-Publishing
Learned Publishing 2016; 29: 5–7
© 2016 John Wiley & Sons Ltd. Learned Publishing
published by John Wiley & Sons Ltd on behalf of ALPSP –
The Association for Learned and Professional Society Publishers.
