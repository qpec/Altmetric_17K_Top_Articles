  
 
1 
 
The Hallucination Machine: A Deep-Dream VR platform for Studying the Phenomenology of 
1 
Visual Hallucinations 
2 
 
3 
Keisuke Suzuki, Warrick Roseboom, David J. Schwartzman, Anil K. Seth 
4 
 
5 
Sackler Centre for Consciousness Science, University of Sussex, Brighton BN1 9QJ, United Kingdom 
6 
and 
7 
Department of Informatics, University of Sussex, Brighton BN1 9QJ, United Kingdom 
8 
Corresponding Author: Keisuke Suzuki, Sackler Centre for Consciousness Science, University of Sussex, Brighton 
9 
BN1 9QJ, United Kingdom, K.Suzuki@sussex.ac.uk ‎
 
 
10 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
2 
 
Abstract  
 
11 
Altered states of consciousness, such as psychotic or pharmacologically-induced 
12 
hallucinations, provide a unique opportunity to examine the mechanisms underlying 
13 
conscious perception. However, the phenomenological properties of these states are 
14 
difficult to isolate experimentally from other, more general physiological and cognitive 
15 
effects of psychoactive substances or psychopathological conditions. Thus, simulating 
16 
phenomenological aspects of altered states in the absence of these other more general 
17 
effects provides an important experimental tool for consciousness science and psychiatry. 
18 
Here we describe such a tool, the Hallucination Machine. It comprises a novel combination 
19 
of two powerful technologies: deep convolutional neural networks (DCNNs) and panoramic 
20 
videos of natural scenes, viewed immersively through a head-mounted display (panoramic 
21 
VR).  By doing this, we are able to simulate visual hallucinatory experiences in a biologically 
22 
plausible and ecologically valid way.  Two experiments illustrate potential applications of the 
23 
Hallucination Machine. First, we show that the system induces visual phenomenology 
24 
qualitatively similar to classical psychedelics. In a second experiment, we find that simulated 
25 
hallucinations do not evoke the temporal distortion commonly associated with altered 
26 
states.  Overall, the Hallucination Machine offers a valuable new technique for simulating 
27 
altered phenomenology without directly altering the underlying neurophysiology. 
28 
 
29 
Keywords:  Visual hallucinations, virtual reality, visual phenomenology, deep convolutional 
30 
neural networks, machine learning. 
 
31 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
3 
 
1.0 
Introduction 
32 
There is a long history of studying altered states of consciousness (ASC) in order to 
33 
better understand phenomenological properties of conscious perception 1,2. Altered states 
34 
are defined as a qualitative alteration in the overall pattern of mental functioning, such that 
35 
the experiencer feels their consciousness is radically different from "normal" 1–3, and are 
36 
typically considered distinct from common global alterations of consciousness such as 
37 
dreaming. ASC are not defined by any particular content of consciousness, but cover a wide 
38 
range of qualitative properties including temporal distortion, disruptions of the self, ego-
39 
dissolution, visual distortions and hallucinations, among others 4–7. Causes of ASC include 
40 
psychedelic drugs (e.g., LSD, psilocybin) as well as pathological or psychiatric conditions such 
41 
as epilepsy or psychosis 8–10. In recent years, there has been a resurgence in research 
42 
investigating altered states induced by psychedelic drugs. These studies attempt to 
43 
understand the neural underpinnings that cause altered conscious experience 11–13 as well 
44 
as investigating the potential psychotherapeutic applications of these drugs 4,12,14. However, 
45 
psychedelic compounds have many systemic physiological effects, not all of which are likely 
46 
relevant to the generation of altered perceptual phenomenology. It is difficult, using 
47 
pharmacological manipulations alone, to distinguish the primary causes of altered 
48 
phenomenology from the secondary effects of other more general aspects of 
49 
neurophysiology and basic sensory processing. Understanding the specific nature of altered 
50 
phenomenology in the psychedelic state therefore stands as an important experimental 
51 
challenge.   
52 
Here, we address this challenge by combining virtual reality and machine learning to 
53 
isolate and simulate one specific aspect of psychedelic phenomenology: visual hallucinations. 
54 
In machine learning, deep neural networks (DNNs) developed for machine vision have now 
55 
improved to a level comparable to that achieved by humans 15,16. For example, deep 
56 
convolutional neural networks (DCNNs) have been particularly successful in the difficult task 
57 
of object recognition in photographs of natural scenes 17,18.  
58 
Studies comparing the internal representational structure of trained DCNNs with 
59 
primate and human brains performing similar object recognition tasks, have revealed 
60 
surprising similarities in the representational spaces between these two distinct systems 19–
61 
21. For example, the neural responses induced by a visual stimulus in the human inferior 
62 
temporal (IT) cortex, widely implicated in object recognition, have been shown to be similar 
63 
to the activity pattern of higher (deeper) layers of the DCNN 22,23. Features selectively 
64 
detected by lower layers of the same DCNN bear striking similarities to the low-level 
65 
features processed by the early visual cortices such as V1 and V4. These findings 
66 
demonstrate that even though DCNNs were not explicitly designed to model the visual 
67 
system, after training for challenging object recognition tasks they show marked similarities 
68 
to the functional and hierarchical structure of human visual cortices.  
69 
Trained DCNNs are highly complex, with many parameters and nodes, such that their 
70 
analysis requires innovative visualisation methods. Recently, a novel visualisation algorithm 
71 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
4 
 
called Deep Dream was developed for this purpose 24,25. Deep Dream works by clamping the 
72 
activity of nodes at a user-defined layer in the DCNN and then inverting the information 
73 
flow, so that an input image is changed until the network settles into a stable state (some 
74 
additional constraints are needed, e.g. ensuring that neighbouring pixels remain strongly 
75 
correlated).  Intuitively, this means changing the image rather than changing the network in 
76 
order to match the features of the image with what is represented in the target layer  – so 
77 
that the resulting image is shaped by what the network ‘expects’ to see, at the level of detail 
78 
determined by the clamped layer.  More precisely, the algorithm modifies natural images to 
79 
reflect the categorical features learnt by the network 24,25, with the nature of the 
80 
modification depending on which layer of the network is clamped (see Figure 1). What is 
81 
striking about this process is that the resulting images often have a marked ‘hallucinatory’ 
82 
quality, bearing intuitive similarities to a wide range of psychedelic visual hallucinations 
83 
reported in the literature (e.g. McKenna, 2004; Shanon, 2002; Siegel & Jarvik, 1975)(see 
84 
Figure 1).   
85 
We set out to simulate the visual hallucinatory aspects of the psychedelic state using 
86 
Deep Dream to produce biologically realistic visual hallucinations. To enhance the immersive 
87 
experiential qualities of these hallucinations, we utilised virtual reality (VR). While previous 
88 
studies have used computer-generated imagery (CGI) in VR that demonstrate some 
89 
qualitative similarity to visual hallucinations 28,29, we aimed to generate highly naturalistic 
90 
and dynamic simulated hallucinations. To do so, we presented 360-degree (panoramic) 
91 
videos of pre-recorded natural scenes within a head-mounted display (HMD), which had 
92 
been modified using the Deep Dream algorithm. The presentation of panoramic video using 
93 
a HMD equipped with head-tracking (panoramic VR) allows the individual’s actions 
94 
(specifically, head movements) to change the viewpoint in the video in a naturalistic manner. 
95 
This congruency between visual and bodily motion allows participants to experience 
96 
naturalistic simulated hallucinations in a fully immersive way, which would be impossible to 
97 
achieve using a standard computer display or conventional CGI VR. We call this combination 
98 
of techniques the Hallucination Machine.  
99 
To investigate the extent to which the Hallucination Machine is able to simulate 
100 
natural visual hallucinations, we conducted two proof-of-concept experiments. The first 
101 
experiment investigated the ecological validity of experiences produced by the Hallucination 
102 
Machine. We compared the simulated experiences produced by the Hallucination Machine 
103 
to unaltered control videos (see Figure 1) and to those of pharmacological psychedelic 
104 
states by having participants rate their subjective experience using an ASC questionnaire 
105 
developed to assess psychedelic experiences 30,31. In a second experiment, we investigated if 
106 
the experience of the Hallucination Machine would also lead to a commonly reported 
107 
aspect of altered states of consciousness - temporal distortion 5,6 
108 
 
 
109 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
5 
 
 
110 
Figure 1. An example of the original scene (top left) and Deep-Dreamed scenes (top right, 
111 
bottom left and right). The top right image was generated by selecting a higher DCNN layer 
112 
that responds selectively to higher-level categorical features (layers = ‘inception_4d/pool’, 
113 
octaves = 3, octave scale = 1.8, iterations = 32, jitter = 32, zoom = 1, step size = 1.5, blending 
114 
ratio for optical flow = 0.9, blending ratio for background = 0.1, for more detail see 48). We 
115 
used these higher-level parameters to generate the Deep Dream video used throughout the 
116 
reported experiments. The bottom left image was generated by fixing the activity of a lower 
117 
DCNN layer that responds selectively to geometric image features (layer=’conv2/3x3’, other 
118 
parameters as above). The bottom right image was generated by selecting a middle DCNN 
119 
layer responding selectively to parts of objects (layer=’inception_3b/output’, other 
120 
parameters as above).  
121 
 
122 
2.0  
Results 
123 
We constructed the Hallucination Machine by applying a modified version of the 
124 
Deep Dream algorithm 25 to each frame of a pre-recorded panoramic video (Figure 1, see 
125 
also Supplemental Video S1) presented using a HMD. Participants could freely explore the 
126 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
6 
 
virtual environment by moving their head, experiencing highly immersive dynamic 
127 
hallucination-like visual scenes.  
128 
2.1 Experiment 1: Subjective experience during simulated hallucination 
129 
In Experiment 1, we compared subjective experiences evoked by the Hallucination 
130 
Machine with those elicited by both control videos (within subjects) and by 
131 
pharmacologically induced psychedelic states 31 (across studies). Twelve participants took 
132 
part in Experiment 1. The results are shown in Figure 2. Visual inspection of the spider chart 
133 
reveals that, across all dimensions of subjective experience probed by the questionnaire, 
134 
the experiences elicited by the Hallucination Machine are qualitatively distinct from the 
135 
control videos (Fig 2a), but qualitatively similar to psilocybin experiences (Fig 2b).  
136 
To quantify these observations, we first conducted Bayesian within-subject t-tests 
137 
comparing responses to the ASC questionnaire following Hallucination Machine, and 
138 
following control videos, on the null hypothesis of ‘no difference’.  The analysis revealed 
139 
evidence supporting the alternative hypothesis, suggesting that for the following 
140 
dimensions there was a significant difference in subjective ratings between video type: 
141 
‘intensity’, ‘patterns’, ‘imagery’, ‘ego’, ‘arousal’, ‘strange’, ‘vivid’, ‘space’, ‘muddle’, ‘spirit’ 
142 
(for statistics see Table 1). Bonferroni corrected, within-subject t-tests were consistent with 
143 
the Bayesian results, with the exception of the ‘ego’, ‘muddle’, and ‘spirit’ dimensions as 
144 
shown by the p-values in Table 1.  
145 
Independent Bayesian t-tests comparing responses to the ASC questionnaire 
146 
following the Hallucination Machine, or following administration of psilocybin (data from a 
147 
previous study31),  also revealed evidence supporting the alternative hypothesis for the 
148 
following dimension ‘intensity’, with weaker evidence for ‘pattern’ and ‘strange’, suggesting 
149 
that there are some qualitative differences between Hallucination Machine and psilocybin 
150 
experiences (see Table 1 for statistics). Crucially, for the remaining questions, Bayesian 
151 
analyses were not sensitive to whether the null or alternative hypothesis was supported, 
152 
but were trending in the direction of the null, i.e. no difference between subjective 
153 
experiences between the Hallucination Machine and psilocybin: ‘vivid’ 
154 
‘time’, ‘space’, ‘muddle’, ‘peace’, and ‘past’. Standard paired t-test Bonferroni corrected for 
155 
multiple comparisons between ASC responses following the Hallucination Machine and 
156 
psilocybin did not reach significance for any of the question.   
157 
Together these analyses suggest that for many dimensions of subjective experience 
158 
– as reflected in the ASC questionnaire - the Hallucination Machine induced significant 
159 
changes as compared to viewing unaltered control videos, and that these changes were 
160 
broadly similar to those caused by the administration of psilocybin.  
161 
 
162 
 
163 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
7 
 
 
164 
 
165 
Figure 2. ASC questionnaire responses obtained in Experiment 1. a. Comparison of 
166 
Hallucination Machine and control video responses. Stronger evidence in favour of a 
167 
difference using Bayesian t-tests between Hallucination Machine and the control videos 
168 
were found for ten of the questions (†: BF10 > 3). Standard t-test showed the significant 
169 
differences for eight of the questions (* p <0.05). b. Comparison of Hallucination Machine 
170 
and responses following administration of psilocybin, taken from 31.  Bayes Factor paired 
171 
sample t-tests revealed that responses to the question ‘intensity’ (†: BF10 > 3) after the 
172 
Hallucination Machine had stronger evidence in favour of a difference from the ratings given 
173 
for psilocybin experiences. c. Abbreviations and questions used in ASC questionnaire. 
174 
Table 1. Bayesian and standard statistical comparisons of ASCQ ratings from Experiment 1 
175 
between Hallucination Machine and control video exposure, and between Hallucination 
176 
Machine and psilocybin administration, data taken from 31. Dagger symbols and bold text 
177 
indicates Bayes Factor values which show evidence in favour of a difference between ASCQ 
178 
responses (†: BF10 > 3). Asterisks after p-value indicates the significant differences in 
179 
standard t-test (* p <0.05).  See Figure 2c for Abbreviations and questions used in ASCQ. 
180 
 
181 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
8 
 
 
182 
 
183 
2.2 Experiment 2: Temporal distortion during simulated hallucination 
184 
Experiment 1 showed that subjective experiences induced by the Hallucination 
185 
Machine displayed many similarities to characteristics of the psychedelic state. Based on 
186 
this finding we next used the Hallucination Machine to investigate another commonly 
187 
reported aspect of ASC – temporal distortions5,6, by asking twenty-two participants to 
188 
complete a temporal production task during presentation of Hallucination Machine, or 
189 
during control videos.  
190 
One participant was excluded from the analysis due to producing intervals in the 
191 
experimental session that were temporally inverted compared to the target durations. A 
192 
two-way Bayesian repeated measures ANOVA consisting of factors target interval [1s, 2s, 4s] 
193 
and video type (control/Hallucination Machine) showed the strongest evidence for an effect 
194 
of target interval only (BF10 = 1.178 × 1046, 1s (M=1.75 s SE=0.09s), 2s (M=2.41 s SE=0.11 s) 
195 
and 4 s (M=4.38 s SE=0.16 s)). A model including only video type showed evidence in favour 
196 
of the null hypothesis (BF10 = 0.194), indicating that video type did not affect interval 
197 
production (Figure 3). An additional two-factorial repeated measures ANOVA revealed a 
198 
significant main effect of target interval (F(20,2) = 267.362, p < 0.01, η2=0.930) without the 
199 
interaction ( F(20, 2) = 0.935, p< 0.401, η2=0.045). However, the main effect of video type 
200 
did not reach significance (F(20,1) = 0.476, p = 0.498, η2=0.023).  
201 
Questions 
Hallucination Machine vs control videos 
Hallucination Machine vs psilocybin  
BF10 
(Bayesian 
t-test) 
t(11) 
p-value  
(Bonferroni 
corrected) 
Effect Size 
(Cohen's d) 
BF10 
(Bayesian 
t-test) 
t(25) 
p-value  
(Bonferroni 
corrected) 
Effect Size 
(Cohen's d) 
intensity 
28.09 † 
4.185 
0.034 *  
1.208 
3.404 † 
-2.55 
0.306 
-1.004 
patterns 
389022 † 
13.7 
0.017 * 
3.955 
2.545 
2.364 
0.442 
0.916 
imagery 
18187 † 
9.803 
0.017 * 
2.83 
- 
- 
- 
- 
mood 
0.866 
1.685 
2.04 
0.486 
- 
- 
- 
- 
ego 
3.162 † 
2.669 
0.374 
0.77 
0.69 
1.335 
3.298 
0.517 
arousal 
37.58 † 
4.391 
0.017 * 
1.268 
- 
- 
- 
- 
strange 
1721 † 
7.44 
0.017 * 
2.148 
2.993 
2.467 
0.357 
0.955 
vivid 
122.1 † 
5.254 
0.017 * 
1.1517 
0.437 
0.723 
8.109 
0.28 
time 
0.39 
0.849 
7.038 
0.245 
0.461 
-0.82 
7.157 
-0.317 
space 
1057 † 
7.005 
0.017 * 
2.022 
0.442 
0.747 
7.854 
0.289 
muddle 
6.613 † 
3.183 
0.153 
0.919 
0.385 
0.428 
11.424 
0.166 
merge 
0.494 
-1.146 
4.692 
-0.331 
0.378 
0.364 
12.223 
0.141 
control 
1.697 
2.218 
16.116 
0.64 
1.617 
2.056 
0.85 
0.796 
spirit 
3.375 † 
2.715 
0.34 
0.784 
1.83 
2.144 
0.714 
0.83 
peace 
1.547 
2.149 
0.935 
0.62 
0.429 
0.688 
8.466 
0.267 
float 
0.44 
1.008 
5.695 
0.291 
0.945 
-1.63 
1.955 
-0.633 
past 
0.488 
-1.133 
4.794 
-0.327 
0.363 
-0.17 
14.705 
-0.067 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
9 
 
 
202 
 
203 
Figure 3. Results of temporal production task during presentation of Hallucination Machine 
204 
or control videos. Produced time intervals are shown for both video types and target 
205 
durations (1 second for low, 2 seconds for middle and 4 seconds for the high pitch tone). 
206 
Bayes Factor analysis revealed strong evidence for no difference in subjective responses 
207 
across video type (‡: BF10 < 1/3). 
208 
 
209 
Post-hoc standard and Bayesian t-tests were applied to the participant’s subjective 
210 
ratings for the six questions about their experiences during each video (see Figure 4).  These 
211 
revealed some differences in the Hallucination Machine compared to control video. 
212 
Participants’ ratings of ‘presence’, “How much did you feel as if you were ‘really there’, BF10 
213 
= 26.960, t(20) = 3.705, p=0.007, Cohen’s d=0.808; and ‘attention’, “How focused were you 
214 
on the time production task”, BF10 = 4.830, t(20) = 2.822, p = 0.077, Cohen’s d=0.616 were 
215 
reduced during the Hallucination Machine. Responses regarding Duration (BF10 = 0.278) and 
216 
Speed (BF10 = 0.281) both revealed evidence for no difference between Hallucination 
217 
Machine and control video. Other comparisons failed to reach an evidentiary threshold in 
218 
both Bayesian and normal t-tests. 
219 
 
220 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
10 
 
 
221 
Figure 4. Questionnaire responses obtained in Experiment 2. Participant’s estimates’ of the 
222 
total duration of Hallucination Machine and control videos in seconds (left panel).  
223 
Participants’ subjective ratings between Hallucination Machine and control videos (centre). 
224 
Questions used in Experiment 2 (Right). All questions were presented inside the head 
225 
mounted display and participants responded to each question using a mouse to indicate 
226 
their responses via a visual analog scale. Bayes Factor analysis revealed evidence in favour 
227 
of a difference across video type for Q1: duration and Q2: speed (†: BF10 > 3), whereas 
228 
evidence for no difference was found for Q3: presence and Q4: attention (‡: BF10 < 1/3). 
229 
3.0 
Discussion 
230 
 
231 
We have described the implementation of the Hallucination Machine, which 
232 
provides a novel method for investigating (visual) hallucinogenic phenomenology. It 
233 
combines two technologies: Panoramic video of natural scenes presented using VR, allowing 
234 
the video to be experienced in a fully immersive environment, and an application of deep 
235 
convolutional neural networks (DCNNs), Deep Dream, which when suitably adapted can 
236 
transform panoramic video to mimic hallucinatory phenomenology in a biologically plausible 
237 
manner. The Hallucination Machine enables systematic and parameterizable manipulation 
238 
of distinct aspects of altered states of consciousness (ASCs), specifically visual hallucinations, 
239 
without involving the widespread systemic effects caused by pharmacological manipulations.  
240 
In two experiments we evaluated the effectiveness of this system. Experiment 1 
241 
compared subjective experiences evoked by the Hallucination Machine with those elicited 
242 
by both (unaltered) control videos (within subjects) and by pharmacologically induced 
243 
psychedelic states (across studies). Comparisons between control and Hallucination 
244 
Machine with natural scenes revealed significant differences in perceptual and imagination 
245 
dimensions (‘patterns’, ‘imagery’, ‘strange’, ‘vivid’, and ‘space’) as well as the overall 
246 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
11 
 
intensity and emotional arousal of the experience. Notably, these specific dimensions were 
247 
also reported as being increased after pharmacological administration of psilocybin 31. 
248 
Experiment 1 therefore showed that hallucination-like panoramic video presented within an 
249 
immersive VR environment gave rise to subjective experiences that displayed marked 
250 
similarities across multiple dimensions to actual psychedelic states 31. Although we were not 
251 
able to directly compare the Hallucination Machine experiences to pharmacologically 
252 
induced psychedelic experiences in the same subjects, the pattern of findings in Experiment 
253 
1 support the conclusion that the Hallucination Machine successfully simulates many 
254 
aspects of ASC induced by psychedelic drugs.  
255 
Experiment 2 tested whether participants’ perceptual and subjective ratings of the 
256 
passage of time were influenced during simulated hallucinations, this was motivated by 
257 
subjective reports of temporal distortion during ASC 5,6. In contrast to these earlier findings, 
258 
neither objective measures (using a temporal production task) nor subjective ratings 
259 
(retrospective judgements of duration and speed, Q1 and Q2 in Figure 4) showed significant 
260 
differences between the simulated hallucination and control conditions. This suggests that 
261 
experiencing hallucination-like phenomenology is not sufficient to induce temporal 
262 
distortions, raising the possibility that temporal distortions reported in pharmacologically 
263 
induced ASC may depend on more general systemic effects of psychedelic compounds.  
264 
A crucial feature of the Hallucination Machine is that the Deep Dream algorithm 
265 
used to modify the input video is highly parameterizable. Even using a single DCNN trained 
266 
for a specific categorical image classification task, it is possible with Deep Dream to control 
267 
the level of abstraction, strength, and category type of the resulting hallucinatory patterns. 
268 
In the current study, we chose a relatively higher layer and arbitrary category types (i.e. a 
269 
category which appeared most similar to the input image was automatically chosen) in 
270 
order to maximize the chances of creating dramatic, vivid, and complex simulated 
271 
hallucinations. Future extensions could ‘close the loop’ by allowing participants (perhaps 
272 
those with experience of psychedelic or psychopathological hallucinations) to adjust the 
273 
Hallucination Machine parameters in order to more closely match their previous 
274 
experiences. This approach would substantially extend phenomenological analysis based on 
275 
verbal report, and may potentially allow individual ASCs to be related in a highly specific 
276 
manner to altered neuronal computations in perceptual hierarchies.  
277 
Another key feature of the Hallucination Machine is the use of highly immersive 
278 
panoramic video of natural scenes presented in virtual reality (VR). Conventional CGI-based 
279 
VR applications have been developed for analysis or simulation of atypical conscious states 
280 
including psychosis, sensory hypersensitivity, and visual hallucinations 28,29,32–34. However, 
281 
these previous applications all use of CGI imagery, which while sometimes impressively 
282 
realistic, is always noticeably distinct from real-world visual input and is therefore 
283 
suboptimal for investigations of altered visual phenomenology.  Our setup, by contrast, 
284 
utilises panoramic recording of real world environments thereby providing a more 
285 
immersive naturalistic visual experience enabling a much closer approximation to altered 
286 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
12 
 
states of visual phenomenology. In the present study, these advantages outweigh the 
287 
drawbacks of current VR systems that utilise real world environments, notably the inability 
288 
to freely move around or interact with the environment (except via head-movements).  
289 
Besides having potential for non-pharmacological simulation of hallucinogenic 
290 
phenomenology, the Hallucination Machine may shed new light on the neural mechanisms 
291 
underlying physiologically-induced hallucinogenic states. This potential rests on the close 
292 
functional mappings between the architecture of DCNNs like those used here and the 
293 
functional architecture of the primate visual system 35, as well as the equivalences between 
294 
the ‘top-down’ functional flow (back propagation in Deep Dream) of the Deep Dream 
295 
algorithm and the role of top-down signalling in Bayesian or ‘predictive processing’ theories 
296 
of perception36. 
297 
A defining feature of the Deep Dream algorithm is the use of backpropagation to 
298 
alter the input image in order to minimize categorization errors. This process bears intuitive 
299 
similarities to the influence of perceptual predictions within predictive processing accounts 
300 
of perception. In predictive processing theories of visual perception, perceptual content is 
301 
determined by the reciprocal exchange of (top-down) perceptual predictions and (bottom-
302 
up) perceptual predictions errors. The minimisation of perceptual prediction error, across 
303 
multiple hierarchical layers, approximates a process of Bayesian inference such that 
304 
perceptual content corresponds to the brain’s “best guess” of the causes of its sensory input. 
305 
In this framework, hallucinations can be viewed as resulting from imbalances between top-
306 
down perceptual predictions (prior expectations or ‘beliefs’) and bottom-up sensory signals. 
307 
Specifically, excessively strong relative weighting of perceptual priors (perhaps through a 
308 
pathological reduction of sensory input, see (Abbott, Connor, Artes, & Abadi, 2007; Yacoub 
309 
& Ferrucci, 2011)) may overwhelm sensory (prediction error) signals leading to hallucinatory 
310 
perceptions 37–42. 
311 
Close functional and more informal structural correspondences between DCNNs and 
312 
the primate visual system have been previously noted 20,35. Broadly, the responses of 
313 
‘shallow’ layers of a DCNN correspond to the activity of early stages of visual processing, 
314 
while the responses of ‘deep’ layers of DCNN correspond to the activity of later stages of 
315 
visual processing. These findings support the idea that feedforward processing through a 
316 
DCNN recapitulates at least part of the processing relevant to the formation of visual 
317 
percepts in human brains. Critically, although the DCNN architecture (at least as used in this 
318 
study) is purely feedforward, the application of the Deep Dream algorithm approximates, at 
319 
least informally, some aspects of the top-down signalling that is central to predictive 
320 
processing accounts of perception. Specifically, instead of updating network weights via 
321 
backpropagation to reduce classification error (as in DCNN training), Deep Dream alters the 
322 
input image (again via backpropagation) while clamping the activity of a pre-selected DCNN 
323 
layer. The network itself is not altered in this process. Therefore, the result of the Deep 
324 
Dream process can be intuitively understood as the imposition of a strong perceptual prior 
325 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
13 
 
on incoming sensory data, establishing a functional (though not computational) parallel with 
326 
the predictive processing account of perceptual hallucinations given above. 
327 
 
328 
Figure 5. Possible hierarchical contributions to simple and complex visual hallucinations. a. 
329 
Veridical Perception: Balanced bottom-up and top-down contributions from all levels of the 
330 
hierarchy. b. Simple Hallucinations: perceptual content is overly influenced by visual 
331 
predictions at lower network levels, with a reduced influence from lower-level input (grey 
332 
arrow), emphasising features like edges and lines. c. Complex Hallucinations: perceptual 
333 
content is overly influenced by visual predictions at higher network levels, with a reduced 
334 
influence from lower-level input (grey arrow), emphasising complex object-based features. 
335 
What determines the nature of this heterogeneity and shapes its expression in 
336 
specific instances of hallucination?  The content of the visual hallucinations in humans range 
337 
from coloured shapes or patterns (simple visual hallucinations) 7,43, to more well-defined 
338 
recognizable forms such as faces, objects, and scenes (complex visual hallucinations)44,45. As 
339 
already mentioned, the output images of Deep Dream are dramatically altered depending 
340 
on which layer of the network is clamped during the image-alteration process. Fixing higher 
341 
layers tends to produce output similar to more complex hallucinations (Figure 5c, Higher 
342 
Layer, see also Supplemental Video S1), while fixing lower layers tends create output images 
343 
better resembling simpler geometric hallucinations (Figure 5b, Lower layer, see also 
344 
Supplemental Video S2 and S3). These observations, together with the functional and 
345 
structural correspondences between DCNNs and the primate visual hierarchy, is consistent 
346 
with the idea that the content of visual hallucinations in humans may be shaped by the 
347 
specificity with which a particular drug (or pathology) influences activity at different levels 
348 
of processing within the visual hierarchy. Some example scenarios are schematically 
349 
illustrated in Figure 5. In comparison to normal (veridical) perception (Figure 5a), simple 
350 
kaleidoscopic phenomenology  - which is somewhat characteristic of psychedelic states 7,43  - 
351 
could be explained by increased influence of lower layers of the visual system during the 
352 
interpretation of visual input, in the absence of contributions from higher categorical layers 
353 
(Figure 5b). Conversely, complex visual hallucinations could be explained by the over 
354 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
14 
 
emphasis of predictions from higher layers of the visual system, with a reduced influence 
355 
from lower-level input (Figure 5c).  
356 
4.0 Conclusion 
357 
We have described a method for simulating altered visual phenomenology similar to 
358 
visual hallucinations reported in the psychedelic state. Our Hallucination Machine combines 
359 
panoramic video and audio presented within a head-mounted display, with a modified 
360 
version of ‘Deep Dream’ algorithm, which is used to visualize the activity and selectivity of 
361 
layers within DCNNs trained for complex visual classification tasks. In two experiments we 
362 
found that the subjective experiences induced by the Hallucination Machine differed 
363 
significantly from control (non-‘hallucinogenic’) videos, while bearing phenomenological 
364 
similarities to the psychedelic state (following administration of psilocybin). The immersive 
365 
nature of our paradigm, the close correspondence in representational levels between layers 
366 
of DCNNs and the primate visual hierarchy along with the informal similarities between 
367 
DCNNs and biological visual systems, together suggest that the Hallucination Machine is 
368 
capable of simulating biologically plausible and ecologically valid visual hallucinations. In 
369 
addition, the method carries promise for isolating the network basis of specific altered 
370 
visual phenomenological states, such as the differences between simple and complex visual 
371 
hallucinations.  Overall, the Hallucination Machine provides a powerful new tool to 
372 
complement the resurgence of research into altered states of consciousness. 
 
373 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
15 
 
5.0 
Methods 
374 
5.1 
Hallucination Machine 
375 
In brief, the Hallucination Machine was created by applying the Deep Dream 
376 
algorithm to each frame of a pre-recorded panoramic video presented using a HMD (Figure 
377 
1). Participants could freely explore the virtual environment by moving their head, 
378 
experiencing highly immersive dynamic hallucination-like visual scenes.  
379 
5.1.1 Panoramic video and presentation 
380 
The video footage was recorded on the University of Sussex campus using a 
381 
panoramic video camera (Point Grey, Ladybug 3). The frame rate of the video was 16 fps at 
382 
a resolution of 4096 x 2048. All video footage was presented using a head mounted display 
383 
(Oculus Rift, Development Kit 2) using in-house software developed using Unity3D. 
384 
5.1.2 DCNN specification and application of Deep Dream 
385 
The DCNN – a deeply layered feedforward neural network – used in this study had 
386 
been pre-trained on a thousand categories of natural photographs used in the Large Scale 
387 
Visual Recognition Challenge 2010 (ILSVRC2010) 17,46. During this training procedure, 
388 
features in all layers are learned via backpropagation (with various modifications) to 
389 
associate a set of training images to distinct categories. Consequently, the trained network 
390 
implements a mapping from the pixels of the input image to the categories, represented as 
391 
activation of specific units of the top layer of the network. Given this network, to create the 
392 
panoramic video we applied the Deep Dream algorithm frame-by-frame to the raw video 
393 
footage.  
394 
The Deep Dream algorithm also uses error backpropagation, but instead of updating 
395 
the weights between nodes in the DCNN, it fixes the weights between nodes across the 
396 
entire network and then iteratively updates the input image itself to minimize 
397 
categorization errors via gradient descent. Over multiple iterations this process alters the 
398 
input image, whatever it might be (e.g., a human face), so that it encompasses features that 
399 
the layer of the DCNN has been trained to select (e.g., a dog). When applied while fixing a 
400 
relatively low level of the network, the result is an image emphasizing local geometric 
401 
features of the input. When applied while fixing relatively high levels of the network, the 
402 
result is an image that imposes object-like features on the input, resembling a complex 
403 
hallucination. Examples of the output of Deep Dream used in Experiments 1 and 2 are 
404 
shown in Figure 1.  
405 
Although the original Deep Dream program was intended to process a single static 
406 
image (Mordvintsev, Tyka, et al., 2015), others have developed implementations of this 
407 
algorithm that process image sequences in order to make videos by blending the 
408 
hallucinatory content of the previous frame with the current frame (Roelof, 2015; Samim, 
409 
2015). The principle here is to take a user defined proportion from 0-1 (blending ratio) of 
410 
the previous frame’s hallucinatory patterns (0 = no information, 1 = all information) and 
411 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
16 
 
integrate it into the current frame.  In this way, each frame inherits some of the 
412 
hallucinatory content of the previous frame, as opposed to Deep Dream starting from 
413 
scratch for each frame. This frame-to-frame inheritance enables the hallucinatory patterns 
414 
to remain relatively constant as the video unfolds.  We extended one such implementation 
415 
47 to optimise the hallucinogenic properties of the video. In our extension, the optical flow 
416 
of each frame is calculated by comparing the difference in the movement of all pixels 
417 
between the current and previous frame. The hallucinatory patterns from areas where the 
418 
optical flow was detected is merged to the current (not-yet-hallucinatory) frame based on 
419 
the weighting provided by the blending ratio. The Deep Dream algorithm is then applied to 
420 
this merged frame. We also optimised the blending ratio between each pair of frames, 
421 
setting different blending ratios in areas of the image with high (foreground, moving areas, 
422 
blending ratio of 0.9) or low (background static areas, blending ratio of 0.1) optical flow. This 
423 
was done to avoid saturation of areas of the image with low optical flows by the higher 
424 
blending ratios used for areas with high optical flow. The details of our implementation of 
425 
Deep Dream are provided in the supplemental material. Our software for creating the Deep 
426 
Dream video can be found on GitHub 48. The Deep Dream video used throughout the 
427 
reported experiments was generated by selecting a higher DCNN layer, which responds 
428 
selectively to higher-level categorical features (layers = ‘inception_4d/pool’, octaves = 3, 
429 
octave scale = 1.8, iterations = 32, jitter = 32, zoom = 1, step size = 1.5, blending ratio for 
430 
optical flow = 0.9, blending ratio for background = 0.1). 
431 
 
432 
5.2 Experiment 1: Subjective experience during simulated hallucination 
433 
5.2.1 Participants 
434 
Twelve participants completed Experiment 1 (mean age = 21.1, SD =2.23; 7 female). 
435 
Participants provided informed consent before taking part and received £10 or course 
436 
credits as compensation for their time. All methods were carried out in accordance with 
437 
approved guidelines provided by the University of Sussex, Research Ethics Committee. 
438 
5.2.2 Experimental Design  
439 
Both experiments were performed in a dedicated VR lab. Participants were fitted 
440 
with a head-mounted display before starting the experiment and exposed, in a counter-
441 
balanced manner, to either the Hallucination Machine or the original unaltered (control) 
442 
video footage. Each video presentation lasted 3 minutes and was repeated twice, with a 
443 
180-degree direction flip of the initial orientation between the two presentations 
444 
(presenting the part of the scene that would have been directly behind their viewpoint in 
445 
the first presentation) to help ensure that participants experienced the majority of the 
446 
panoramically-recorded scene. Participants were encouraged to freely investigate the scene 
447 
in a naturalistic manner. While sitting on a stool they could explore the video footage with 
448 
3-degrees of freedom rotational movement. While the video footage is spherical, there is a 
449 
bind spot of approximately 33-degrees located at the bottom of the sphere due to the field 
450 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
17 
 
of view of the camera. After each video, participants were asked to rate their experiences 
451 
for each question via an ASC questionnaire which used a visual analog scale for each 
452 
question (see Figure 2c for questions used).  We used a modified version of an ASC 
453 
questionnaire, which was previously developed to assess the subjective effects of 
454 
intravenous psilocybin in fifteen healthy human participants 31. All data referring to 
455 
Psilocybin was taken from this study31. 
456 
5.2.3 Analysis 
457 
Bayesian paired t-tests were used to compare ASC questionnaire subjective ratings 
458 
between the control condition and the Hallucination Machine, while Bayesian independent 
459 
t-tests were used to compare Hallucination Machine with subjective ratings following 
460 
psilocybin administration (data taken from the original study 31). We quantified how close to 
461 
the null (no difference between results), or to the alternative hypothesis (difference in 
462 
results), each result was using JASP 49 with a default Cauchy prior of .707 half-width at half-
463 
maximum 50. A BF10 > 3.0 is interpreted as evidence for accepting the alternative hypothesis 
464 
(i.e. there is a difference), whereas BF10 < 1/3 is interpreted as evidence for accepting the 
465 
null hypothesis (i.e. there is no difference)51. Standard paired t-test Bonferroni corrected for 
466 
multiple comparisons were also conducted.   
467 
5.3 Experiment 2: Temporal distortion during simulated hallucination 
468 
5.3.1 Participants 
469 
A new group of Twenty-two participants that did not participate in Experiment 1 
470 
completed Experiment 2 (M age =23.9, SD =6.71, 13 female). Participants provided informed 
471 
consent before taking part and received £10 or course credits as compensation for their 
472 
time. All methods were carried out in accordance with approved guidelines provided by the 
473 
University of Sussex, Research Ethics Committee. 
474 
5.3.2 Experimental Design 
475 
The experiment began with a practice session of a standard temporal production 
476 
task. In each of 20 trials, participants heard one of three tones, each of a different pitch 
477 
(low: 220Hz, middle: 440Hz, and high: 1760Hz, each lasting 250 milliseconds). On each trial 
478 
the pitch was randomly selected. Participants were asked to produce specific time intervals 
479 
for each tone (1 second for low, 2 seconds for middle and 4 seconds for the high pitch 
480 
tone)52,53. Participants were instructed to respond immediately after the tone had ceased by 
481 
holding the left mouse button down for the target time interval for each specific tone 
482 
(Figure 6). After producing the interval, they were shown both their produced interval, and 
483 
the target interval, as two dots on a one-dimensional scale, as well as a numeric 
484 
representation (e.g. produced interval “2.4 seconds”, target interval “2.0 seconds”). The 
485 
average numbers of tones per practice session was 6.12 (SD = 1.96) Low, 6.54 (SD = 1.61) 
486 
Middle, and 6.33 (SD = 2.18) High. Participants had to repeat the practice session if the 
487 
Pearson’s correlation between the target and produced intervals was less than 0.5.  
488 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
18 
 
Once the practice was finished, participants began the experimental session. This 
489 
consisted of 12 blocks.  In each block a panoramic video was shown; either the control video 
490 
(6 blocks) or the Hallucination Machine (6 blocks), and similar to Experiment 1, participants 
491 
were instructed to explore the scene freely in a naturalistic manner. The order of the videos 
492 
was counter-balanced across participants. Each block lasted 3 minutes, leading to a total 
493 
exposure of 18 minutes for each video type. While participants explored the immersive 
494 
video, low, middle or high pitch tones were presented in a random order (the average 
495 
numbers of tones per block were 6.17 (SD = 2.02) Low, 6.16 (SD = 2.00) Middle, 6.21 (SD = 
496 
1.94) High for Hallucination Machine, and 6.62 (SD = 2.44) Low, 6.63 (SD = 2.42) Middle, and 
497 
6.61 (SD = 2.55) High for the control video). Immediately after hearing the tone, participants 
498 
had to produce the interval relating to the tone (one second, two seconds, or four seconds) 
499 
(Figure 6). Following the participant’s response there was a random inter-trial interval of 
500 
between 2 and 4 seconds (uniformly distributed). After each block, participants answered 
501 
six questions about their experiences during the video (Figure 4). The questions were 
502 
presented inside the head mounted display and participants responded to the questions 
503 
using a mouse to indicate a value on a visual analog scale.  
504 
 
505 
Figure 6. Experiment 2 temporal production task structure. While viewing either panoramic 
506 
Hallucination Machine or control videos, participants were asked to produce one of three 
507 
specific time intervals. Each time interval had been associated with a differing pitch tone 
508 
during a practice session (1 second for low, 2 seconds for middle and 4 seconds for the high 
509 
pitch tone). Participants responded immediately after the tone had ceased by holding the 
510 
left mouse button down for the target time interval for each specific tone. After the button 
511 
was released there was an inter-trial interval of between 2-4 seconds. 
512 
5.3.3 Analysis 
513 
A Bayesian two-factorial repeated measures ANOVA consisting of the factors interval 
514 
production [1s, 2s, 4s] and video type (control/Hallucination Machine) was used to 
515 
investigate the effect of video type on interval production. A standard two-factorial 
516 
repeated measures ANOVA using the same factors as above was also conducted. 
517 
A two-factorial repeated measures ANOVA consisting of the factors interval 
518 
production [1s, 2s, 4s] and video type (control/Hallucination Machine) was used to 
519 
investigate the effect of video type on interval production. Similar to Experiment 1, for cases 
520 
in which standard statistics did not reveal a significant difference, we quantified how close 
521 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
19 
 
to the null (no difference between results) or alternative hypothesis (difference in results) 
522 
each result was by an additional two-way Bayesian ANOVA using the same factors as above. 
523 
In a similar fashion, for cases in which standard t-tests did not reveal significant differences 
524 
in subjective ratings between video type we used additional Bayesian t-tests. 
525 
 
526 
Data Availability: Video materials used in the study are available in the supplemental 
527 
material. The datasets generated in Experiment 1 and 2 are available from the 
528 
corresponding author upon request. 
529 
 
530 
Acknowledgements: K.S., D.J.S., and A.K.S. are grateful to the Dr. Mortimer and Theresa 
531 
Sackler Foundation, which supports the Sackler Centre for Consciousness Science. W.R. is 
532 
supported by EU FET Proactive grant TIMESTORM: Mind and Time: Investigation of the 
533 
Temporal Traits of Human-Machine Convergence. We would also like to thank Ed Venables 
534 
for his help with data collection and Benjamin Ador for assistance creating the radar plot. 
535 
 
536 
Author Contributions 
537 
K.S., W.R., D.S., A.K.S., conceived and designed the study. K.S. created the materials and 
538 
developed the Hallucination Machine system. K.S. W.R. and D.S. designed and carried out 
539 
the analyses and statistical testing. K.S., D.S., and A.K.S. designed Experiment 1 and 
540 
recorded the data. K.S., W.R., and A.K.S. designed Experiment 2 and recorded the data. K.S., 
541 
W.R., D.S., A.K.S. wrote the manuscript together. 
542 
Additional Information 
543 
Competing Interests: The authors declare no competing financial interests. 
 
544 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
20 
 
6.0 References 
545 
1. 
Oxman, T. E., Rosenberg, S. D., Schnurr, P. P., Tucker, G. J. & Gala, G. The language of 
546 
altered states. J. Nerv. Ment. Dis. 176, 401–8 (1988). 
547 
2. 
Tart, C. T. SCIENTIFIC FOUNDAnONS FOR THE STUDY OF ALTERED STATES OF 
548 
CONSCIOUSNESS. Sci. Found. study altered states Conscious. 3, 93–124 (1972). 
549 
3. 
Bayne, T. & Hohwy, J. Modes of Consciousness. Conscious. after Sev. Brain Damage 
550 
Medical, Leg. Ethical, Philos. Perspect. 1–29 (2007). 
551 
4. 
Carhart-Harris, R. L. et al. Psilocybin with psychological support for treatment-
552 
resistant depression: an open-label feasibility study. The Lancet Psychiatry 3, 619–627 
553 
(2016). 
554 
5. 
Aronson, H., Silverstein, A. B. & Klee, G. D. Influence of Lysergic Acid Diethylamide 
555 
(LSD-25) on Subjective Time. Arch. Gen. Psychiatry 1, 469 (1959). 
556 
6. 
Boardman, W. K., Goldstone, S. & Lhamon, W. T. Effects of lysergic acid diethylamide 
557 
(LSD) on the time sense of normals; a preliminary report. AMA. Arch. Neurol. 
558 
Psychiatry 78, 321–4 (1957). 
559 
7. 
Cowan, J. D. in The Neuroscience of Visual Hallucinations (eds. Collerton, D., 
560 
Mosimann, U. P. & Perry, E.) 219–254 (John Wiley & Sons, Ltd, 2015). 
561 
8. 
Waters, F. et al. Visual Hallucinations in the Psychosis Spectrum and Comparative 
562 
Information From Neurodegenerative Disorders and Eye Disease. Schizophr. Bull. 40, 
563 
S233–S245 (2014). 
564 
9. 
Revonsuo, A., Kallio, S. & Sikka, P. What is an altered state of consciousness? Philos. 
565 
Psychol. 22, 187–204 (2009). 
566 
10. 
Cavanna, A. E., Bagshaw, A. P. & McCorry, D. The Neural Correlates of Altered 
567 
Consciousness During Epileptic Seizures. Discov. Med. 8, 31–36 (2009). 
568 
11. 
Gallimore, A. R. Restructuring consciousness -the psychedelic state in light of 
569 
integrated information theory. Front. Hum. Neurosci. 9, 346 (2015). 
570 
12. 
Carhart-Harris, R. L. et al. The entropic brain: a theory of conscious states informed by 
571 
neuroimaging research with psychedelic drugs. Front. Hum. Neurosci. 8, 20 (2014). 
572 
13. 
Carhart-Harris, R. L. et al. Neural correlates of the psychedelic state as determined by 
573 
fMRI studies with psilocybin. Proc. Natl. Acad. Sci. U. S. A. 109, 2138–43 (2012). 
574 
14. 
McKenna, D. J. Clinical investigations of the therapeutic potential of ayahuasca: 
575 
rationale and regulatory challenges. Pharmacol. Ther. 102, 111–129 (2004). 
576 
15. 
LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nat. Rev. 521, 436–444 (2015). 
577 
16. 
Schmidhuber, J. Deep learning in neural networks: An overview. Neural Networks 61, 
578 
85–117 (2015). 
579 
17. 
Krizhevsky, A., Sulskever, Ii. & Hinton, G. E. ImageNet Classification with Deep 
580 
Convolutional Neural Networks. Adv. Neural Inf. Process. Syst. 1–9 (2012). 
581 
18. 
Szegedy, C. et al. Going Deeper with Convolutions. in Computer Vision and Pattern 
582 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
21 
 
Recognition (CVPR) (2015). 
583 
19. 
Kriegeskorte, N. Deep Neural Networks: A New Framework for Modeling Biological 
584 
Vision and Brain Information Processing. Annu. Rev. Vis. Sci. 1, 417–446 (2015). 
585 
20. 
Yamins, D. L. K. & Dicarlo, J. J. Using goal-driven deep learning models to understand 
586 
sensory cortex. Nat. Neurosci. 19, (2016). 
587 
21. 
Horikawa, T. & Kamitani, Y. Generic decoding of seen and imagined objects using 
588 
hierarchical visual features. Nat. Commun. 8, 15037 (2017). 
589 
22. 
Cadieu, C. F. et al. Deep Neural Networks Rival the Representation of Primate IT 
590 
Cortex for Core Visual Object Recognition. PLoS Comput. Biol. 10, (2014). 
591 
23. 
Khaligh-Razavi, S. M. & Kriegeskorte, N. Deep Supervised, but Not Unsupervised, 
592 
Models May Explain IT Cortical Representation. PLoS Comput. Biol. 10, (2014). 
593 
24. 
Mordvintsev, A., Olah, C. & Tyka, M. Inceptionism: Going Deeper into Neural 
594 
Networks. Google Research Blog (2015). Available at: 
595 
http://googleresearch.blogspot.co.uk/2015/06/inceptionism-going-deeper-into-
596 
neural.html.  
597 
25. 
Mordvintsev, A., Tyka, M. & Olah, C. deepdream (google/deepdream). GitHub 
598 
repository (2015). Available at: https://github.com/google/deepdream.  
599 
26. 
Shanon, B. Ayahuasca Visualizations A Structural Typology. J. Conscious. Stud. 3–30 
600 
(2002). 
601 
27. 
Siegel, R. K. & Jarvik, M. E. in Hallucinations: behavior, experience and theory (eds. 
602 
Siegel, R. K. & West, L. J.) 81–161 (Wiley, 1975). 
603 
28. 
Banks, J. et al. Constructing the hallucinations of psychosis in Virtual Reality. J. Netw. 
604 
Comput. Appl. 27, 1–11 (2004). 
605 
29. 
Yellowlees PM & Cook JN. Education about hallucinations using an internet. Ovid 
606 
Medlin. Psychiatry 30, 534–539 (2006). 
607 
30. 
Dittrich, A. The Standardized Psychometric Assessment of Altered States of 
608 
Consciousness (ASCs) in Humans. Pharmacopsychiatry 31, 80–84 (1998). 
609 
31. 
Muthukumaraswamy, S. D. et al. Broadband Cortical Desynchronization Underlies the 
610 
Human Psychedelic State. J. Neurosci. 33, 15171–15183 (2013). 
611 
32. 
Banks, J. et al. A Virtual Environment to Simulate the Experience of Psychosis. in Proc. 
612 
VIIth Digital Image Computing: Techniques and Applications (ed. Sun C., Talbot H., O. 
613 
S. and A. T.) (2003). 
614 
33. 
Jingyuan, P. O., Zhao, J. & Tennyson, R. Technology Enhanced Learning for People with 
615 
Disabilities: Approaches and Applications. (IGI Global, 2010). 
616 
34. 
Kay, T. Auti-Sim: A playable simulation of sensory hypersensitivity. YouTube Video 
617 
(2013). Available at: https://www.youtube.com/watch?v=DwS-qm8hUxc.  
618 
35. 
Kriegeskorte, N. Deep neural networks : a new framework for modelling biological 
619 
vision and brain information processing. biorxiv (2015). 
620 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
22 
 
doi:http://dx.doi.org/10.1101/029876 
621 
36. 
Whittington, J. & Bogacz, R. Learning in cortical networks through error back-
622 
propagation. bioRxiv (Cold Spring Harbor Labs Journals, 2015). doi:10.1101/035451 
623 
37. 
Teufel, C. et al. Shift toward prior knowledge confers a perceptual advantage in early 
624 
psychosis and psychosis-prone healthy individuals. Proc. Natl. Acad. Sci. U. S. A. 112, 
625 
13401–6 (2015). 
626 
38. 
Corlett, P. R., Frith, C. D. & Fletcher, P. C. From drugs to deprivation: a Bayesian 
627 
framework for understanding models of psychosis. Psychopharmacology (Berl). 206, 
628 
515–30 (2009). 
629 
39. 
De Ridder, D., Vanneste, S. & Freeman, W. The Bayesian brain: Phantom percepts 
630 
resolve sensory uncertainty. Neurosci. Biobehav. Rev. 44, 4–15 (2014). 
631 
40. 
Friston, K. J. Hallucinations and perceptual inference. Behav. Brain Sci. 28, 764–766 
632 
(2005). 
633 
41. 
Reichert, D. P., Series, P. & Storkey, A. J. Hallucinations in Charles Bonnet Syndrome 
634 
Induced by Homeostasis : a Deep Boltzmann Machine Model. Nips 2020–2028 (2010). 
635 
42. 
Reichert, D. P., Seriès, P. & Storkey, A. J. Charles Bonnet syndrome: evidence for a 
636 
generative model in the cortex? PLoS Comput. Biol. 9, e1003134 (2013). 
637 
43. 
Bressloff, P. C., Cowan, J. D., Golubitsky, M., Thomas, P. J. & Wiener, M. C. Geometric 
638 
visual hallucinations, Euclidean symmetry and the functional architecture of striate 
639 
cortex. Philos. Trans. R. Soc. Lond. B. Biol. Sci. 356, 299–330 (2001). 
640 
44. 
Collerton, D., Perry, E. & McKeith, I. Why people see things that are not there: a novel 
641 
Perception and Attention Deficit model for recurrent complex visual hallucinations. 
642 
Behav. Brain Sci. 28, 737-57-94 (2005). 
643 
45. 
Manford, M. & Andermann, F. Complex visual hallucinations. Clinical and 
644 
neurobiological insights. Brain 121, 1819–1840 (1998). 
645 
46. 
Russakovsky, O. et al. ImageNet Large Scale Visual Recognition Challenge. Int. J. 
646 
Comput. Vis. 115, 211–252 (2015). 
647 
47. 
Samim. DeepDream Animator (samim23/DeepDreamAnim). GitHub repository (2015). 
648 
Available at: https://github.com/samim23/DeepDreamAnim.  
649 
48. 
Ksk-S. DeepDreamVideoOpticalFlow (ksk-S/DeepDreamVideoOpticalFlow). GitHub 
650 
repository (2017). Available at: https://github.com/ksk-
651 
S/DeepDreamVideoOpticalFlow.  
652 
49. 
JASP Team. JASP (Version 0.8.0.0)[Computer software]. (2016). 
653 
50. 
Rouder, J. N., Speckman, P. L., Sun, D., Morey, R. D. & Iverson, G. Bayesian t tests for 
654 
accepting and rejecting the null hypothesis. Psychon. Bull. Rev. 16, 225–237 (2009). 
655 
51. 
Wagenmakers, E.-J. A practical solution to the pervasive problems of p values. 
656 
Psychon. Bull. Rev. 14, 779–804 (2007). 
657 
52. 
Field, D. T. & Groeger, J. A. Temporal interval production and short-term memory. 
658 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
  
 
23 
 
Percept. Psychophys. 66, 808–819 (2004). 
659 
53. 
Fortin, C., Rousseau, R., Bourque, P. & Kirouac, E. Time estimation and concurrent 
660 
nontemporal processing: Specific interference from short-term-memory demands. 
661 
Percept. Psychophys. 53, 536–548 (1993). 
662 
 
663 
.
CC-BY-NC 4.0 International license
It is made available under a 
(which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint
. 
http://dx.doi.org/10.1101/213751
doi: 
bioRxiv preprint first posted online Nov. 3, 2017; 
