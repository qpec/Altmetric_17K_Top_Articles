 Full Terms & Conditions of access and use can be found at
http://www.tandfonline.com/action/journalInformation?journalCode=psai20
Self and Identity
ISSN: 1529-8868 (Print) 1529-8876 (Online) Journal homepage: http://www.tandfonline.com/loi/psai20
The best option illusion in self and social
assessment
David Dunning
To cite this article: David Dunning (2018): The best option illusion in self and social assessment,
Self and Identity, DOI: 10.1080/15298868.2018.1465460
To link to this article:  https://doi.org/10.1080/15298868.2018.1465460
Published online: 24 Apr 2018.
Submit your article to this journal 
View related articles 
View Crossmark data
 Self and IdentIty, 2018
https://doi.org/10.1080/15298868.2018.1465460
The best option illusion in self and social assessment
David Dunning
department of Psychology, University of Michigan, ann arbor, MI, USa
ABSTRACT
I discuss the best option illusion, the tendency for people to select what 
they believe is the most reasonable option when solving problems 
or deciding on a course of action. Such a strategy is straightforward, 
sensible and difficult to quibble with, but occasionally the seemingly 
best option turns out to be anything but—leading to systematic errors 
and problems that must be identified, addressed, and managed. 
Specifically, people are more likely to be surprised in a negative 
direction than a positive one, give themselves positive credit for 
wrong answers, and stick to their answers far more than they should 
after exposure to contrary evidence. In self-judgment, the illusion 
leads to the Dunning-Kruger effect, in which people fail to recognize 
their own incompetence. In social judgment, it leads to the Cassandra 
quandary, in which people fail to identify when another person’s 
competence exceeds their own.
At times, the most straightforward of strategies may serve as the root of pervasive human 
misfortune. It is the most banal observation that explains many vagaries of self and social 
perception.
Consider the following example. In the mid-twentieth century, geologists at Royal Dutch/
Shell began to realize they had a problem. They had a terrific albeit imperfect procedure for 
estimating the amount of oil that might lie under any location they were considering to drill. 
However, after they had won and paid for the drilling rights to those locations, they often 
found much less oil than they expected. At certain locations, they might predict a 40% chance 
of producing oil, but struck oil in exploratory wells in such locations only once or twice out 
of ten tries. Their overvaluation of drilling sites was severe enough to cost the company 
millions of dollars (Russo & Schoemaker, 1992).
Why might the geologists been so consistently disappointed? I would suggest, as many 
others have before, that this overvaluation is a product of the most obvious of human ten-
dencies. That observation is that people, when faced with a problem, overwhelmingly tend 
to choose the option that they think is the best one available, that is, the alternative that 
seems the most correct or which holds optimal value. The geologists of Royal Dutch/Shell 
© 2018 Informa UK limited, trading as taylor & francis Group
KEYWORDS
Self-assessment; confidence; 
overconfidence; dunning-
Kruger effect; social 
judgment
ARTICLE HISTORY
Received 17 September 2017 
accepted 11 april 2018
Published online 24 apr 2018
CONTACT david dunning 
 ddunning@umich.edu
this article is based on research described in the author’s lifetime achievement award address to the International Self and 
Social Identity at its 2016 annual preconference at San antonio, texas.
§department of Psychology,University of Michigan,ann arbor,MI,USa.
 2 
  
D. DUNNING
obviously followed this dictum, bidding only on those sites where they thought they the 
best chance of striking oil, setting aside other alternatives that did not look so promising.
The best option illusion
However, in selecting only the apparent best sites to drill, they were setting themselves up 
for frequent disappointment along with any success. To be sure, they were likely on average 
to identify good places to drill, but in choosing to bid on only those fields that seemingly 
had the most potential, they were setting up Royal Dutch/Shell for a greater number and 
degree of surprises on the negative side than they were surprises on the positive. There was, 
indeed, money in hills and valleys, but the amount was much less likely to exceed the geol-
ogists optimistic expectations than it was to fall short of them.
This disappointment arises because any identification of an option as the best involves 
two inputs. First, there are the true indications that the option the best. For the geologists, 
these were the correct clues to oil that they picked up on as they examined a location. 
Second, however, are misleading indicators—signs indicating that the option is the best but 
which actually deceive. These signs comprise the error component of judgment, and they 
lead people to expect much more than they actually encounter, such as ample oil in a new 
drilling site. These may be true signs of oil that fail some of the time. Alternatively, they may 
be signs that the geologists believe in, but which were false or overly promising. Or, they 
may be negative signs that the geologists ignored, missed, or misread. Or, the error compo-
nent can come from external pressures the geologists operate under that cause them to 
distort their reading of the evidence of front of them. Perhaps executives, for example, could 
be pressing them to identify promising new fields that the executives can announce in an 
upcoming stockholder’s meeting.
Whatever its source, this error component of choice assured that the ultimate reality that 
Royal Dutch/Shell found would misalign with its expectations, usually in a negative direc-
tion—that its geologists would encounter more unpleasant surprises rather than pleasant 
ones once they drilled. It also means that if the geologists had drilled in some of the locations 
they had previously rejected, they would have found more oil than their calculations would 
have suggested. There, the error component in rejected sites would work in reverse, leading 
to overly assured dismissals of some drilling sites that in reality would have produced a 
reasonable profit. However, having never drilled there, the geologists would never have 
encountered the negative errors inherent in their rejection decisions (Capen, Clapp, & 
Campbell, 1971).
As such, the geologists of Royal Dutch/Shell company fell prey to the best option illusion, 
which I would define as thus: People generally choose what they believe to be the best 
alternative among all those available, at least in the sense that no other option looks superior. 
Often, there judgments will be right and all will be well. But at times these options will turn 
out not to be necessarily the best. As such, choosing the apparent best may, in the main, 
produce more benefits than costs, but one of those costs will be a pervasive vulnerability 
to overconfidence—or worse.
Of course, choosing the best option is an obvious and beneficial way to make decisions. 
I would bet it is a better strategy (most of the time) than just choosing randomly. However, 
it is a strategy that comes with a consequential side effect, in that choosing the best option 
also means choosing the one that opens people up the most to possible disappointment, 
 SELF AND IDENTITY 
 
 3
frustration, or unhappiness. It is more difficult for a best option to exceed expectation than 
it is for it to disappoint. The downside ranges far lower than the upside. At worst, best options 
provide for a downside that can be catastrophic. As such, best options may not maximize 
outcomes as much as people imagine, but might maximize misfortune far more than they 
would ever anticipate. Consider British Prime Minister Neville Chamberlin’s historic decision 
in 1938 to appease Nazi Germany by allowing it to annex lands in nearby Czechoslovakia. 
At the time, it appeared to be the best option available to the public and many politicians, 
not just Chamberlin, but it only fueled Nazi aggressiveness that ultimately led to the tragedy 
of World War II starting just one short year later (Gilbert & Gott, 1967).
The winner’s curse
In the behavioral economic literature, this problem associated with choosing the best is 
known as the winner’s curse (Thaler, 1988), and usually refers to the paradox facing the winner 
at a competitive auction. Auction bidders, such as those bidding to drilling rights to a poten-
tial oil field, calculate their bids, but constrain themselves by how much they think the thing 
they are bidding on is worth. Often, the auction goes not to the richest person but to the 
individual who overestimates the worth of the object at auction. As such, when winners 
take ownership of an object, they find that the profit or joy they receive from it turns out to 
be much less than they anticipated. This holds true for sports teams going all out to draft 
potential players early in their league’s player draft—those teams making the most effort to 
gain such a player are likely those who overestimated the player’s potential the most (Massey 
& Thaler, 2013).
This tendency, common parlance suggests, also holds true for institutions and organiza-
tions as well as individual. Consider elite colleges, who choose students, in part, based on 
terrific scores on college admissions tests, like the SAT. Of course, the scores of the students 
they select combine some part accurate indication of ability with some part lucky positive 
error on the part of those students the day they took the test. As such, the true ability of any 
entering class at an elite college is usually less than the apparent ability suggested by the 
test scores of its entering students. To be sure, using data likes test scores leads to choosing 
good students, but usually students who in the flesh are not as good as their paper scores 
would suggest. Conversely, at the other end of the academic hierarchy, the ability of students 
at less selective colleges usually turns out to be higher than their test scores indicate.
The inevitability of overconfidence
What is true of bidders at an auction is also true of all people as they make choices. The “best 
option” strategy guarantees that people will be overconfident over the long run in the choices 
they make. People choose among the best options they have—and so must imbue those 
choices with some confidence—but those choices are subject to error that can lead them 
to be far from the best. As such, overconfidence is inevitable. There is a floor to how much 
doubt people can assign to a choice they make, in that it makes no sense to be less confident 
in the choice made than in the choices rejected. Indeed, in one study, we checked, and found 
that 95% of the time participants chose as the best option the alternative they were the 
most confident in, in that there was no other option they thought more likely to be right. A 
 4 
  
D. DUNNING
full 79% of participants uniformly chose the option they imbued with the most confidence 
(Dunning, 2018b).
To be sure, there are times when people fail to choose the best option, but those times 
comprise special cases. There may be times when people know that they have little idea 
about the best option to take, and so they guess and know they are guessing. The research 
on overconfidence reveals this: When people are only 50% confident in a choice between 
two options, they tend to be right roughly 50% of the time (Lichtenstein, Fischhoff, & Phillips, 
1982). Or, there may be times when quick intuition leads to one answer and more careful 
analytical calculation leads to another, and people consciously choose the option that con-
tradicts the advice of either their intuition or analysis (Risen, 2016). However, in the main, 
however, people pick the best option, to their knowledge, that they have available.
Psychological research on human knowledge reveals just how easily people can slip into 
a best option illusion, choosing an option that has the appearance but not the reality of 
being the optimal choice. Each human has a remarkable capacity for information that can 
help him or her to stitch together a rationale leading to correct decisions. The human brain 
has a remarkable capacity for expertise. By age 60, the average English speaker knows the 
equivalent of 48,000 dictionary entries 60 (Brysbaert, Stevens, Mandera, & Keuleers, 2016), 
with some estimates ranging as high as 100,000 (Landauer, 1986). People can recall 97% of 
over 600 photographs they have seen before (Shepard, 1973). Beyond that, people carry a 
number of heuristics and metaphors. One estimate asserts that a thirty-five year old can 
carry 1.5 x 109 bits of information rumbled among the neurons in the brain (Landauer, 1986).
As such, all this knowledge, contained in an integrated, flexible, and efficient organic 
database allows people to answer any type of question thrown at them or solve any problem 
(Marsh, Cantor, & Brashier, 2016). They can go beyond the information given to fashion an 
explanation for events, and to achieve an understanding of what to do next. If asked what 
they would take to a picnic on Mars, they can likely answer the question even though they 
have never had a reason to think of the topic before. If they need to improvise a doorstop 
to keep a door open, they know that the full gallon bottle of cherry juice in the refrigerator 
will likely work better than a half-full bag of frozen bagels in the freezer.
However, part of the reason why people can talk themselves into options that only seem 
to be the best also has to do with that astonishing capacity. People can often take the 
knowledge they have and patch together reasonable conclusions that just happen to be 
wrong (Sanchez & Dunning, 2018; Wu & Dunning, 2018). After all, ask and people show equal 
facility explaining why cautious firefighters are more successful than more aggressive peers 
as they do explaining the opposite (Anderson, Lepper, & Ross, 1980). They can also explain 
why city-raised youths make better soldiers than their rural counterparts just as well as they 
can explain the reverse (Lazarsfeld, 1949). They can just as easily explain why four different 
solutions, one correct and three wrong, to the Wason selection task in logical reasoning is 
really the right one (Evans & Wason, 1976).
Taking credit for wrong answers
This may explain one primary consequence of the of the best option illusion: People seem 
to give themselves credit for their wrong answers as well as correct ones. In 2014, I conducted 
a political survey the day after the fall U.S. Congressional mid-term elections that took place 
that November. I asked both liberals and conservatives to answer 16 factual questions (e.g., 
 SELF AND IDENTITY 
 
 5
has the poverty rate gone up during the Obama presidential administration?), the truth of 
which could easily be verified by a simple Internet search. Not surprisingly, conservatives 
endorsed facts friendly to their political viewpoints more than liberals did. And, naturally, 
liberals did the opposite. However, of key interest, roughly 40% of the “facts” each side 
endorsed as true were false (Dunning, 2018a).
That high rate of false answers mattered, in that participants rated themselves as “well-in-
formed” voters to the extent they answered questions correctly. They also rated themselves 
as less informed when they admitted they did not know the answer to a question (controlling 
for number of questions right). However, when they gave a wrong answer to a question (con-
trolling for number right), they gave themselves positive credit in terms of being well-in-
formed—almost as much as they did when they got the right answer. In short, people are 
confident they are knowledgeable even when they appear to base their inferences on mistaken 
facts and erroneous opinions produced by faulty reasoning (Dunning, 2018a; see also  Dunlosky, 
Hartwig, Rawson, & Lipko, 2011; Dunlosky & Lipko, 2007; Koriat, 2008; van Loon, de Bruin, van 
Gog, & van Merriënboer, 2013; Williams, Dunning, & Kruger, 2013, for similar findings).
The stickiness of best options
This genius in being able to construct a rationale for answers, both right and wrong, might 
also explain another way in which people demonstrate inappropriate faith in their judgments 
and conclusions. People display habits that exacerbate the best option illusion. Specifically, 
they tend to reject, or at least not consider fully, the potential worth of alternative choices 
when confronted with them (Dunning, 1999; Dunning & McElwee, 1995; Dunning, Meyerowitz, 
& Holzberg, 1989; Dunning, Perie, & Story, 1991; Sheldon, Dunning, & Ames, 2014). People 
move away from their favored conclusions much too little given contrary evidence. In fact, 
if led to believe an idea is their own, people are more intransigent about listening to alter-
native opinions, thinking their own ideas are true (Gregg, Mahadevan, & Sedikides, 2017) 
and that other people’s decisions require more scrutiny (Mata, Fiedler, Ferreira, & Almeida, 
2013). They think of their own opinions as more objective and rational than those of others, 
and that people who disagree with them are either ignorant or misguided (Ross & Ward, 
1996). They overestimate the number of people who agree with them (Ross, Greene, & House, 
1977). They tend to believe that the preferences and beliefs of others will come to align with 
their own in the future, if given enough time (Rogers, Moore, & Norton, 2017). If they can 
modify a decision rule to their liking, even by a little, they are more likely to abide by its 
conclusions (Dietvorst, Simmons, & Massey, 2018). Further, to the extent people believe them-
selves to be an expert, they are more likely to think their own ideas are correct (Critcher & 
Dunning, 2009; Ehrlinger & Dunning, 2003; Ottati, Price, Wilson, & Sumaktoyo, 2015).
People display this undue favoritism in many different corners of the psychological liter-
ature. In the literature on advice, for example, researchers show that people are prone to 
give credence to their own conclusions than they are to the opinions of others. In a para-
digmatic example, people might be asked to estimate the length of the Nile River. After 
doing so, they are given the estimate of another individual and asked if they want to revise 
their own estimate. Time and again, researchers show that respondents would be far more 
accurate if they just averaged their original estimate with the estimate given by the other 
individual. People, however, largely refuse to do so. They frequently budge a little bit in their 
estimates, about 30% of the way, toward the advice the other person gives them, but that 
 6 
  
D. DUNNING
is all. (Harvey & Fischer, 1997; Lim & O’Connor, 1995; Yaniv, 2004; Yaniv & Kleinberger, 2000); 
for a review, see Bonaccio & Dalal, 2006). In over a third of cases, they refuse to budge at all 
(Soll & Larrick, 2009).
The favoritism people show to their own conclusions is more impressively revealed by 
how people respond to others who have more expertise than they do. In one study, mid-
level executives answered statistical questions about their own country, such as the per-
centage of people living in urban areas, or the country of another executive. They then 
exchanged estimates. For their own country, the executive’s naturally leaned on their own 
expertise, in that the most common response, about 40% of the time, was to stick to their 
original estimate. However, when considering questions about the other executive’s country, 
respondents did give the other executive’s judgments some weight. That weight, however, 
was not completely reciprocal, in that the most common tendency was to average their 
original estimate with the one the other executive had given. Executive completely adopted 
the other person’s estimate less than 5% of the time (Soll & Larrick, 2009).
But perhaps such rejection of advice should not be a surprise. In real world circumstances 
of substantial consequence, a surprising number of people reject the advice of experts. In 
2011, roughly half a million medical patients in the United States, around 1 to 2%, checked 
themselves out of hospitals against their doctor’s advice. About 20 percent of patients fail 
to fill first-time prescriptions. Fewer than 2 out of every 3 Americans over 50 carry through 
with recommendations to screen for colon cancer (Koven, 2013). According to the Centers 
for Disease Control and Prevention, 25% of Medicare recipients, around 5 million people, 
fail to take their prescribed blood pressure medication (Brin, 2017). These dismissals of advice 
matter, in that the American College of Preventative Medicine in 2102 found that patient 
noncompliance caused roughly 125,000 deaths per year, and cost the U.S. health system as 
much as $300 billion per annum (Brin, 2017).
The reasons people may refuse medical advice are complex and involve many different 
issues. One common issue, however, is that patients often think they know best. They harbor 
misconceptions about disease or medicine that lead to their noncompliance and dismissal 
of advice from doctors. In one Philadelphia survey, 38% of patients erroneously endorsed 
the belief that exposing lungs to the air would cause tumors to spread. For this reason, 10% 
would refuse lung cancer surgery if a doctor recommended it, and a similar percentage 
would refuse to believe a physician who tried to dispel this belief (Margolis et al., 2003). Older 
adults, particularly those suffering from arthritis, often reject physician advice to exercise 
because they associate the soreness from exercise with injury and physical deterioration, 
rather than as a potential harbinger of physical improvement (Gecht, Connell, Sinacore, & 
Prohaska, 1996; Holden, Nicholls, Young, Hay, & Foster, 2012; Wilcox, Der Ananian, Sharpe, 
Robbins, & Brady, 2005).
In another set of studies, over 90% patients under continuing care for chronic high blood 
pressure stated they could tell when their blood pressure rose or fell, even though their phy-
sicians had instructed them that high blood pressure is the “silent killer” that displays no detect-
able external symptoms. Unfortunately, patients tended to medicate themselves according 
to their intuitions rather than follow their physician’s advice to faithfully take their medication. 
Nearly two-thirds of the patients spontaneously asked the interviewer not to tell their doctor 
that they were setting aside his or her advice (Meyer, Leventhal, & Gutmann, 1985).
In a similar vein, psychological research has long shown that people tend to dismiss the 
value of statistical or algorithmic decision aids, thinking their own human reasoning is superior 
 SELF AND IDENTITY 
 
 7
to any computerized or statistical procedure to reach a decision (Diab, Pui, Yankelevich, & 
Highhouse, 2011; Dietvorst, Simmons, & Massey, 2015; Eastwood, Snook, & Luther, 2012). The 
truth, however, is that such mathematical aids tend to be unfailingly as good their human 
counterparts and often prove to be superior (Grove, Zald, Lebow, Snitz, & Nelson, 2000).
In one classic demonstration of this reluctance to use decision aids, mental health pro-
fessionals were tested on their ability to distinguish clients suffering from psychosis from 
mere neurosis. On average, the clinicians achieved 62% accuracy—lackluster, but better 
than chance. They were then shown a formula that made the same predictions with 70% 
accuracy, and told they could use the formula any way they wished. A group thus armed 
with the formula failed to achieve any overall gain in accuracy (Dawes, Faust, & Meehl, 1989; 
Goldberg, 1968).
Implications for self-judgment
The best option illusion carries straightforward implications for self and social judgment. 
Let us consider first the surprisingly difficult and complex task of self-judgment (Dunning, 
2005, 2015; Dunning, Heath, & Suls, 2004). The best option illusion suggests that people 
generally choose the option that seems to be the most reasonable and correct, by whatever 
logic, including intuition, that got them there. However, at times, that logic will be wrong 
and people will not know it. In particular, people who frequently reach wrong conclusions 
will not be in a position to see how misguided those conclusions are. They will think they 
are doing just fine when they are doing quite poorly.
This condition—confidence in judgment yet incompetence in actual performance—has 
been repeatedly demonstrated in the literature, and is now known as the Dunning-Kruger 
effect (Caputo & Dunning, 2005; Dunning, Johnson, Ehrlinger, & Kruger, 2003; Ehrlinger, 
Johnson, Banner, Dunning, & Kruger, 2008; Hodges, Regehr, & Martin, 2001; Kruger & 
Dunning, 1999). People producing the worst performances have very little insight into how 
bad their performances are (for a review, see Dunning, 2011). This is true of students handing 
in a failing exam in an intermediate-level psychology course (Dunning et al., 2003), medical 
residents achieving the worst grades after their OB/GYN clerkship (Edwards, Kellner, Sistrom, 
& Magyari, 2003), medical laboratory technicians taking a test on everyday lab skills (Haun, 
Zeringue, Leach, & Foley, 2000), students failing a CPR workshop (Vnuk, Owen, & Plummer, 
2006)), bridge club players posting the worst scores over the course of a year (Simons, 2013), 
competitors at a trap-and-skeet competition taking a quiz on firearm care and safety 
(Ehrlinger et al., 2008), chess and poker players doing badly in tournament play (Park & 
Santos-Pinto, 2010), and beginning drivers failing their first driving test (Mynttinen et al., 
2009). In each case, respondents think they were doing just fine when they were doing 
anything but. They dramatically overestimate their performance, often being almost, but 
not quite, as confident in their answers as are counterparts showing true expertise and 
superlative performance.
Behavioral implications
This lack of recognition also reveals itself in behavior and choice. People will forgo insurance 
against poor performance. This tendency was revealed in an experiment Paul Ferraro (2010) 
completed in a university economics class. Before taking an exam, students were asked if 
 8 
  
D. DUNNING
they wanted to “buy” an insurance policy against receiving a bad score. Under Policy A, the 
instructor would deduct 10 points from the student’s exam score, but add 20 points if the 
student scored in the bottom half of the class. Under Policy B, the instructor would deduct 
only 2 points, but would add 4 points if the student scored between the 50th and 75th 
percentile on the exam. If students could anticipate perfectly their future exam score, Policy 
A should have been twice as popular than Policy B, since it would pay off for 50% of students 
versus only 25%. Policy B, however, proved to be the popular one—and only 33% bought a 
policy, suggesting that most thought they would score in the 76th percentile or better. And 
of the 19 students who had scored in the bottom half in all previous exams in the class, only 
4 bought Policy A.
People also forgo advice, even when they need it. In ongoing research, we have given 
quizzes to respondents in areas where they should show some expertise. For example, in 
one study, we ask parents with small children to complete a quiz on household hazards for 
children. Parents who performed poorly on this exam tend to think they are doing well. More 
important, when given a chance to pay a small fee to get some advice on any question on 
the quiz—namely, the chance to how another person has answered the question—top and 
poor performers asked for advice at the same rate, only about 13% of the time. To be sure, 
respondents overall were more likely to ask for advice when they are about to get an item 
wrong than when they leaned toward the right answer, but this tendency does not lead 
poor performers to ask for more advice relative to top performers. Essentially, they seem not 
especially responsive to how often they endorsed wrong answers (Yan & Dunning, 2018).
Caveats
Two notes are necessary about the Dunning-Kruger effect. First, the research on the effect 
is not without its critics. Other researchers have claimed that the effect is nothing more than 
a statistical artifact driven by regression-to-the-mean. That is, if one looks at extreme poor 
performers, their self-perceptions cannot be as extreme simply between the perception of 
ability is not perfectly correlated with the reality of it (Krueger & Mueller, 2002). Across several 
studies, however, we have tested for statistical artifacts and found that they fail to account 
for the effect (Ehrlinger et al., 2008; Kruger & Dunning, 2002; Schlösser, Dunning, Johnson, 
& Kruger, 2013), as well as several findings described in our original report of the effect (see 
Studies 3 and 4, Kruger & Dunning, 1999).
Second, there is an important boundary condition to the effect. The effect arises predom-
inantly in those areas in which the skills or expertise needed to judge whether a conclusion 
is correct is the same as the skills needed to produce the right conclusion in the first place 
(Dunning, 2011; Dunning et al., 2003; Kruger & Dunning, 1999). This congruence is more 
common than one might think. The skills needed to judge the logic of an argument, for 
example, requires the same expertise—namely, knowledge of the rules of logic—as that 
needed to produce a logically sound argument. Judging the parenting skills of another 
person requires that one knows good parenting skills. Other areas of life do not share this 
congruence of judgment and production skills and so are more immune to the Dunning-
Kruger effect. It is relatively easy to know whether one can run fast, or throw a ball accurately, 
or complete an aerial somersault. It is equally easy to tell when one is a slow runner, or cannot 
throw accurately, or complete that somersault. All it takes to spot a poor performance is a 
good pair of eyes or a functioning sense of pain.
 SELF AND IDENTITY 
 
 9
Implications for social judgment
More recent work has turned attention from judgments of the self to those of other people, 
and has documented systematic problems that arise when people try to judge the expertise 
of their peers. The problem is that their own expertise is imperfect, and what they consider 
the “expert” decision can be misguided. This imperfect expertise leads to a bias we refer to 
as the Cassandra quandary, after the princess from Greek mythology who was given the gift 
of true prophecy only to be cursed by Apollo never to be believed by her fellow mortals.
Identifying the best and the brightest
The Cassandra bias is that people often have adequate expertise to accurately recognize 
true incompetence among their peers, in that the competence of anyone who chooses dif-
ferently from the self is suspect. However, people fail to have adequate expertise to reliably 
identify peers who demonstrate superior experience. In short, when highly competent peo-
ple choose differently from the self, those differences are, again, read as potential incompe-
tence when they really reflect the exact opposite (Dunning, 2018c). In sum, people often 
lack the competence necessary to recognize competence or excellence that outstrips their 
own. They fail to have the virtuosity necessary to recognize a true virtuoso. As Sir Arthur 
Conan Doyle put it, in the guise of his famous character Sherlock Holmes, mediocrity rec-
ognizes nothing above itself. As a consequence, the best and the brightest often hide in 
plain sight.
In recent work, we have shown that Sir Arthur Conan Doyle is right. Study participants 
take tests in logical reasoning, numerical reasoning, financial literacy, or chess expertise, for 
example, and then are asked to assess the performances of other people chosen to represent 
gross incompetence to perfect skill. Respondents are relatively good at judging poor per-
formers, overestimating the performance of the worst two performers they see by roughly 
15%. However, they underestimate the top two performers they see by almost twice that—
29% (Dunning & Cone, 2018). The degree of this underestimation was so profound that if 
converted to a metaphorical IQ scale, the very top performer in each study is judged to be 
operating at only an IQ of 104 for that particular skill, when in fact that person operates at 
a skill-specific level near a “genius” IQ of 134.
Participants in these studies also have more difficulty identifying top performers than 
bottom ones. In a study about financial literacy, participants were asked which of their peers 
they would approach for financial advice. Participants chose the person with a perfect score 
on a financial literacy quiz only 29% of the time, whereas they correctly identified the worst 
performer as the one to avoid 43% of the time. In another study, participants were asked to 
spot the worst or the best performer out of a group of three individuals. The skill was “global 
literacy,” and the peers being judged had completed a 12-item quiz on world affairs. 
Participants were quite good at spotting bad performers. When looking over a group in 
which two people scored 5 of the quiz and the last scored only 1, participants accurately 
identified the worst performer roughly 72% of the time. However, when the task was spotting 
the good performer, an entirely different picture emerged. When looking over a group in 
which one person had scored 11 on the quiz and the other two only 7, participants accurately 
identified the best performer only 25% of the time (Dunning & Cone, 2018).
 10 
  
D. DUNNING
Making social comparisons
These misjudgments of others can cycle back to misjudgments of the self, in particular 
judgments of how the skill exhibited by the self compares to that displayed by another 
person. In one study, participants took a quiz on logical reasoning and then were presented 
with quizzes filled out by other people. For each, participants were asked whether they had 
done better on the quiz, the peer had done better, or they were tied. They were given a 
financial incentive for getting the answer to these comparative questions right.
However, once again, people proved much less accurate in their assessments with dealing 
with a superior peer rather than an inferior one. When comparing themselves against the 
worst performer in the bunch, participants were largely right in their comparative judgments. 
A full 84% of participants thought their own performance bettered this peer, with an addi-
tional 7% suggesting only a tie. In reality, the figures were 89 and 6%–rather close to partic-
ipant’s judgments and presenting a strikingly accurate picture (Dunning, 2018b).
This picture of accuracy, however, evaporated almost completely in comparative judg-
ments against a superior peer. This peer had gotten a perfect score on the reasoning quiz, 
a feat matched by only one participant out of 101 in the study. However, 27% of participants 
judged they had beaten the peer and another 27% thought they had tied—a rather erro-
neous and misguided set of assessments (Dunning, 2018b).
But the most ironic set of judgments came from the study done on chess, which examined 
what could happen when the Cassandra quandary runs up against the Dunning-Kruger 
effect. High expert participants, those who did well on a quiz about chess strategy and also 
sported the highest official United States Chess Federation ratings, seemed largely sensible 
in their judgments about peers they could beat. When they looked over a peer who had 
done horribly on the chess quiz, they were absolutely certain they could beat that peer, but 
were only 50–50 about whether they could beat a peer who aced the quiz. Low expert 
participants, however, provided a set of estimates that did not seem so reasonable. They 
were only 60% sure they could beat the worst performing peer, but 70% sure they could 
beat the peer who aced the quiz. In short, they were more confident they could beat a peer 
showing a near grandmaster mind than did other participants who actually knew a good 
deal about chess (Dunning & Cone, 2018).
Concluding remarks
At the beginning of this article, I suggested that sometimes the most pervasive and intran-
sigent of problems in human reasoning may follow from the most obvious of choices, select-
ing the option that appears to be the best one among those that are offered. Obviously, this 
strategy likely leads to the highest rate of making optimal decisions, but choosing the appar-
ent best makes the individual quite vulnerable to psychological mishap when it goes wrong. 
Some likely outcomes are overconfidence, misguided views of self, and an inability to rec-
ognize the most expert of our peers.
To be sure, I do not think I would suggest that people do the opposite—decide upon the 
best option available and then do the reverse, or choose randomly, or not think options 
through at all. I would, however, caution people to recognize the structural problems that 
choosing the best produce for the human condition. People do need to temper their 
 SELF AND IDENTITY 
 
 11
confidence, or augment at times their conclusions via advice from others when the stakes 
are high enough. They also need to become more expert on knowing when to turn to experts.
I would recommend these as best options potentially to follow, although I caution that 
any best option always runs the risk of being a mere illusion.
Acknowledgements
The author thanks his research collaborators for their crucial efforts in the empirical and theoretical 
work this article is based on. The novel statements made herein, however, are those of the author alone.
Disclosure statement
No potential conflict of interest was reported by the author.
References
Anderson, C. A., Lepper, M. R., & Ross, L. (1980). Perseverance of social theories: The role of explanation 
in the persistence of discredited information. Journal of Personality and Social Psychology, 39, 1037–
1049.
Bonaccio, S., & Dalal, R. S. (2006). Advice taking and decision-making: An integrative literature review, 
and implications for the organizational sciences. Organizational Behavior and Human Decision 
Processes, 101, 127–151.
Brin, D. W. (2017, January 17). Why don’t patients follow their doctors’ advice? AAMC News. Retrieved 
September 12, 2017, from https://news.aamc.org/patient-care/article/patients-follow-doctors-
advice-better-outcomes/
Brysbaert, M., Stevens, M., Mandera, P., & Keuleers, E. (2016). How many words do we know? Practical 
estimates of vocabulary size dependent on word definition, the degree of language input and the 
participant’s age. Frontiers in Psychology, 7, 1–11.
Capen, E. C., Clapp, R. V., & Campbell, W. M. (1971). Competitive Bidding in High-Risk Situations. Journal 
of Petroleum Technology, 23, 641–653.
Caputo, D. D., & Dunning, D. (2005). What you don’t know: The role played by errors of omission in 
imperfect self-assessments. Journal of Experimental Social Psychology, 41, 488–505.
Critcher, C. R., & Dunning, D. (2009). How chronic self-views influence (and mislead) self-evaluations 
of performance: Self-views shape bottom-up experiences with the task. Journal of Personality and 
Social Psychology, 97, 931–945.
Dawes, R. M., Faust, D., & Meehl, P. (1989). Clinical versus actuarial judgment. Science, 243, 1668–1674.
Diab, D. L., Pui, S. Y., Yankelevich, M., & Highhouse, S. (2011). Lay Perceptions of Selection Decision Aids 
in US and Non-US Samples. International Journal of Selection and Assessment, 19, 209–216.
Dietvorst, B. J., Simmons, J. P., & Massey, C. (2015). Algorithm aversion: People erroneously avoid 
algoithms after seeing them err. Journal of Expeirmental Psyhology: General, 114, 114–126.
Dietvorst, B. J., Simmons, J. P., & Massey, C. (2018). Overcoming algorithm aversion: People will use 
imperfect algorithms if they can (even slightly) modify them. Management Science, 64, 1155–1170.
Dunlosky, J., & Lipko, A. R. (2007). Metacomprehension: A brief history and how to improve its accuracy. 
Current Directions in Psychological Science, 16, 228–232.
Dunlosky, J., Hartwig, M. K., Rawson, K. A., & Lipko, A. R. (2011). Improving college students’ evaluation 
of text learning using idea-unit standards. Quarterly Journal of Experimental Psychology, 64, 467–484.
Dunning, D. (1999). A newer look: Motivated social cognition and the schematic representation of 
social concepts. Psychological Inquiry, 10, 1–11.
Dunning, D. (2005). Self-insight: Roadblocks and detours on the path to knowing thyself. New York, NY: 
Psychology Press.
 12 
  
D. DUNNING
Dunning, D. (2011). The Dunning-Kruger effect: On being ignorant of one’s own ignorance. In J. Olson 
& M. P. Zanna (Eds.), Advances in experimental social psychology (Vol. 44, pp. 247–296). New York, 
NY: Elsevier.
Dunning, D. (2015). On identifying human capital: Flawed knowledge leads to faulty judgments of 
expertise by individuals and groups. In S. Thye & E. Lawler (Eds.), Advances in Group Processes (Vol. 
32, pp. 149–176). New York, NY: Emerald.
Dunning, D. (2018a). Everyday paralogia: How mistaken beliefs bolster a false sense of expertise 
(Unpublished manuscript). University of Michigan, Ann Arbor, MI.
Dunning, D. (2018b). Social comparison goes awry: How lopsided accuracy in social judgment underlies 
flattering self-assessments (Unpublished manuscript). University of Michigan, Ann Arbor, MI.
Dunning, D. (2018c). The flawed evaluator problem: Explaining lopsided accuracy in social judgments of 
expertise (Unpublished manuscript). University of Michigan, Ann Arbor, MI.
Dunning, D., & Cone, J. (2018). The Cassandra quandary: How flawed expertise prevents people from 
recognizing superior skill and knowledge among their peers (Unpublished manuscript). University of 
Michigan, Ann Arbor, MI.
Dunning, D., Heath, C., & Suls, J. (2004). Flawed self-assessment: Implications for health, education, and 
the workplace. Psychological Science in the Public Interest, 5, 71–106.
Dunning, D., Johnson, K., Ehrlinger, J., & Kruger, J. (2003). Why people fail to recognize their own 
incompetence. Current Directions in Psychological Science, 12, 83–87.
Dunning, D., & McElwee, R. O. (1995). Idiosyncratic trait definitions: Implications for self-description 
and social judgment. Journal of Personality and Social Psychology, 68, 936–946.
Dunning, D., Meyerowitz, J. A., & Holzberg, A. D. (1989). Ambiguity and self-evaluation: The role of 
idiosyncratic trait definitions in self-serving assessments of ability. Journal of Personality and Social 
Psychology, 57, 1082–1090.
Dunning, D., Perie, M., & Story, A. L. (1991). Self-serving prototypes of social categories. Journal of 
Personality and Social Psychology, 61, 957–968.
Eastwood, J., Snook, B., & Luther, K. (2012). What people want from their professionals: Attitudes toward 
decision-making strategies. Journal of Behavioral Decision Making, 25, 458–468.
Edwards, R. K., Kellner, K. R., Sistrom, C. L., & Magyari, E. J. (2003). Medical student self-assessment 
of performance on an obstetrics and gynecology clerkship. American Journal of Obstetrics and 
Gynecology, 188, 1078–1082.
Ehrlinger, J., & Dunning, D. (2003). How chronic self-views influence (and potentially mislead) 
assessments of performance. Journal of Personality and Social Psychology, 84, 5–17.
Ehrlinger, J., Johnson, K., Banner, M., Dunning, D., & Kruger, J. (2008). Why the unskilled are unaware? 
Further explorations of (lack of) self-insight among the incompetent. Organizational Behavior and 
Human Decision Processes, 105, 98–121.
Evans, J. St. B. T., & Wason, P. C. (1976). Rationalization in a reasoning task. British Journal of Psychology, 
67, 479–486.
Ferraro, P. J. (2010). Know thyself: Competence and self-awareness. Atlantic Economic Journal, 38, 
183–196.
Gecht, M. R., Connell, K. J., Sinacore, J. M., & Prohaska, T. R. (1996). A survey of exercise beliefs and 
exercise habits among people with arthritis. Arthritis Care & Research, 9, 82–88.
Gilbert, M., & Gott, R. (1967). The appeasers. London: Weidenfeld & Nicolson.
Goldberg, L. R. (1968). Simple models or simple processes? Some research on clinical judgment. 
American Psychologist, 23, 483–496.
Gregg, A. P., Mahadevan, N., & Sedikides, C. (2017). The SPOT effect: People spontaneously prefer their 
own theories. Quarterly Journal of Experimental Psychology, 70, 996–1010.
Grove, W. M., Zald, D. H., Lebow, B. S., Snitz, B. E., & Nelson, C. (2000). Clinical versus mechanical 
prediction: A meta-analysis. Psychological Assessment, 12, 19–30.
Harvey, N., & Fischer, I. (1997). Taking advice: Accepting help, improving judgment, and sharing 
responsibility. Organizational Behavior and Human Decision Processes, 70, 117–133.
Haun, D. E., Zeringue, A., Leach, A., & Foley, A. (2000). Assessing the competence of specimen-processing 
personnel. Laboratory Medicine, 31, 633–637.
 SELF AND IDENTITY 
 
 13
Hodges, B., Regehr, G., & Martin, D. (2001). Difficulties in recognizing one’s own incompetence: Novice 
physicians who are unskilled and unaware of it. Academic Medicine, 76, S87–S89.
Holden, M. A., Nicholls, E. E., Young, J., Hay, E. M., & Foster, N. E. (2012). Role of exercise for knee pain: 
What do adults in the community think? Arthritis Care & Health, 64, 1554–1564.
Koriat, A. (2008). Subjective confidence in one’s answers: The consensuality principle. Journal of 
Experimental Psychology: Learning, Memory, and Cognition, 34(4), 945–959.
Koven, S. (2013, April 22). Why patients don’t always follow doctor’s orders—No matter how serious. 
Boston Globe, G14.
Krueger, J., & Mueller, R. A. (2002). Unskilled, unaware, or both? The contribution of social-perceptual 
skills and statistical regression to self-enhancement biases. Journal of Personality and Social 
Psychology, 82, 180–188.
Kruger, J., & Dunning, D. (1999). Unskilled and unaware of it: How difficulties in recognizing one’s own 
incompetence lead to inflated self-assessments. Journal of Personality and Social Psychology, 77, 
1121–1134.
Kruger, J., & Dunning, D. (2002). Unskilled and unaware—But why? A reply to Krueger and Mueller. 
Journal of Personality and Social Psychology, 82, 189–192.
Landauer, T. K. (1986). How much do people remember? Some estimates of the quantity of learned 
information in long-term memory. Cognitive Science, 10, 477–493.
Lazarsfeld, P. F. (1949). The American soldier—An expository review. Public Opinion Quarterly, 13, 
377–404.
Lichtenstein, S., Fischhoff, B., & Phillips, L. D. (1982). Calibration of probabilities: The state of the art. In D.  
Kahneman, P. Slovic, & A.  Tversky  (Eds.), Judgment under uncertainty: Heuristics and biases (pp.306–
334). Cambridge, UK: Cambridge University Press.
Lim, J. S., & O’Connor, M. (1995). Judgmental adjustment of initial forecasts: Its effectiveness and biases. 
Journal of Behavioral Decision Making, 8, 149–168.
van Loon, M. H., de Bruin, A. B. H., van Gog, T., & van Merriënboer, J. J. G. (2013). Activation of inaccurate 
prior knowledge affects primary-school students’ metacognitive judgments and calibration. Learning 
and Instruction, 24, 15–25.
Margolis, M. L., Christie, J. D., Silvestri, G. A., Kaiser, L., Santiago, S., & Hansen-Flaschen, J. (2003). Racial 
differences pertaining to a belief about lung cancer surgery: Results of a multicenter survey. Annals 
of Internal Medicine, 139, 558–563.
Marsh, E. J., Cantor, A. D., & Brashier, N. M. (2016). Believing that humans swallow spiders in their sleep: 
False beliefs as side effects of the processes that support accurate knowledge. Psychology of Learning 
and Motivation, 64, 93–132.
Massey, C., & Thaler, R. H. (2013). The loser’s curse: Decision making and market efficiency in the national 
football league draft. Management Science, 59, 1479–1495.
Mata, A., Fiedler, K., Ferreira, M. B., & Almeida, T. (2013). Reasoning about others’ reasoning. Journal of 
Experimental Social Psychology, 49, 486–491.
Meyer, D., Leventhal, H., & Gutmann, M. (1985). Common-sense models of illness: Example of 
hypertension. Health Psychology, 4, 115–135.
Mynttinen, S., Sundström, A., Vissers, J., Koivukoski, M., Hakuli, K., & Keskinen, E. (2009). Self-assessed 
driver competence among novice drivers—A comparison of driving test candidate assessments 
and examiner assessments in a Dutch and Finnish sample. Journal of Safety Research, 40, 301–309.
Ottati, V., Price, E. D., Wilson, C., & Sumaktoyo, N. (2015). Self-perceptions of expertise increase closed-
minded cognition: The earned dognmatism effect. Journal of Experimental Social Psychology, 61, 
131–138.
Park, Y.-J., & Santos-Pinto, L. (2010). Overconfidence in tournaments: Evidence from the field. Theory 
and Decision, 69, 143–166.
Risen, J. (2016). Believing what we do not believe: Acquiescence to superstitious beliefs and other 
powerful intuitions. Psychological Review, 123, 182–207.
Rogers, T., Moore, D. A., & Norton, M. I. (2017). The belief in a favorable future. Psychological Science, 
28, 1290–1301.
 14 
  
D. DUNNING
Ross, L., & Ward, A. (1996). Naive realism in everyday life: Implications for social conflict and 
misunderstanding. In T. Brown, E. Reed, & E. Turiel (Eds.), Values and knowledge (pp. 103–135). 
Hillsdale, NJ: Erlbaum.
Ross, L., Greene, D., & House, P. (1977). The “false consensus effect”: An egocentric bias in social 
perception and attribution processes. Journal of Experimental Social Psychology, 13, 279–301.
Russo, J. E., & Schoemaker, P. J. H. (1992). Managing overconfidence. Sloan Management Review, 33, 7–17.
Sanchez, C., & Dunning, D. (2018). Overconfidence among beginners: Is a little learning a dangerous 
thing? Journal of Personality and Social Psychology, 114, 10–28.
Schlösser, T., Dunning, D., Johnson, K., & Kruger, J. (2013). How unaware are the unskilled? Empirical 
tests of the “signal extraction” counterexplanation for the Dunning-Kruger effect in self-evaluations 
of performance. Journal of Economic Psychology, 39, 85–100.
Sheldon, S., Dunning, D., & Ames, D. R. (2014). Emotionally unskilled, unaware, and uninterested in 
learning more: Biased self-assessments of emotional intelligence. Journal of Applied Psychology, 
99, 125–137.
Shepard, R. N. (1973). Learning 10,000 pictures. Quarterly Journal of Expeirmental Psychology, 25, 156–
163.
Simons, D. J. (2013). Unskilled and optimistic: Overconfident predictions despite calibrated knowledge 
of relative skill. Psychonomic Bulletin & Review, 20, 601–607.
Soll, J. B., & Larrick, R. P. (2009). Strategies for revising judgment: How (and how well) people use others’ 
opinions. Journal of Experimental Psychology: Learning, Memory, and Cognition, 35, 780–805.
Thaler, R. H. (1988). The winner’s curse. Journal of Economic Perspectives, 2, 191–202.
Vnuk, A., Owen, H., & Plummer, J. (2006). Assessing proficiency in adult basic life support: Student and 
expert assessment and the impact of video recording. Medical Teacher, 28, 429–434.
Wilcox, S., Der Ananian, C., Sharpe, P. A., Robbins, J., & Brady, T. (2005). Correlates of physical activity 
in persons with arthritis: Review and recommendations. Journal of Physical Activity and Health, 2, 
230–252.
Williams, E. F., Dunning, D., & Kruger, J. (2013). The hobgoblin of consistency: Algorithmic judgment 
strategies underlie inflated self-assessments of performance. Journal of Personality and Social 
Psychology, 104, 976–994.
Wu, K., & Dunning, D. (2018). Hypocognition: Making sense of the world beyond one’s conceptual 
reach. Review of General Psychology, 22, 25–35.
Yan, H., & Dunning, D. (2018). The paradox of advice (Unpublished manuscript). University of Michigan, 
Ann Arbor, MI.
Yaniv, I. (2004). Receiving other people’s advice: Influence and benefit. Organizational Behavior and 
Human Decision Processes, 93, 1–13.
Yaniv, I., & Kleinberger, E. (2000). Advice taking in decision making: Egocentric discounting and 
reputation formation. Organizational Behavior and Human Decision Processes, 83, 260–281.
