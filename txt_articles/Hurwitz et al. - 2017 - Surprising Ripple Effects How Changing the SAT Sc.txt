 Educational Evaluation and Policy Analysis
March 2017, Vol. 39, No. 1, pp. 77 
–103
DOI: 10.3102/0162373716665198
© 2016 AERA. http://eepa.aera.net
a stark contrast exists between the college 
application and subsequent college enrollment 
behavior of lower income students, compared 
with their similarly capable but wealthier peers 
(Hoxby & Avery, 2013; Hoxby & Turner, 2013). 
Many factors contribute to the suboptimal col-
lege-exploration and selection process of lower 
income students, including confusion about 
financial aid availability, poor access to high-
quality college guidance, “micro-barriers” such 
as college application fees, and familial obliga-
tions or cultural norms, which limit the scope of 
colleges that students might consider (Avery & 
Turner, 2010; Radford, 2013; Ross, White, 
Wright, & Knapp, 2013; Smith, Hurwitz, & 
Howell, 2015). In recent years, the College Board 
has implemented a number of policy changes in 
an effort to encourage a broadening of college 
possibilities for low-income students. One policy 
shift, which we investigate here, targeted the 
SAT score-sending behavior of low-income stu-
dents who took the SAT with a fee waiver.
College entrance exam score sending is a col-
lege-exploration step in which students from 
low-income backgrounds may not optimally 
engage. Yet, it is a key component of the college 
search, application, and selection process. 
Among recent cohorts, about 90% of first-time, 
degree-seeking students enrolling at traditional 
BA/BS granting institutions are either required 
665198 EPAXXX10.3102/0162373716665198Surprising Ripple EffectsHurwitz et al.
research-article2016
Surprising Ripple Effects: How Changing the SAT  
Score-Sending Policy for Low-Income Students  
Impacts College Access and Success
Michael Hurwitz
The College Board
Preeya P. Mbekeani
Margaret M. Nipson
Harvard University
Lindsay C. Page
University of Pittsburgh
Subtle policy adjustments can induce relatively large “ripple effects.” We evaluate a College Board 
initiative that increased the number of free SAT score reports available to low-income students and 
changed the time horizon for using these score reports. Using a difference-in-differences analytic 
strategy, we estimate that targeted students were roughly 10 percentage points more likely to send 
eight or more reports. The policy improved on-time college attendance and 6-year bachelor’s com-
pletion by about 2 percentage points. Impacts were realized primarily by students who were com-
petitive candidates for 4-year college admission. The bachelor’s completion impacts are larger than 
would be expected based on the number of students driven by the policy change to enroll in college 
and to shift into more selective colleges. The unexplained portion of the completion effects may result 
from improvements in nonacademic fit between students and the postsecondary institutions in which 
they enroll.
Keywords: college access, college success, SAT policy, difference-in-differences
 Hurwitz et al.
78
or recommended to submit official college 
entrance exam scores with college applications.1 
Likewise, across the cohorts that we examine in 
our analysis, 74% of all on-time college enrollers 
(and 84% of 4-year college enrollers) first 
enrolled in a college to which they sent their SAT 
scores.2 Similarly, more than three quarters of 
sampled on-time enrollers who completed col-
lege sent their SAT scores to the colleges from 
which they graduated. In short, entrance exam 
score sending is a key component of the college 
search. Given this, limited score sending may 
translate into fewer, or fewer successful, college 
applications, in turn decreasing the likelihood of 
prospective college students being admitted to 
any college or to an institution that represents a 
sound match on academic, financial, and other 
dimensions. Importantly, some of these dimen-
sions, such as affordability, may be uncertain at 
the time of college application.
The SAT is a college entrance exam taken by 
about 1.4 million high school graduates in the 50 
states and D.C. annually over the cohorts included 
in this study. Nationally, for the cohorts that we 
examine, nearly half of high school graduates 
take the SAT during their secondary school expe-
rience. For example, approximately 46% of class 
of 2009 students did so.3 The rate of participation 
has risen somewhat in recent years. Among class 
of 2013 graduates, for example, 49% of students 
took the SAT.4 When registering for the SAT, and 
for a short duration after sitting for the exam, stu-
dents can select up to four colleges to receive 
their official SAT score report free of charge. This 
process requires students to know where they 
would like to apply and to be confident enough in 
their exam performance to select colleges without 
knowing whether their scores are high enough to 
meet institution-specific admissions standards. 
Students from low-income backgrounds may 
especially face uncertainty. Nine days after the 
SAT administration and several weeks before stu-
dents receive their scores, this free score-send 
window closes. Students must then pay US$11.25 
per score send.5 Although modest, the cost of 
these additional score sends may nevertheless 
stand as a real barrier to low-income students 
sending scores and applying to an optimal num-
ber and range of postsecondary institutions.
To combat this potential barrier, in the fall of 
2007, the College Board implemented a new 
SAT score-sending policy whereby all low-
income students who took the exam with a fee 
waiver were afforded four additional free and 
flexible score sends to be used at any time during 
their high school careers. This policy shock 
changed the free score-sending opportunities for 
lower income students in two key ways. First, it 
allowed lower income students to send SAT 
scores for free to as many as eight colleges, com-
pared with higher income students who contin-
ued to receive the standard four free score sends. 
Second, this policy changed the time horizon 
over which eligible students could take advan-
tage of the additional four free score sends, 
allowing them to receive and react to their SAT 
performance before selecting institutions to 
which to send their scores.
To understand the impact of such a policy 
shift on the college search, application, and col-
lege-going behavior of students from low-income 
backgrounds, we would ideally conduct an 
experiment. In such an experiment, we would 
identify all low-income students eligible by vir-
tue of registering for the SAT with a fee waiver, 
and then we would randomize the opportunity to 
make use of the flexible score-send policy. Such 
an experimental approach would ensure compa-
rability of treated and control students prior to 
policy assignment, and it would therefore allow 
us to examine the downstream impacts of the 
policy and investigate heterogeneity in those 
impacts by student characteristics, such as SAT 
performance. We hypothesize that this policy 
shock would influence students’ college search 
and application behavior both by encouraging 
students to send scores (and apply) to more 
schools and by encouraging them to apply to 
schools that represented a better fit, at least aca-
demically, given their ability to respond to infor-
mation about their own SAT performance prior to 
score reporting. Furthermore, we hypothesize 
that such shifts in student behavior at the college 
search and application stage could lead to down-
stream improvements in degree completion.
Because the College Board implemented this 
policy shift systematically for an entire cohort of 
SAT takers, we are unable to examine its impact in 
an experimental framework. Instead, we capitalize 
on a difference-in-differences (DID) analytic 
strategy to investigate the policy’s impact on SAT 
score sending, college access, college quality, and 
 79
bachelor’s degree attainment outcomes for low-
income students. Using a standard application of 
the DID estimation strategy, we measure the 
change in each outcome for fee-waiver students 
before and after the introduction of the enhanced 
score-send policy and compare this change with 
that occurring among students who did not use 
SAT fee waivers and therefore were not afforded 
four free and flexible score sends.
Our investigation is informed by and builds 
on the work of Pallais (2015), who examined 
how students changed their score-sending behav-
ior after the ACT, another college entrance exam, 
increased the default number of free score reports 
from three to four (with no change in the timing 
with which such free score reports could be uti-
lized). Pallais finds that this small policy shift 
contributed to increases in score sending as well 
as increases in the range of institutions to which 
students sent scores. Using data on a sample of 
first-year college students collected through the 
American Freshman Survey, Pallais then links 
the change in ACT score-sending policy to a 
jump in selectivity among colleges attended by 
lower income students.
In our study, we match student-level score 
data directly with data from the National Student 
Clearinghouse and trace out each student’s 
unique postsecondary trajectory. This allows us 
to determine whether the flexible score report 
policy actually increased college enrollment 
among low-income students in addition to the 
shifting along intensive margins that Pallais doc-
uments. Finally, we document the substantial 
downstream ripple effects of this seemingly sub-
tle policy shift by revealing the impact of the 
flexible score report policy on college comple-
tion outcomes for low-income SAT takers.
We corroborate Pallais’s (2015) finding that 
students are responsive to the option of sending 
more scores for free. Specifically, we estimate that 
this shift in College Board policy resulted in an 
increase of 0.4 in the number of score reports sent 
by fee-waiver recipients, on average. As we illus-
trate in the left panel of Figure 1, much of this 
change was driven by the fraction of fee-waiver 
recipients sending exactly four score reports falling 
by 10 percentage points and an offsetting 10 per-
centage point increase in the probability of sending 
eight or more score reports. Considering longer 
term impacts, the flexible score report policy also 
increased both on-time college-going and bache-
lor’s completion rates (within 6 years) by 2 and 1.7 
percentage points, respectively. Our estimates are 
FIGURE 1. Distribution of score sends, by fee-waiver use and policy exposure.
 Hurwitz et al.
80
robust to a number of different model specifica-
tions and sample restrictions. Furthermore, as we 
discuss in detail below, we are able to address and 
rule out the threats to validity that commonly affect 
studies using a DID framework as well as threats 
unique to this study’s setting.
The impacts of the flexible score report policy 
on bachelor’s completion are larger than would 
be expected based on the number of students 
enrolling in college and shifting into more selec-
tive colleges as a result of the policy. The unex-
plained portion of the completion effects likely 
results from improvements in nonacademic fit 
between students and the postsecondary institu-
tions in which they enroll that are not easily 
assessed given the data available.
We structure the remainder of the article as 
follows. In the section “Background,” we situate 
our study within the relevant literature and 
describe the College Board policy shift that we 
examine. Then in the section “Research Design,” 
we detail our data and analytic strategy. In the 
next section, we present “Results” and then dis-
cuss “Threats to Validity” in the following sec-
tion. We conclude with a discussion in the 
“Discussion” section.
Background
Whether and where students attend college 
are important drivers for college completion and 
subsequent labor market outcomes (Baum, Ma, 
& Payea, 2013; Card, 1999; Goodman, Hurwitz, 
& Smith, 2015; Howell & Pender, 2016). Despite 
the importance of these decisions, students’ pro-
cesses for college search, application, and selec-
tion can be quite haphazard (Avery, Howell, & 
Page, 2014; Radford, 2013). Even among those 
who do attend college, many undermatch, pri-
marily because they did not apply to any institu-
tions that aligned well with their own academic 
qualifications (Dillon & Smith, 2013; Smith, 
Pender, & Howell, 2013). Although this might be 
surprising at first, in fact, a growing body of 
research in both psychology and behavioral eco-
nomics reveals that individuals often veer from 
decisions that we would predict based on rational 
models of behavior. This is especially so when 
decision makers must parse complex options, 
when they are poorly informed, and when they 
are inexperienced and young (Casey, Jones, & 
Somerville, 2011; Milkman, Beshears, Choi, 
Laibson, & Madrian, 2012; Ross et al, 2013; 
Thaler & Mullainathan, 2008; Thaler & Sunstein, 
2008).
The college search and selection process is 
rife with complex decisions, and students often 
lack appropriate support (Hurwitz & Howell, 
2014; see Page & Scott-Clayton, 2016, for a 
recent review). This complexity, itself, may be a 
barrier to students achieving college access and 
success at greater rates. Given this, greater sup-
port and guidance can improve students’ postsec-
ondary access and success by facilitating their 
decision-making processes (Ross et al., 2013). In 
addition to comprehensive supports, the behav-
ioral economics literature has revealed that even 
small “tweaks” to the process can have meaning-
ful downstream impacts (e.g., Castleman, Owen, 
& Page, 2015; Castleman & Page, 2014, 2015, 
2016; Castleman, Page, & Schooley, 2014; 
Lavecchia, Liu, & Oreopoulos, 2014).
From this growing body of literature, three 
studies are particularly relevant to our research 
focus. First, Smith (2013) utilizes an instrumen-
tal variables (IVs) analytic strategy to examine 
the causal impact of the number of college appli-
cations a student completes on successful college 
enrollment. Capitalizing on exogenous variation 
in the adoption rates of the Common Application 
and the Common Application’s impact on col-
lege application behavior, Smith finds that the 
probability of college enrollment increases when 
students are prompted to submit more applica-
tions. Smith reasons that there are at least two 
potential mechanisms at play in the result that he 
observes. First, additional college applications 
may translate into a higher probability of being 
accepted. Second, additional applications may 
increase students’ chances of being accepted into 
a college that is a good fit along a number of 
dimensions that may be uncertain at the time of 
application (e.g., how much financial aid they 
will be offered or where their friends will be 
attending school). To the extent that SAT score 
sending is an ingredient of successful college 
applications, we hypothesize that we would also 
see increases in college access as a result of addi-
tional college score sends.
This hypothesis is also informed by recent 
work examining the impact of policy shifts in the 
number of free score sends afforded by the ACT 
 Surprising Ripple Effects 
81
(Pallais, 2015). In the late 1990s, the ACT shifted 
the number of free score sends available to test 
takers from three to four. Pallais finds through 
quasi-experimental methods that this seemingly 
minor shift in policy induced students to send 
their scores to more schools and to schools of a 
wider range of quality. She finds that low-income 
students also were more likely to attend more 
selective colleges as a result.
Third, Bond, Bulman, Li, and Smith (2016) 
find that SAT performance is hard to predict. 
Even controlling for prior measures such as 
PSAT performance, residual variation in SAT 
scores is substantial. Furthermore, Bond and col-
leagues observe that student score-sending 
behavior does differ before and after students 
know their SAT scores and that students who per-
form well on the SAT are more aggressive in 
shifting the types of institutions to which they 
send their scores once equipped with information 
on their own performance. Therefore, the ability 
to send scores for free even after knowing about 
one’s own performance may have represented a 
particular opportunity for those eligible students 
who do relatively well or better than potentially 
expected on the SAT. These three studies, 
together, point to the potential for relatively large 
and potentially heterogeneous ripple effects from 
small and inexpensive policy adjustments that 
enhance the college search and selection 
process.
In this study, we capitalize on a policy shift 
that was enacted by the College Board’s Board of 
Trustees in the summer of 2007 to “help low-
income students achieve equity in score report-
ing opportunities.”6 Beginning in the fall of 2007, 
low-income students who took the SAT using a 
fee waiver received four free score reports that 
they could utilize at any point in their high school 
careers, in addition to the standard four free score 
reports that must be used between test registra-
tion and 9 days after sitting for the exam or else 
are forfeited. To qualify for SAT fee waivers, stu-
dents must meet income guidelines specified by 
the National School Lunch Program or they must 
face other unique circumstances, such as living 
in subsidized public housing or enrollment in 
Federal TRIO or Upward Bound programs. 
Students living in foster homes, homeless stu-
dents, orphans, and wards of the state are also 
eligible for SAT fee waivers. Although students 
may take the SAT twice with fee waivers, they 
are only granted one set of four flexible score 
reports.
To investigate the college-related outcomes 
associated with this policy shift, we focus on the 
following three research questions:
Research Question 1: Does the provision of 
four free flexible score reports change the 
number and quality of colleges to which 
low-income students send scores?
Research Question 2: Does the provision of 
four free flexible score reports increase the 
probability of college attendance and shift 
where low-income students enroll?
Research Question 3: Does this policy shift 
have downstream effects on college com-
pletion?
Research Design
Data Sources and Sample
To answer our research questions, we draw on 
two main data sources. Our first source of data is 
student-level microdata from the College Board. 
These data include student SAT scores, the col-
leges to which students sent their score reports, 
and a rich set of demographic data, including 
race/ethnicity and gender and the identity of each 
student’s high school. Beginning with the gradu-
ating cohort of 2007, all of whom graduated from 
high school before the flexible score report intro-
duction, these data also include information on 
whether students took the SAT using a fee waiver. 
For earlier cohorts, we are unable to identify 
individual fee-waiver usage.
We merge these records to data from the 
National Student Clearinghouse (NSC) that pro-
vide semester-level information on whether and 
where students are enrolled in college. NSC data 
allow us to track students into and through col-
lege. These data track students through the sum-
mer of 2015. Approximately 98% of all college 
students enroll in NSC participating institutions, 
although the actual coverage is lower than 98% 
due to imperfect matching and suppression of 
student records (Dynarski, Hemelt, & Hyman, 
2015).
We utilize several rules for inclusion in our 
main analytic sample, and we construct several 
different analytic samples to ensure that our 
 Hurwitz et al.
82
results are consistent and robust to alternate 
specifications. As the vast majority of students 
last take the SAT in either the spring of their high 
school junior year or in the fall of their senior 
year, and as our research design relies on cross-
cohort comparisons, we restrict our sample to 
students who took the SAT at these test adminis-
tration time points. This restriction allows us to 
remove students who may have been categorized 
into the wrong high school cohort. Nearly 90% 
of students in each cohort meet this requirement. 
Next, we limit our sample to students who gradu-
ated from high school prior to or in 2009 for three 
main reasons. First, a central outcome in this 
research is 6-year bachelor’s completion: 
Relatively few students who take the SAT with a 
fee waiver complete college within 4 years. 
Given the NSC data to which we currently have 
access, cohorts graduating after 2009 would be 
excluded from analyses investigating this out-
come. Second, some higher achieving students 
from the 2010, 2011, and 2012 cohorts were 
included in the Expanding College Opportunities 
project (Hoxby & Turner, 2013), which had simi-
lar aims to the flexible score report initiative. To 
avoid possible confounding, we omit these stu-
dents from our sample. Finally, and perhaps more 
importantly, beginning in the spring of 2009, the 
College Board introduced Score Choice, a sec-
ond policy shock that impacted score reporting. 
Under Score Choice, students are afforded the 
option to “put their best foot forward” by strate-
gically sending scores from the SAT administra-
tion on which they performed best. Prior to Score 
Choice, official score reports included test results 
from all SAT administrations. Ultimately, Score 
Choice provided strong incentives for students to 
withhold scores from colleges until they com-
pleted all testing, which means forgoing the use 
of the standard four fee waivers. Without sepa-
rately conducting an analysis of the impacts of 
Score Choice, we risk confounding the impacts 
of this policy with the flexible score reporting 
policy for low-income students, which is our 
focus here. Fortunately, even with the exclusion 
of these later cohorts, we are able to obtain pre-
cise estimates of the impact of the flexible score-
sending policy.
In response to the flexible score report policy, 
it is possible that students who ordinarily would 
not take the SAT with a fee waiver would be 
tempted to seek these waivers more aggressively. 
Shifts in the composition of fee-waiver or non-
fee-waiver students after this policy shock might, 
therefore, bias our results. To address this poten-
tial source of bias, we identify in our sample the 
subset of students from high fee-waiver use high 
schools and the subset from high schools where 
no students utilize fee waivers. In high fee-
waiver use high schools, the majority (or all) of 
SAT test takers were already using fee waivers 
before the new policy, so this particular source of 
bias is minimized. Nevertheless, as we show, our 
results are robust to whether we include in our 
sample all U.S. high school students who take the 
SAT or whether we restrict our examination only 
to students who attended high schools with no 
fee-waiver use or the highest fee-waiver use 
before the implementation of the flexible score 
report policy.
To verify that the differential impacts between 
fee-waiver and non-fee-waiver students that we 
document are due to the flexible score-send pol-
icy rather than an extension of differing preexist-
ing trends between students in these two groups, 
we also incorporate into our analyses students 
from the 2004 through 2006 cohorts. As we are 
unable to identify students as SAT fee-waiver 
recipients in cohorts prior to 2007, for these lon-
ger trend analyses we focus on high schools 
where either all or no students from the 2007 
through 2009 cohorts used SAT fee waivers. In 
these high schools, we assume identical participa-
tion rates for students in the 2004 through 2006 
cohorts. For example, if a student from the 2005 
cohort attended a high school where all SAT test 
takers from the 2007 through 2009 cohorts used a 
fee waiver, he or she was designated as also hav-
ing used a fee waiver in taking the SAT.
Finally, we exclude from our main analytic 
sample students who did not send SAT scores to 
colleges or scholarship organizations. A primary 
justification for this decision is to maintain con-
sistency with Pallais (2015), who makes a similar 
analytic decision in her examination of the shift 
in ACT’s score-sending policy. As Pallais argues, 
students who do not send ACT scores are likely 
to be either students who send scores from the 
alternate college entrance exam or students who 
take a college entrance exam for reasons other 
than college admissions. It is also possible that 
test takers opted to pursue a 2-year, open 
 Surprising Ripple Effects 
83
enrollment institution or a 4-year institution that 
did not require entrance exam scores for admis-
sion. In short, in the ACT context, Pallais’s logic 
for excluding nonscore senders in her main spec-
ifications is that the additional free score send 
only changes student behavior on the intensive, 
not the extensive margin. If a student were never 
planning to send scores, offering him or her the 
opportunity to send more scores should not con-
vert nonscore senders into senders. Although the 
structure of the policy shift we examine is differ-
ent, as we show, our results indeed corroborate 
Pallais’s conclusion that shifts in score sending 
are occurring on the intensive rather than the 
extensive margin. The fraction of students send-
ing at least one score was not influenced by the 
policy shock. Given this result, we deem it appro-
priate to follow Pallais and conduct our analyses 
after conditioning on score sending.
Nevertheless, we present sensitivity analyses 
and placebo tests in which we refit our primary 
models on a sample that includes those who send 
no scores as well as exclusively for the sample of 
nonscore senders. Consistent with Pallais, our 
conclusions are robust to the inclusion of the 
nonscore senders, though the magnitudes of the 
parameter estimates are somewhat smaller, 
which is to be expected as slightly less than a 
quarter of students sent zero scores.
Measures
We focus on outcomes within three domains: 
volume and characteristics of score sending, col-
lege attendance, and college completion. For pur-
poses of comparability, in many instances the 
measures on which we focus are influenced by 
the choices made by Pallais (2015). The college 
characteristics on which we focus include the 
SAT scores for all enrolled students, as reported 
within the Integrated Postsecondary Data System 
(IPEDS) in 2007.7 As was done by Pallais, we 
treat this college-level characteristic as time 
invariant to avoid mistaking changes in student 
SAT performance over time with the policy 
impacts of interest. We additionally consider as 
an outcome the fraction of students enrolling in a 
Most Competitive or Highly Competitive college, 
as defined by the National Center for Education 
Statistics (NCES)–Barron’s admissions competi-
tiveness index data files from 2008.8
With data from the NSC, we construct stu-
dent-level college enrollment and completion 
metrics. Although we create separate outcome 
measures for students enrolling at 2- and 4-year 
institutions, we emphasize that the distinction 
between these two sectors has become increas-
ingly blurry over time. Some colleges histori-
cally considered 2-year institutions have recently 
introduced a small number of bachelor’s degree 
programs, whereas many 4-year institutions also 
offer associate’s degrees (Bidwell, 2014; Radwin 
& Horn, 2014). As relatively few SAT test takers 
earn associate’s degrees, our completion out-
come of primary interest is bachelor’s degree 
attainment. For each cohort, we track students 
for 6 years after high school graduation, except 
for the 2004 high school graduation cohort, 
which we track for 7 years.
Analysis
We obtain all estimates of the impact of the 
flexible score report policy using the DID ana-
lytic framework specified by Equation 1, where 
FeeWvri is an indicator variable specifying 
whether a student took the SAT with a fee waiver, 
postSpring2007i is an indicator specifying 
whether the student last took the SAT after the 
formal rollout of the flexible score report policy, 
SATDatei is a vector of fixed effects for the date 
on which the student last took the SAT, and Xi 
represents a vector of student covariates, includ-
ing race, gender, and SAT scores.9 As was done 
by Pallais (2015), we include fixed effects for the 
student’s high school to ensure that results are 
not driven by disproportionate growth in the 
number of SAT test takers at high schools where 
students 
are 
predisposed 
toward 
certain 
outcomes.
Y
FeeWvr
+
postSpring2007
FeeWvr
+
+
+
i
i
i
i
i
i
=
+
×
α
β
β
β
β
ε
1
2
3
4
SATDate
X
i.
 
(1)
Parameter β2 in Equation 1 represents the unbi-
ased impact of the flexible score report policy on 
outcome, Yi. Considering the potential effects on 
college enrollment and completion, we conjec-
ture that these downstream outcomes are driven 
by increases in score sends that result directly 
from the flexible score report policy shock. We 
utilize identical model specifications for all 
 Hurwitz et al.
84
outcomes. Therefore, where we have binary out-
comes, our results are derived from linear prob-
ability models.
Results
Descriptive Statistics
In Table 1 (columns 1–4), we provide descrip-
tive statistics on score-sending behavior, student 
demographics, SAT score performance, and col-
legiate outcomes, disaggregated by whether a 
student last took the SAT prior to the flexible 
score report rollout and whether he or she used a 
fee waiver to take the SAT. In columns 5 to 8, we 
provide an extended cohort view of these data, 
focusing on the subset of students within schools 
where all students or no students in the 2007 
through 2009 cohorts utilized fee waivers to take 
the SAT. As noted previously, we assume that the 
fee-waiver usage of students from the 2004 
through 2006 cohorts matched that of students 
from the same high schools in the 2007 through 
2009 cohorts.
Unsurprisingly, stark differences exist between 
fee-waiver users and non-fee-waiver users. In 
contrast to non-fee-waiver students, the majority 
of students using fee waivers belong to underrep-
resented minority groups. On the mathematics 
and the critical reading sections of the SAT, fee-
waiver students score nearly 100 points (or a full 
standard deviation) lower than non-fee-waiver 
students. Fee-waiver students are nearly 20 per-
centage points less likely to a attend a 4-year col-
lege on-time (e.g., within 180 days of high school 
graduation); they attend colleges where the aver-
age matriculant performs worse on the SAT and is 
as much as 40 percentage points less likely to 
have earned a bachelor’s degree within 6 years of 
high school completion.
Despite these differences, the number of SAT 
score reports sent by these two groups is fairly 
similar. Among students from the 2007 through 
2009 cohorts who last took the SAT prior to the 
flexible score report rollout (columns 1 and 2), 
the average number of score reports was 5.48 for 
non-fee-waiver students and 5.14 for students 
who took the exam using a fee waiver. After the 
flexible score report policy, the modest gap in 
score sends between these two groups vanished, 
and fee-waiver students modestly edged out non-
fee-waiver students on this metric (columns 3 
and 4). Comparing the score-sending statistics in 
the first four columns provides a preview of what 
we show through statistical models. After the 
policy shock, fee-waiver recipients increased 
their number of score reports by about 0.40 above 
and beyond the increase among non-fee-waiver 
students who were not eligible for the flexible 
score-send policy.10
Impacts on Score Sending
In Table 2, we present results from fitting 
Equation 1 to the entire sample of score senders 
from the 2007 through 2009 cohorts. In addition, 
we present results from restricted subsamples of 
students from high schools with either consis-
tently high fee-waiver use or no fee-waiver use to 
minimize inclusion of any students who pursued 
fee waivers in response to the flexible score 
report policy. In each of the samples, the addition 
of covariates and high school fixed effects mini-
mally changes the parameter estimates. This 
serves as reassuring evidence that the composi-
tion of fee-waiver students and non-fee-waiver 
students are not changing over time in a way that 
might jeopardize the validity of our conclusions.
The parameter estimates in Table 2, Panel A, 
indicate increases in the number of score sends 
from the flexible score report policy of about 0.4. 
Using the baseline data shown in Table 1, this 
increase translates to an 8% increase in score 
sends. Comparison of column 3 with column 12 
reveals that fitted models using the entire sample 
of students generates a parameter estimate of the 
policy’s impact on score sending nearly identical 
to that for the subsample least susceptible to stu-
dents maneuvering into the treatment group (i.e., 
using a fee waiver only to receive the flexible 
score report benefits). Any presence of students 
entering into the treatment group only to receive 
extra free score reports should lead to larger 
parameter estimates in the full sample than in the 
subsample containing only students from consis-
tently 0% and 100% fee-waiver usage schools. 
We find no evidence of such strategizing.
In this table, we also offer some evidence that 
the characteristics of the institutions to which 
students sent their scores changed modestly as a 
result of the flexible score report policy. Panel B 
shows that the maximum average SAT score 
among all score sends increased by up to 20 
 85
TABLE 1
Descriptive Statistics
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
 
2007 to 2009 HS cohorts
2004 to 2009 HS cohorts (only high schools with 0% and 100% 
fee-waiver usage)
 
Last SAT before fall 2007
Last SAT fall 2007 or later
Last SAT before fall 2007
Last SAT fall 2007 or later
 
Fee waiver
No fee waiver
Fee waiver
No fee waiver
Fee waiver
No fee waiver
Fee waiver
No fee waiver
Fraction sending SAT scores
0.717
0.745
0.751
0.779
0.744
0.776
0.727
0.794
Mean number of score sends among senders
5.136
5.483
5.555
5.515
4.894
5.298
5.441
5.563
Male
0.371
0.470
0.379
0.467
0.421
0.470
0.417
0.472
Female
0.627
0.529
0.621
0.533
0.576
0.529
0.583
0.528
Black
0.305
0.067
0.314
0.074
0.352
0.034
0.312
0.037
White
0.154
0.716
0.141
0.692
0.094
0.809
0.067
0.782
Hispanic
0.352
0.076
0.368
0.085
0.387
0.047
0.490
0.059
Asian
0.118
0.092
0.115
0.102
0.042
0.060
0.056
0.077
SAT critical reading
439.332
543.072
442.168
536.245
403.852
566.898
399.746
561.395
SAT math
452.756
553.939
453.633
548.447
406.272
567.408
405.806
563.646
Attended 2-year college on-time
0.203
0.124
0.216
0.135
0.172
0.078
0.217
0.089
Attended 4-year college on-time
0.518
0.733
0.540
0.737
0.421
0.757
0.426
0.761
Average SAT score of college attended
1,063.320
1,140.226
1,059.947
1,129.193
1,038.578
1,162.036
1,038.643
1,152.366
Bachelor’s in 4 years
0.168
0.414
0.179
0.409
0.092
0.439
0.123
0.450
Bachelor’s in 5 years
0.296
0.579
0.313
0.576
0.182
0.602
0.215
0.610
Bachelor’s in 6 years
0.347
0.627
0.374
0.631
0.218
0.648
0.256
0.660
Observations
137,469
1,085,997
260,276
1,352,967
5,628
305,640
3,784
105,828
Note. The “0%” and “100%” fee-waiver high schools were identified as those where 0% and 100% of SAT test takers from the 2007 through 2009 cohorts used fee waivers to cover exam fees. For earlier cohorts, we 
assume that all students from the “100%” fee-waiver high schools received fee waivers and that none of the students from the “0%” fee-waiver high schools received fee waivers. Students from the 2004 cohort are tracked 
through 2011 and all other cohorts are tracked for 6 years. Time to bachelor’s degree receipt is marked by whether a student has earned this degree by August, 4, 5, and 6 years after high school graduation. All descriptive 
statistics, except for those presented in the first row, are for students who sent at least 1 SAT score. On-time college-going is defined as attending a college within 180 days of high school graduation. HS = high school.
 86
TABLE 2
Impacts of the Four Free Flexible Score Sends on Score-Sending Behavior (2007–2009 Cohorts)
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)
 
U.S. high schools
U.S. high schools with consistent 0% 
and 70% + fee-waiver usage
U.S. high schools with consistent 
0% and 80% + fee waiver usage
U.S. high schools with consistent 
0% and 100% fee-waiver usage
A. Dependent variable: Number of score sends
 Post × FeeWaiver
0.459***
0.450***
0.410***
0.283***
0.340***
0.311***
0.259**
0.325***
0.291**
0.293*
0.491***
0.412**
 
(0.054)
(0.054)
(0.060)
(0.096)
(0.092)
(0.103)
(0.108)
(0.105)
(0.119)
(0.166)
(0.166)
(0.189)
 Observations
2,836,709
2,836,709
2,836,709
291,918
291,918
291,918
265,532
265,532
265,532
206,482
206,482
206,482
 R2
.015
.169
.229
.018
.185
.241
.018
.192
.247
.016
.209
.263
 Student covariates
No
No
Yes
No
No
Yes
No
No
Yes
No
No
Yes
 High school fixed effects
No
Yes
Yes
No
Yes
Yes
No
Yes
Yes
No
Yes
Yes
B. Maximum college-reported SAT score (1,600-point scale) among score sends
 Post × FeeWaiver
8.341***
5.856***
1.987**
10.933***
3.358**
–0.426
12.620***
4.829**
0.283
21.294***
11.847**
4.449
 
(1.119)
(0.732)
(0.946)
(2.169)
(1.648)
(1.890)
(2.382)
(2.128)
(2.378)
(5.518)
(5.109)
(5.032)
 Observations
2,751,058
2,751,058
2,751,058
279,555
279,555
279,555
254,503
254,503
254,503
199,775
199,775
199,775
 R2
.024
.160
.373
.036
.216
.379
.033
.224
.393
.023
.255
.450
 Student covariates
No
No
Yes
No
No
Yes
No
No
Yes
No
No
Yes
 High school fixed effects
No
Yes
Yes
No
Yes
Yes
No
Yes
Yes
No
Yes
Yes
C. Range of college-reported SAT score (1,600-point scale) among score sends
 Post × FeeWaiver
7.322***
8.821***
6.946***
5.132**
6.368***
4.478*
5.567**
7.087***
4.844*
16.259*
16.479**
13.414*
 
(1.048)
(1.226)
(1.483)
(2.243)
(1.902)
(2.359)
(2.388)
(2.048)
(2.686)
(8.770)
(6.999)
(7.782)
 Observations
2,751,058
2,751,058
2,751,058
279,555
279,555
279,555
254,503
254,503
254,503
199,775
199,775
199,775
 R2
.021
.092
.152
.053
.147
.193
.050
.147
.194
.012
.130
.185
 Student covariates
No
No
Yes
No
No
Yes
No
No
Yes
No
No
Yes
 High school fixed effects
No
Yes
Yes
No
Yes
Yes
No
Yes
Yes
No
Yes
Yes
Note. The parameter estimates associated with Post × FeeWaiver indicate the impact of the four free flexible policy shock on the outcomes listed in Panels A, B, and C. Standard errors are clustered by state. All regres-
sions include indicator variables for the date of last SAT administration. Student covariates include student race, student gender, and student SAT scores on the math and critical reading sections of the exam. We linked 
college-level data on SAT scores of incoming students from IPEDS 2007 to student score-send data, estimating the college-reported average SAT score as the sum of the 25th and 75th percentiles for math and critical 
reading divided by a factor of 2. Regressions include only those students who sent at least 1 SAT score and who last took the SAT either in the spring of their HS junior year or the fall of their HS senior year. In this 
table, we classify high schools by the consistency of fee-waiver usage. So, for example, to qualify as a 70% + fee-waiver high school, 70% or more of students last taking the SAT during the spring of their junior year 
and during the fall of their senior year must have used SAT fee waivers. IPEDS = Integrated Postsecondary Data System; HS = high school.
*p < .10. **p < .05. ***p < .01.
 Surprising Ripple Effects 
87
points, though the significance and magnitude of 
this estimate are sensitive to both the sample 
selection and covariates incorporated into the 
model. As we favor models that include high 
school fixed effects and a comprehensive set of 
covariates, and estimates from these models are 
of the lowest magnitude, we conclude that the 
evidence in Table 2 does not support strong con-
clusions about shifts in the types of colleges stu-
dents targeted with additional score sends. 
Despite the highly significant estimates that 
some models yield, we argue that even the largest 
point estimates in Panel B are of limited substan-
tive significance. For example, the 20-point dif-
ference in SAT scores referenced above is 
equivalent to the difference in SAT scores 
between matriculating students at Harvard and 
MIT. There exists greater consistency in terms of 
statistical significance in Panel C, yet the changes 
in range between the highest and lowest SAT 
scores within a student’s score-send portfolio are 
of similar magnitudes to those in Panel B and 
similarly lack substantive significance. Our larg-
est estimates in Panel B (approximately 20 SAT 
points) are roughly equivalent to the 0.5 ACT 
point boost that Pallais documents after ACT’s 
increase in the default number of score sends. 
Our estimates on range of SAT scores (Panel C) 
are smaller than the comparable estimates that 
Pallais presents of about 0.9 ACT points.11
In Table 3, we show changes in the distribu-
tion of the number of score reports using the 
2007 to 2009 sample as well as the expanded 
2004 to 2009 cohort sample containing only stu-
dents from 0% and 100% usage high schools.12 
We further divide the sample into SAT terciles 
based on the SAT scores of students in the 100% 
fee-waiver usage schools between 2004 and 
2009. We motivate the discussion of this table by 
graphing the distribution of score sends for fee-
waiver and non-fee-waiver students in both the 
pre- and postpolicy shock periods (Figure 1).
The left panel in Figure 1 depicts a sharp 
decline in the fraction of fee-waiver students 
sending exactly four score reports after the enact-
ment of the flexible fee-waiver policy. This 
decline is primarily offset by a spike in the frac-
tion of students sending exactly 8 score reports, 
with smaller positive changes in the fraction of 
students sending more than 8 score reports. In the 
right-hand panel of Figure 1, we show changes in 
the distribution of score sends among non-fee-
waiver students who did not benefit from the 
flexible score report policy. Among these stu-
dents, there was a considerably smaller decline in 
the share of students sending exactly 4 score 
reports, balanced out by tiny increases in the 
fraction of students sending 5, 6, or 7 score 
reports. This group experienced no change in the 
fraction of students sending 8 score reports. In 
Figure 2, we show that the reshuffling of score-
send counts were somewhat more pronounced 
among those with top tercile SAT performance, 
for example, students with SAT scores at or 
exceeding 880.
The DID parameter estimates in which we 
estimate changes in the distribution of score 
reports due only to the flexible score policy by 
netting out the changes among non-fee-waiver 
students clearly show that about 10% of fee-
waiver students increased the number of score 
sends from four to eight. The declines in the frac-
tion of students sending exactly four reports 
(Table 3, column 5) ranges from about 6 to 13 
percentage points and are offset by increases in 
the fraction of students sending eight or more 
score reports of 7 to 14 percentage points. A smat-
tering of smaller, but highly statistically signifi-
cant negative coefficients also exist for models 
estimating outcomes of exactly 5, 6, and 7 score 
sends, suggesting that even students who were 
not simply using the default 4 score reports took 
advantage of the opportunity to send more scores.
Figures 1 and 2 and Table 3 also reveal that 
the flexible score report policy had considerably 
smaller, but statistically significant, positive, 
impacts on the percentage of students sending 
exactly 1, 2, and 3 scores. The flexible score 
report policy relaxed some of the pressure on stu-
dents to send SAT scores to colleges at the time 
of registration. Some students may have planned 
to send more score reports after learning their 
SAT scores, but opted against such actions after 
discovering that their previous college aspira-
tions were unrealistic. The fact that impacts of 
the highest magnitude are localized among stu-
dents with the lowest SAT scores adds some cre-
dence to this conjecture. Alternatively, some 
students intending to send SAT scores after the 
window for free registration score sends had 
closed may simply have forgotten to follow 
through with their intentions.
 88
Finally, in column 1 of Table 3, we present evi-
dence that the change in the score-sending policy 
had virtually no impact on whether students sent 
any SAT scores, confirming our earlier decision 
to focus on those students who sent any scores 
both before and after the policy shock.
TABLE 3
Impacts on Distribution of Number of Score Sends
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
 
At least 1 
score
1 score
2 scores
3 scores
4 scores
5 scores
6 scores
7 scores
8 or more 
scores
A. All U.S. high schools; cohorts 2007 to 2009
 Post × 
FeeWaiver
−0.002
0.012***
0.009***
0.007***
−0.080***
−0.022***
−0.021***
−0.014***
0.109***
(0.005)
(0.003)
(0.001)
(0.002)
(0.012)
(0.003)
(0.003)
(0.003)
(0.010)
 Observations
3,732,863
2,836,709
2,836,709
2,836,709
2,836,709
2,836,709
2,836,709
2,836,709
2,836,709
 R2
.172
.046
.026
.022
.090
.015
.015
.017
.183
B. All U.S. high schools; cohorts 2007 to 2009 (SAT = 400–720)
 Post × 
FeeWaiver
0.001
0.019***
0.008
0.004
−0.088***
−0.000
−0.016***
−0.009**
0.081***
(0.010)
(0.007)
(0.005)
(0.004)
(0.011)
(0.003)
(0.003)
(0.004)
(0.008)
 Observations
259,195
138,631
138,631
138,631
138,631
138,631
138,631
138,631
138,631
 R2
.191
.182
.117
.105
.143
.092
.088
.083
.137
C. All U.S. high schools; cohorts 2007 to 2009 (SAT = 730–870)
 Post × 
FeeWaiver
−0.005
0.005
0.008***
0.008***
−0.079***
−0.018***
−0.015***
−0.014***
0.105***
(0.007)
(0.005)
(0.002)
(0.002)
(0.016)
(0.003)
(0.004)
(0.003)
(0.011)
 Observations
619,626
378,994
378,994
378,994
378,994
378,994
378,994
378,994
378,994
 R2
.133
.084
.059
.052
.094
.046
.046
.046
.115
D. All U.S. high schools; cohorts 2007 to 2009 (SAT ≥ 880)
 Post × 
FeeWaiver
0.007*
0.006***
0.004***
0.004**
−0.073***
−0.030***
−0.025***
−0.014***
0.128***
(0.004)
(0.002)
(0.001)
(0.002)
(0.014)
(0.004)
(0.004)
(0.003)
(0.010)
 Observations
2,854,042
2,319,084
2,319,084
2,319,084
2,319,084
2,319,084
2,319,084
2,319,084
2,319,084
 R2
.138
.041
.027
.024
.084
.016
.015
.017
.188
E. U.S. high schools with consistent 0% and 100% fee-waiver usage; cohorts 2004 to 2009
 Post × 
FeeWaiver
−0.016
0.033*
−0.006
−0.006
−0.085***
−0.014*
0.000
−0.018**
0.095***
(0.016)
(0.018)
(0.008)
(0.011)
(0.026)
(0.007)
(0.007)
(0.008)
(0.018)
 Observations
539,717
420,880
420,880
420,880
420,880
420,880
420,880
420,880
420,880
 R2
.177
.070
.048
.042
.153
.031
.032
.033
.197
F. U.S. high schools with consistent 0% and 100% fee-waiver usage; cohorts 2004 to 2009 (SAT = 400–720)
 Post × 
FeeWaiver
0.052***
0.050*
−0.027
0.018
−0.134***
−0.002
0.006
0.017
0.072**
(0.018)
(0.026)
(0.022)
(0.019)
(0.035)
(0.012)
(0.018)
(0.012)
(0.034)
 Observations
18,836
10,372
10,372
10,372
10,372
10,372
10,372
10,372
10,372
 R2
.389
.431
.362
.363
.393
.290
.249
.240
.288
G. U.S. high schools with consistent 0% and 100% fee-waiver usage; cohorts 2004 to 2009 (SAT = 730–870)
 Post × 
FeeWaiver
−0.031
0.011
0.011
−0.009
−0.073**
−0.022
0.013
−0.013
0.081***
(0.020)
(0.019)
(0.015)
(0.012)
(0.031)
(0.018)
(0.012)
(0.012)
(0.024)
 Observations
52,909
32,852
32,852
32,852
32,852
32,852
32,852
32,852
32,852
 R2
.247
.235
.211
.206
.283
.169
.164
.154
.229
H. U.S. high schools with consistent 0% and 100% fee-waiver usage; cohorts 2004 to 2009 (SAT ≥ 880)
 Post × 
FeeWaiver
−0.020
0.011
−0.006
−0.011
−0.055*
−0.036***
−0.012
−0.036*
0.144***
(0.021)
(0.013)
(0.010)
(0.009)
(0.029)
(0.012)
(0.011)
(0.018)
(0.027)
 Observations
467,972
377,656
377,656
377,656
377,656
377,656
377,656
377,656
377,656
 R2
.155
.065
.048
.043
.155
.033
.033
.033
.199
Note. The parameter estimates associated with Post × FeeWaiver indicate the impact of the four free flexible policy shock on the outcomes listed 
in the table columns. Standard errors are clustered by state. For cohorts before 2007, we do not have data on fee-waiver usage and instead assign 
all students from the 100% usage high schools as having taken the SAT with fee waivers. We similarly assign students from the 0% usage high 
schools as having taken the SAT without using fee waivers. All regressions include indicator variables for date of last SAT administration, fixed 
effects for the student’s high school, and a vector of student covariates, including student race, student gender, and student SAT scores on the math 
and critical reading sections of the exam. All regressions include only those students who sent at least 1 SAT score (except for column 1) and who 
last took the SAT either in the spring of their HS junior year or the fall of their HS senior year. Terciles are defined based on the students taking 
the SAT from 100% fee-waiver high schools from the 2004 through 2009 HS cohorts. HS = high school.
*p < .10. **p < .05. ***p < .01.
 89
College Enrollment and Completion
In Table 4, we show DID estimates of the 
flexible score report’s impact on college atten-
dance and completion using the overall sample 
(Panel A) and samples restricted only to those 
students least likely to alter their fee-waiver sta-
tus in response to the policy (Panels B–D). All 
four panels convey similar stories. The policy 
shock increased on-time college-going rates, 
defined as enrolling in college within 180 days 
of high school graduation, by 2 to 4 percentage 
points (column 2), and all models (except for 
Panel D) suggest that the majority of this 
increase was driven by students enrolling at 
4-year colleges (column 4). Consistent with the 
previously discussed finding that this policy 
shock had at most a minor effect on the charac-
teristics of the colleges to which students sent 
scores, we find no statistically significant 
changes in the average SAT scores of the col-
leges in which students enrolled (column 5). 
This finding, however, does not allow us to dis-
count the possibility that some students may 
have shifted from institutions with lower to 
higher average SAT scores. Students on the mar-
gin of college attendance whose decisions were 
ultimately tipped by the shift in the score-send 
policy likely enrolled in less selective institu-
tions; this may have muted any effects of other 
students switching into institutions with higher 
average SAT scores. In column 6 of Table 4, we 
show evidence of a small effect on upward shift-
ing, with students one half of a percentage point 
more likely to enroll in colleges in the top two 
Barron’s categories (Most Competitive and 
Highly Competitive).
The final three columns in Table 4 show the 
DID estimates on college completion. In all spec-
ifications, parameter estimates on the percentage 
of students earning bachelor’s degrees with 4, 5, 
or 6 years of high school graduation are positive 
and statistically significant. The magnitude of 
impact on bachelor’s completion within 4 years 
is about 1 to 2 percentage points, within 5 years 
is 1.4 to 3, and within 6 years is 1.7 to 3.5 per-
centage points.
Some of these completion effects are obvi-
ously attributable to the observed increases in 
on-time college attendance presented in column 
2, though the completion impacts are too large to 
be driven only by the influx of students into the 
college pipeline. For the boost in completion to 
FIGURE 2. Distribution of score sends, by tercile of SAT performance, fee-waiver use, and policy exposure.
 90
TABLE 4
Impacts of the Four Free Flexible Score Sends on College Enrollment/Completion (2007–2009 Cohorts)
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
 
Attends college
Attends college 
on-time
Attends 2-year 
college on-time
Attends 4-year 
college on-time
Average SAT of 
college enrolled
Attends top two 
Barron’s category
Earned bachelor’s 
degree in 4 years
Earned bachelor’s 
degree in 5 years
Earned bachelor’s 
degree in 6 years
A. All U.S. high schools
 Post × 
FeeWaiver
0.010***
0.020***
0.007**
0.013***
1.345
0.005***
0.008***
0.014***
0.017***
(0.002)
(0.003)
(0.003)
(0.004)
(0.852)
(0.002)
(0.002)
(0.002)
(0.003)
 Observations
2,836,709
2,836,709
2,836,709
2,836,709
2,030,448
2,836,709
2,836,709
2,836,709
2,836,709
 R2
.056
.086
.143
.185
.483
.237
.202
.200
.188
B. U.S. high schools with consistent 0% and 70% + fee-waiver usage
 Post × 
FeeWaiver
0.023***
0.043***
0.017***
0.026**
1.764
0.005**
0.011***
0.024***
0.032***
(0.006)
(0.010)
(0.004)
(0.010)
(2.334)
(0.002)
(0.003)
(0.005)
(0.007)
 Observations
291,918
291,918
291,918
291,918
194,816
291,918
291,918
291,918
291,918
 R2
.120
.168
.175
.263
.514
.232
.277
.295
.285
C. U.S. high schools with consistent 0% and 80% + fee-waiver usage
 Post × 
FeeWaiver
0.025***
0.043***
0.013**
0.031**
2.289
0.004**
0.016***
0.027***
0.034***
(0.009)
(0.013)
(0.006)
(0.012)
(2.839)
(0.002)
(0.004)
(0.007)
(0.009)
 Observations
265,532
265,532
265,532
265,532
181,606
265,532
265,532
265,532
265,532
 R2
.123
.173
.180
.264
.510
.236
.270
.289
.282
D. U.S. high schools with consistent 0% and 100% fee-waiver usage
 Post × 
FeeWaiver
0.025***
0.044***
0.032**
0.012
9.661
0.006
0.022*
0.030**
0.035***
(0.006)
(0.014)
(0.016)
(0.024)
(8.433)
(0.007)
(0.012)
(0.014)
(0.012)
 Observations
206,482
206,482
206,482
206,482
154,962
206,482
206,482
206,482
206,482
 R2
.138
.182
.200
.234
.490
.244
.218
.222
.217
Note. The parameter estimates associated with Post × FeeWaiver indicate the impact of the four free flexible policy shock on the outcomes listed in the table columns. Standard errors are clustered by state. Each panel 
indicates the restrictions that we specified for sample inclusion based on percentage of SAT test takers at the student’s high school who use fee waivers. These sample selections are intended to remove students from 
schools where the probability of SAT fee-waiver usage is influenced by the policy. All regressions include indicator variables for date of last SAT administration, fixed effects for the student’s high school, and a vector 
of student covariates, including student race, student gender, and student SAT scores on the math and critical reading sections of the exam. We define the average SAT of the enrolled college using IPEDS 2007 data, 
estimating this metric as the sum of the 25th and 75th percentiles for math and critical reading divided by a factor of 2. Regressions include only those students who sent at least 1 SAT score and who last took the SAT 
either in the spring of their HS junior year or the fall of their HS senior year. IPEDS = Integrated Postsecondary Data System; HS = high school.
*p < .10. **p < .05. ***p < .01.
 Surprising Ripple Effects 
91
be determined solely by the shifts on enrollment 
margins, roughly 85% of the students induced to 
enroll would need to complete their bachelor’s 
degrees within 6 years.13 If we made the gener-
ous assumption that induced students entered 
college with an average math and verbal com-
bined SAT score of 880, about 43% would be 
expected to earn bachelor’s degrees within 6 
years.14 As no overwhelming shifts in college 
enrollment characteristics are evident, these sur-
prisingly large completion impacts may reflect 
improvements in college nonacademic fit. With 
more score reports sent to colleges, fee-waiver 
students may benefit from an expanded set of 
college options once they receive college accep-
tances. New college options might include 
affordable alternatives, academic programs that 
are better aligned with the students’ interests, 
locations that students find more palatable, and 
other features of college fit uncertain at the time 
of college application (e.g., Smith, 2013). All of 
these 
factors 
could 
favorably 
influence 
completion.
In Table 5, we disaggregate college atten-
dance and completion by terciles of student SAT 
performance. These results incorporate all stu-
dents from the 2007 through 2009 cohorts, and 
we include Appendix Tables A2 and A3 (avail-
able in the online version of the journal) that 
separately present these parameter estimates for 
the 2004 through 2009 cohorts (0% and 100% 
fee-waiver sample) and again for the 2007 
through 2009 cohorts (0% and 100% fee-waiver 
sample). All results are robust to the alternate 
sample restrictions in these online appendix 
tables.
In the first column of Table 5, we show an 
intriguing relationship between the policy’s 
impact on average number of score reports and 
student academic ability, as measured by SAT 
scores. Most notably, the policy impact tends to 
be larger among students of higher academic 
ability. For students in the 400 to 720 SAT score 
range, the flexible score report policy increased 
the average score reports sent by 0.26. Among 
students with SAT scores at or exceeding 880, the 
impact was more than twice as large. The policy 
induced these relatively higher achieving stu-
dents to send approximately 0.55 more score 
reports to colleges, on average. Appendix Tables 
A2 and A3 (available in the online version of the 
journal) suggest that our main specifications may 
actually represent a lower bound on the differen-
tial policy impact between relatively higher and 
relatively lower achieving students. When we 
restrict the sample to only those high schools 
with consistently 0% or consistently 100% usage, 
we estimate average impacts on volume of score 
reports in the vicinity of 0.9 for relatively higher 
achieving students (e.g., Appendix Table A2, 
Panel D, column 1, available in the online ver-
sion of the journal). This is on par with the intent-
to-treat impact of the Expanding College 
Opportunity intervention on actual college appli-
cations (Hoxby & Turner, 2013, Table 1).
Perhaps unsurprisingly, we find that the over-
all policy impact on 4-year college-going is 
driven entirely by students in the highest SAT 
tercile (Table 5, column 4). These students have 
the academic qualifications for admissibility to at 
least some 4-year institutions that are primarily 
baccalaureate granting. Students with slightly 
lower academic qualifications (SAT scores 
between 730 and 870) would be less compelling 
candidates to 4-year institutions, and so increased 
score sending would be less likely to translate 
into a heightened probability of 4-year college 
attendance for such students.
As with college-going, the positive bachelor’s 
completion effects of the policy are limited only 
to the students with SAT scores at or above 880. 
For these relatively higher achieving students, 
the policy impacts on 4- and 6-year bachelor’s 
completion are 1.5 and 1.9 percentage points, 
respectively, both of which are virtually identical 
to the impact on 4-year college-going. For com-
pletion effects to be driven entirely by the mar-
ginal students drawn into the 4-year college 
pipeline, every single one of these marginal stu-
dents would need to complete their bachelor’s 
degrees within 4 years. This scenario is highly 
unrealistic. Under the generous assumption that 
37% of these relatively high-achieving induced 
students earn a bachelor’s degree in 4 years and 
65% earn a bachelor’s degree in 6 years, about 
40% of the overall completion effects in Table 5 
remain unexplained by the shifting college 
enrollment behavior.15
For the relatively higher achieving students, 
there was no impact on the average institutional 
SAT scores of the colleges where they matricu-
lated; however, there was a modest impact of 0.6 
 92
TABLE 5
Impacts of the Four Free Flexible Score Sends on Score-Sending/Enrollment/Completion (2007–2009 Cohorts), by Student SAT Scores—All High Schools
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
 
Number of 
score sends
Attends college 
on-time
Attends 2-year 
college on-time
Attends 4-year 
college on-time
Average SAT of 
college enrolled
Attends top two 
Barron’s category
Earned bachelor’s 
degree in 4 years
Earned bachelor’s 
degree in 6 years
A. Full range of student SAT scores
 Post × 
FeeWaiver
0.410***
0.020***
0.007**
0.013***
1.345
0.005***
0.008***
0.017***
(0.060)
(0.003)
(0.003)
(0.004)
(0.852)
(0.002)
(0.002)
(0.003)
 Observations
2,836,709
2,836,709
2,836,709
2,836,709
2,030,448
2,836,709
2,836,709
2,836,709
 R2
.229
.086
.143
.185
.483
.237
.202
.188
B. Students scoring in the lowest SAT tercile, defined by SAT fee-waiver recipients (SAT = 400–720)
 Post × 
FeeWaiver
0.257***
−0.001
0.005
−0.006
−5.637*
0.000
−0.006**
−0.006*
(0.073)
(0.007)
(0.007)
(0.006)
(3.238)
(0.000)
(0.002)
(0.003)
 Observations
138,631
138,631
138,631
138,631
33,517
138,631
138,631
138,631
 R2
.187
.178
.154
.176
.439
.149
.135
.149
C. Students scoring in the middle SAT tercile, defined by SAT fee-waiver recipients (SAT = 730–870)
 Post × 
FeeWaiver
0.416***
0.014**
0.012*
0.002
−0.137
0.001
0.002
0.005
(0.076)
(0.006)
(0.006)
(0.008)
(1.140)
(0.001)
(0.002)
(0.003)
 Observations
378,994
378,994
378,994
378,994
176,422
378,994
378,994
378,994
 R2
.145
.101
.132
.147
.259
.055
.102
.109
D. Students scoring in the highest SAT tercile, defined by SAT fee-waiver recipients (SAT ≥ 880)
 Post × 
FeeWaiver
0.548***
0.014***
–0.002
0.017***
1.681
0.006**
0.015***
0.019***
(0.053)
(0.002)
(0.003)
(0.004)
(1.114)
(0.002)
(0.003)
(0.003)
 Observations
2,319,084
2,319,084
2,319,084
2,319,084
1,820,509
2,319,084
2,319,084
2,319,084
 R2
.230
.054
.115
.109
.442
.255
.156
.119
E. SAT = 880–1,290
 Post × 
FeeWaiver
0.564***
0.013***
–0.002
0.015***
1.415
0.003*
0.013***
0.018***
(0.052)
(0.002)
(0.003)
(0.004)
(1.298)
(0.002)
(0.003)
(0.003)
 Observations
1,937,636
1,937,636
1,937,636
1,937,636
1,473,128
1,937,636
1,937,636
1,937,636
 R2
.187
.056
.112
.111
.299
.113
.133
.108
F. SAT ≥ 1,300
 Post × 
FeeWaiver
0.500***
0.025***
−0.012**
0.037***
3.885
0.017**
0.010
0.007
(0.121)
(0.007)
(0.005)
(0.008)
(2.772)
(0.008)
(0.008)
(0.008)
 Observations
381,448
381,448
381,448
381,448
347,381
381,448
381,448
381,448
 R2
.219
.085
.085
.085
.333
.250
.101
.082
Note. The parameter estimates associated with Post × FeeWaiver indicate the impact of the four free flexible policy shock on the outcomes listed in the table columns. Standard errors are clustered by state. All regressions 
include indicator variables for date of last SAT administration, fixed effects for the student’s high school, and a vector of student covariates, including student race, student gender, and student SAT scores on the math 
and critical reading sections of the exam. We define the average SAT of the enrolled college using IPEDS 2007 data, estimating this metric as the sum of the 25th and 75th percentiles for math and critical reading divided 
by a factor of 2. Regressions include only those students who sent at least 1 SAT score and who last took the SAT either in the spring of their HS junior year or the fall of their HS senior year. Terciles are defined based 
on the students taking the SAT from 100% fee-waiver high schools from the 2004 through 2009 HS cohorts. IPEDS = Integrated Postsecondary Data System; HS = high school.
*p < .10. **p < .05. ***p < .01.
 Surprising Ripple Effects 
93
percentage points on the probability of attending 
a college in the top two Barron’s selectivity cat-
egories. Students are substituting more selective 
colleges for less selective ones, but the magni-
tude of this shifting suggests only second-order 
contributions to the increases in bachelor’s com-
pletion rates. To give a concrete example, if 
about 0.6% of students shift upward into colleges 
where the probability of bachelor’s completion 
increases by one third, overall bachelor’s com-
pletion rates would only increase by 0.2 percent-
age points. Instead, for this group of students in 
particular, a substantial fraction of the bachelor’s 
completion impact may be driven by improve-
ments in nonacademic fit.
The Expanding College Opportunities (ECO) 
Project, initiated by Caroline Hoxby and Sarah 
Turner (2013), has provided compelling evi-
dence that very high-achieving low-income stu-
dents engage in suboptimal college search 
processes and that these processes can be 
improved through low-cost nudges, such as the 
provision of free college application fee waivers 
and semicustomized lists of potential colleges to 
consider. These students who have SAT scores—
or the ACT equivalent—of 1,300 (mathematics 
and critical reading combined) or higher would 
be highly desirable candidates for admission at 
virtually all postsecondary institutions across the 
nation. Expanding a student’s college search 
horizon to include more selective colleges, col-
leges that offer more generous financial aid pack-
ages and colleges that better match the student’s 
extracurricular and academic interests, is only 
effective if colleges are willing to enroll the stu-
dent. As the highest achieving students are least 
likely to confront this issue, they stand to gain the 
most from expanded college opportunities.
In the bottom two panels of Table 5, we fur-
ther parse the relatively higher achieving stu-
dents into two groups: higher achieving non-ECO 
eligible students (e.g., those with SAT scores 
between 880 and 1,290) and ECO-eligible stu-
dents with SAT scores at or above 1,300. As rela-
tively few fee-waiver students achieve SAT 
scores in the 1,300+ range, estimates from this 
subgroup are less precise. Nevertheless, Table 5 
demonstrates that ECO-eligible students experi-
enced the largest benefits in terms of college 
enrollment and college choice from the flexible 
score report policy. These very high-achieving 
students experienced a 3.7 percentage point 
increase in the probability of enrolling at a 4-year 
college on-time and a 1.7 percentage point 
increase in the probability of enrolling at college 
in the top two Barron’s categories. As the base-
line 4- and 6-year bachelor’s completion rates for 
these very high-achieving fee-waiver students 
were already higher than their lower achieving 
peers, there is less room for the policy to move 
the needle on these college completion metrics.16 
The completion parameter estimate for these stu-
dents is about 1.4 percentage points, but esti-
mated imprecisely. Collectively, the evidence in 
this table is consistent with Hoxby and Turner’s 
(2013) conclusions that the highest achieving 
low-income students are particularly receptive to 
nudges to improve college search and selection. 
Furthermore, our results show that nudges can 
benefit college-eligible students who are not as 
high performing as the ECO students.
Threats to Validity
We identify several potential threats to the 
validity of our findings and provide additional 
evidence to rule out each threat as an unlikely 
source of bias. The first threat impacts virtually 
all empirical studies that use a DID analytic 
framework. Namely, the integrity of the causal 
interpretation of our results hinges on the 
assumption that trends in all outcomes for the 
non-fee-waiver group serve as accurate counter-
factuals for the trends that would have occurred 
among fee-waiver students had the flexible score 
policy not been implemented. Two major changes 
occurred during this study’s time frame that have 
the potential to disrupt parallel trends between 
the lower income fee-waiver students and their 
better-off peers who took the SAT without a fee 
waiver. First, the economic upheaval caused by 
the Great Recession had the potential to induce 
college-going among individuals who otherwise 
might have entered the labor market after high 
school graduation (Barr & Turner, 2013). A dif-
ferential impact of the Great Recession by stu-
dent socioeconomic status could jeopardize the 
assumption of parallel trends. Second, and 
closely related, enhanced participation in and 
generosity of the federal Pell grant program 
beginning in the 2009–2010 academic year 
(Baum & Payea, 2013) may have boosted college 
 Hurwitz et al.
94
access or retention among the low-income stu-
dents eligible to receive this grant aid.
Regarding the Great Recession, Long 
(2015) shows that the resulting increase in 
postsecondary participation primarily occurred 
among individuals not of traditional college 
age. The economic downturn induced nontra-
ditional students to return to higher education 
and workforce training. These nontraditional 
students are not included in our analyses, thus 
allaying concerns over this particular potential 
threat. Of course, despite compelling evidence 
that, as a whole, traditionally aged college stu-
dents did not shift into higher education as a 
result of the economic downturn, there is still a 
risk that overall patterns might conceal differ-
ing responses between higher income and 
lower income individuals. To investigate this 
further, we recreate Panel A of Table 4 for two 
subsets of students: those residing in states 
with the largest upticks in unemployment dur-
ing the Great Recession and those residing in 
states with the mildest increases in unemploy-
ment. We use the state categorization imple-
mented by Long (2015) to conduct these 
sensitivity analyses.17 If unemployment shocks 
from the Great Recession differentially 
impacted higher income and lower income stu-
dents, we might expect parameter estimates to 
differ between models fitted from students 
residing in high unemployment growth states 
and those fitted to students from low unem-
ployment growth states.
In Panels A and B of Table 6, we show that 
model parameter estimates are nearly identical 
when students are separated into groups based 
upon the state-level severity of the Great 
Recession’s impact on unemployment. In col-
umn 1, we show that the flexible score report 
policy increased total scores sent by 0.48 in the 
high recession impact states and 0.32 in the low 
recession impact states. Both of these estimates 
differ significantly from zero, though they do 
not differ significantly from each other. The 
remaining columns in Panels A and B demon-
strate that parameter estimates on college-
going and completion are similar between the 
two subsamples. Collectively, these results pro-
vide evidence that the simultaneity of the Great 
Recession is not a compelling threat to validity 
in these analyses.
The Pell grant expansion of 2009 to 2010, 
which affected only the poorest of sampled stu-
dents, adds another potential threat to the parallel 
trends assumption between fee-waiver and non-
fee-waiver students. The enhanced generosity of 
the Pell might have induced some students into the 
postsecondary pipeline or it may have increased 
retention among already enrolled sample students. 
Based on the stringent requirements for receipt of 
SAT fee waivers, we reason that most of these stu-
dents are also eligible to receive at least some Pell 
funds, whereas the majority of non-fee-waiver stu-
dents in our counterfactual group would not be eli-
gible for such federal aid.
We have no data on actual household income 
or Pell eligibility, so addressing this threat per-
fectly is not possible. However, the SAT student 
questionnaire does survey students about house-
hold income range. If we trust that students have 
a general sense of the range in which their house-
hold income falls, we can refit our main specifi-
cations conditioning on students self-reporting 
that they fall into income categories correspond-
ing to a high likelihood of Pell receipt (i.e., a 
household income of less than US$40,000 per 
year). Figure 3 offers a preview of these model 
results by illustrating that increases in score 
sending among lower income students using SAT 
fee waivers increased markedly compared with 
lower income students who did not use fee waiv-
ers. Increases in on-time 4-year college-going 
and BA completion among the lowest income 
fee-waiver students were larger than the corre-
sponding increases among the lowest income 
students who took the SAT without fee waivers.
In the bottom two panels of Table 6, we sepa-
rately fit our DID models to students with self-
reported household income less than US$40,000 
(Panel C) and self-reported household income 
less than US$60,000, including those with 
incomes less than US$40,000 (Panel D). As stu-
dents with parental incomes less than US$40,000 
should have benefited from Pell expansion 
regardless of whether they used SAT fee waivers, 
low-income, non-fee-waiver test takers might 
serve as a more persuasive counterfactual if the 
Pell expansion did generate different trends in 
college-going or college choice between low-
income and high-income students.18 The param-
eter estimates on college-going and completion 
are similar in Panels C and D to those throughout 
 95
TABLE 6
Threats to Validity From Great Recession and Pell Expansion (2007–2009 Cohorts)
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
 
Number of 
score sends
Attends 
college
Attends college 
on-time
Attends 2-year 
college on-time
Attends 4-year 
college on-time
Average SAT of 
college enrolled
Attends top two 
Barron’s category
Earned bachelor’s 
degree in 4 years
Earned bachelor’s 
degree in 5 years
Earned bachelor’s 
degree in 6 years
A. High unemployment change states
 Post × 
FeeWaiver
0.480***
0.009***
0.016***
0.005
0.012***
2.285*
0.006***
0.007**
0.016***
0.019***
(0.037)
(0.002)
(0.003)
(0.004)
(0.004)
(1.249)
(0.002)
(0.003)
(0.003)
(0.005)
 Observations
1,339,794
1,339,794
1,339,794
1,339,794
1,339,794
940,203
1,339,794
1,339,794
1,339,794
1,339,794
 R2
.244
.041
.072
.143
.179
.478
.214
.202
.200
.184
B. Low unemployment change states
 Post × 
FeeWaiver
0.328***
0.011***
0.025***
0.009*
0.016***
0.056
0.003
0.009***
0.011***
0.015***
(0.115)
(0.003)
(0.003)
(0.004)
(0.004)
(1.131)
(0.002)
(0.003)
(0.002)
(0.002)
 Observations
1,345,156
1,345,156
1,345,156
1,345,156
1,345,156
978,293
1,345,156
1,345,156
1,345,156
1,345,156
 R2
.219
.071
.101
.138
.196
.484
.261
.203
.206
.198
C. Reported parental income <US$40K
 Post × 
FeeWaiver
0.639***
0.005**
0.008**
−0.002
0.010**
1.367
0.003
0.008***
0.015***
0.017***
(0.050)
(0.002)
(0.004)
(0.003)
(0.005)
(1.144)
(0.002)
(0.003)
(0.003)
(0.003)
 Observations
416,595
416,595
416,595
416,595
416,595
242,595
416,595
416,595
416,595
416,595
 R2
.215
.088
.115
.144
.205
.450
.184
.189
.200
.193
D. Reported parental income <US$60K
 Post × 
FeeWaiver
0.617***
0.005***
0.011***
0.003
0.008*
0.317
0.001
0.008***
0.014***
0.016***
(0.044)
(0.001)
(0.003)
(0.003)
(0.004)
(0.763)
(0.001)
(0.002)
(0.002)
(0.003)
 Observations
689,728
689,728
689,728
689,728
689,728
424,261
689,728
689,728
689,728
689,728
 R2
.205
.072
.099
.142
.196
.431
.171
.184
.189
.181
Note. The parameter estimates associated with Post × FeeWaiver indicate the impact of the four free flexible policy shock on the outcomes listed in the table columns. Standard errors are clustered by state. High unem-
ployment change states include AL, AZ, CA, FL, GA, ID, IL, IN, MI, NJ, NV, OH, OR, RI, SC, TN, UT, and WA. Low unemployment states include AK, CO, CT, D.C., DE, HI, KS, KY, LA, MA, MD, ME, MN, MO, 
NE, NH, NY, OK, PA, TX, VA, WI, and WV. We include D.C. as low unemployment change state based on unemployment data retrieved from BLS (http://data.bls.gov/timeseries/LASST110000000000003?data_
tool=XGtable). All regressions include indicator variables for date of last SAT administration, fixed effects for the student’s high school, and a vector of student covariates, including student race, student gender, and 
student SAT scores on the math and critical reading sections of the exam. We define the average SAT of the enrolled college using IPEDS 2007 data, estimating this metric as the sum of the 25th and 75th percentiles for 
math and critical reading divided by a factor of 2. Regressions include only those students who sent at least 1 SAT score and who last took the SAT either in the spring of their HS junior year or the fall of their HS senior 
year. BLS = Bureau of Labor Statistics; IPEDS = Integrated Postsecondary Data System; HS = high school.
*p < .10. **p < .05. ***p < .01.
 96
the rest of the article, suggesting that Pell grant 
expansion is an unlikely driver of our results.
As a final sensitivity check to the parallel 
trends assumption, we show the results of models 
fitted to the subset of students who did not utilize 
any score sends. Recall from Table 3 that the 
flexible score report policy did not shift students 
from nonsenders to score senders. These non-
senders instead likely submit ACT scores to col-
leges, apply to colleges not requiring college 
entrance examinations, or, in some instances, 
may fail to engage in the college application pro-
cess at all. This group of nonscore senders serves 
as the foundation for the placebo test results 
shown in Panel A of Table 7.19 If the downstream 
ripple effects shown earlier in the article reflected 
growth in 4-year college attendance and bache-
lor’s completion among fee-waiver students rela-
tive to non-fee-waiver students unrelated to the 
flexible score report policy, we would expect to 
observe similar ripple effects among non-score-
sending students. As we show in Panel A of Table 
7, none of the parameter estimates on 4-year col-
lege-going or bachelor’s completion are signifi-
cant, reinforcing that the ripple effects are in fact 
a direct result of the policy’s clear influence on 
student score-sending volume.
Unrelated to the assumption of parallel trends, 
it is also possible that the flexible score report 
policy induced students to take the SAT, perhaps 
in lieu of or in addition to the ACT. The direction 
of bias from this threat is ambiguous. An influx 
of higher achieving students using SAT fee waiv-
ers would likely yield upwardly biased estimates, 
whereas the bias would be downward if the com-
position of induced students was primarily stu-
dents who otherwise would not have taken any 
college entrance exam.
In Table 8, we test whether the flexible score 
report policy increased the fraction of high school 
seniors taking the SAT. Increased SAT participa-
tion rates in high fee-waiver usage high schools 
without comparable increases in high schools 
with zero usage might raise suspicion of bias. To 
FIGURE 3. Score sending and college-going among students self-identifying as low-income.
Note. LT = less than.
 97
TABLE 7
Sensitivity Analyses Using Nonscore Senders and Full Sample (2007–2009 Cohorts)
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
 
Attends 
college
Attends college 
on-time
Attends 2-year 
college on-time
Attends 4-year 
college on-time
Average SAT of 
college enrolled
Attends top two 
Barron’s category
Earned bachelor’s 
degree in 4 years
Earned bachelor’s 
degree in 5 year.
Earned bachelor’s 
degree in 6 years
Number of 
score sends
A. Nonscore senders
 Post × 
FeeWaiver
0.008*
0.007
0.010*
−0.003
−3.690**
−0.000
0.003
0.003
0.000
 
(0.004)
(0.006)
(0.005)
(0.006)
(1.538)
(0.000)
(0.003)
(0.003)
(0.004)
 
 Observations
 896,154
 896,154
 896,154
 896,154
 338,163
 896,154
 896,154
 896,154
 896,154
 
 R2
.092
.114
.171
.239
.456
.138
.186
.196
.186
 
B. Score senders and nonscore senders
 Post × 
FeeWaiver
0.010***
0.018***
0.011***
0.007*
1.911**
0.005***
0.005**
0.009***
0.011***
0.313***
(0.003)
(0.004)
(0.004)
(0.004)
(0.927)
(0.001)
(0.002)
(0.002)
(0.003)
(0.048)
 Observations 3,732,863
3,732,863
3,732,863
3,732,863
2,368,611
3,732,863
3,732,863
3,732,863
3,732,863
3,732,863
 R2
.066
.099
.160
.227
.484
.225
.215
.222
.211
.282
Note. The parameter estimates associated with Post × FeeWaiver indicate the impact of the four free flexible policy shock on the outcomes listed in the table columns. Standard errors are clustered by state. All regressions 
include indicator variables for date of last SAT administration, fixed effects for the student’s high school, and a vector of student covariates, including student race, student gender, and student SAT scores on the math and 
critical reading sections of the exam. We define the average SAT of the enrolled college using IPEDS 2007 data, estimating this metric as the sum of the 25th and 75th percentiles for math and critical reading divided by a 
factor of 2. Regressions include only those students who last took the SAT either in the spring of their HS junior year or the fall of their HS senior year. IPEDS = Integrated Postsecondary Data System; HS = high school.
*p < .10. **p < .05. ***p < .01.
 98
formally test for this, we simply collapse indi-
vidual SAT test-taking data into a high school by 
cohort data set, and regress the fraction of seniors 
(from the Common Core of Data) taking the SAT 
on cohort fixed effects and an interaction between 
an indicator for whether the high school fell into 
the high fee-waiver usage category and an indi-
cator for whether the cohort was exposed to flex-
ible score report policy.20 None of the Table 8 
parameter estimates on this interaction term are 
statistically significant or of a magnitude to sug-
gest test taking increases in response to the pol-
icy. As such, we dismiss this as a source of bias.
Discussion
In this study, we investigate the impact of an 
exogenous shift in the College Board’s score-
sending policy focused on low-income students 
who took the SAT with a fee waiver. Beginning 
in the fall of 2007, the College Board provided 
these low-income test takers with a “treatment 
package” that consisted of four additional free 
score sends that could be utilized over a longer 
time horizon. This policy shock had an immedi-
ate impact on student SAT score sending, which 
Smith (2016) reveals as a reasonable although 
imperfect proxy for actual college applications. 
Relating this policy shift to previous literature 
that causally links the number of college applica-
tions to college enrollment (Smith, 2013), we 
predict and indeed find that the seemingly minor 
policy change yielded downstream ripple effects 
on initial college access and ultimate bachelor’s 
degree attainment. Across a host of different 
sample specifications and using a DID estima-
tion strategy, we estimate impacts on on-time 
college enrollment between 2 and 4.4 percentage 
points, and impacts on 6-year bachelor’s degree 
attainment of 1.7 to 3.5 percentage points.
A comparative strength of this analysis is our 
ability to follow students over a long time hori-
zon and to consider not only college access but 
also college completion outcomes. This provides 
stable ground for back-of-the envelope calcula-
tions of the potential returns to this simple and 
inexpensive policy change. Given the impacts 
cited above, we estimate that, from each high 
school cohort, the number of bachelor’s degree 
recipients increases by about 3,000 as a result of 
the policy change at a total cost that has an upper 
bound of approximately US$2.4 million, or 
 
about US$800 per additional bachelor’s degree 
recipient.21 The typical bachelor’s degree recipi-
ent enjoys a lifetime earnings premium of nearly 
US$300,000 over the typical high school gradu-
ate (Baum & Ma, 2014). Although this figure is 
not causally estimated, for at least two reasons, 
we might expect this to be an underestimate of 
the overall returns to bachelor’s degree attain-
ment. First, Card (1999) reports that causal esti-
mates of the effect of education on earnings are 
typically 20% to 40% larger than observed dif-
ferences. Second, the lifetime earnings premium 
TABLE 8
Threats to Validity From Increased SAT Test-Taking Rates
(1)
(2)
(3)
(4)
 
Change in share of seniors taking SAT after implementation of four free flexible score-send policy
 
0% and 70% + fee-
waiver usage schools: 
Cohort 2007, 2009
0% and 80% + fee-
waiver usage schools: 
cohort 2007, 2009
0% and 100% fee-
waiver usage schools: 
cohort 2007, 2009
0% and 100% fee-
waiver usage schools: 
cohort 2004–2007, 2009
Post × HighFeeWvrUse
0.004
0.008
−0.008
0.017
 
(0.015)
(0.015)
(0.024)
(0.019)
Observations
8,725
8,481
7,805
18,389
R2
.966
.965
.964
.905
Note. The unit of observation is a high school, by cohort. We regress the fraction of seniors who took the SAT (senior fall and junior spring takers) 
on cohort indicator variables, and an interaction term (Post × HighFeeWvrUse) between the 2009 cohort indicator variable and an indicator for 
whether the high school was a high fee-waiver usage high school, flexibly defined by the column headers. We continue to include fixed effects for 
high schools. In these regressions, we omit students from the 2008 high school cohort because some of these students took the SAT under the new 
policy, whereas others did not. Standard errors are clustered by state. High school senior enrollment is collected from the Common Core of Data. 
In rare instances, the number of SAT test takers in a cohort exceeds enrollment. We cap the fraction of seniors taking the SAT at 1.0.
 Surprising Ripple Effects 
99
does not capture the many other public and pri-
vate benefits that accrue as a result of higher edu-
cation (Baum et al., 2013). Even so, given the 
low-cost nature of the policy shift, earnings pre-
miums of substantially lower magnitudes would 
still translate into a favorable return on invest-
ments from the flexible score report policy.
Over the past four decades, several crucial 
pieces of work have contributed greatly to our 
understanding of college selection and college 
choice. Economists like Long (2004) and Manski 
and Wise (1983) have helped to uncover how stu-
dents select into colleges as well as the transfor-
mation of this selection process over time. More 
recently, Hoxby and Avery (2013) have drawn 
attention to the differential college search, appli-
cation, and enrollment behavior by socioeco-
nomic status, even among the highest achieving 
high school students. Suboptimal behavior at all 
stages of the college selection process has been 
carefully documented in the literature and has 
concerned policymakers and practitioners alike. 
Despite great strides in understanding the 
nuances and complexity of the student college-
choice process, influencing this process has 
proven to be more challenging, until recently.
Several recent studies have demonstrated that 
reshaping students’ postsecondary trajectories is 
not only achievable but often can be accomplished 
with fairly small policy tweaks. Direct-to-student 
interventions, such as the Expanding College 
Opportunity (ECO) project, demonstrate that 
informational packets containing eight college 
application fee waivers to selective colleges favor-
ably influenced college choice (Hoxby & Turner, 
2013). An onerous financial aid application sys-
tem has also been implicated in suppressing col-
lege-going. Bettinger, Long, Oreopoulos, and 
Sanbonmatsu (2012) show that Free Application 
for Federal Student Aid (FAFSA) assistance, 
paired with college cost estimates, increased col-
lege-going and persistence. Recent statewide ini-
tiatives to replace high school standardized tests, 
which often held no stakes for individual students, 
with either the SAT or ACT, have similarly 
increased college-going rates as well as academic 
match (Goodman, 2013; Hurwitz, Smith, Niu, & 
Howell, 2015; Hyman, 2016; Klasik, 2013). 
Light-touch reminders and assistance can facili-
tate students’ timely transition from high school to 
college (Castleman & Page, 2015). Finally, and 
paradoxically, some of the obstacles driving sub-
optimal college search and selection may be 
erected by the colleges themselves. Not only do 
application “micro barriers” like admissions 
essays and application fees reduce application vol-
ume, they also negatively impact enrollment of 
students from underrepresented groups such as 
Black and Hispanic students (Smith et al., 2015).
Our work builds on these previous studies and 
contributes to this burgeoning literature. We have 
uncovered another relatively simple and inex-
pensive avenue for increasing college access and 
improving choice. Through the ripple effects we 
document, we demonstrate promising evidence 
that simple measures have the power to influence 
not only how students approach college selection 
but also whether they ultimately complete.
Authors’ Note
The views expressed in this article do not reflect the 
views or opinions of the College Board. All errors are our 
own. Authorship order was determined alphabetically. 
Michael Hurwitz is a full-time employee of the College 
Board. Data for this study were provided by the 
 
College Board. College Board officials reviewed the 
results of the study prior to dissemination of our findings.
Acknowledgments
We are grateful to the College Board and the Center 
for Education Policy Research at Harvard University 
for providing data access and financial support to 
make this research possible. We thank Christopher 
Avery, Jon Fullerton, George Loewenstein, Jessica 
Howell, and Jon Smith for helpful suggestions on 
earlier versions of this article.
Declaration of Conflicting Interests
The author(s) declared no potential conflicts of inter-
est with respect to the research, authorship, and/or 
publication of this article.
Funding
The author(s) disclose receipt of the following finan-
cial support for the research, authorship, and/or publi-
cation of this article: The study was supported by a 
research grant from The College Board to the Center 
for Education Policy Research at Harvard University.
Notes
1. This statistic is calculated for first-time degree-
seeking students entering college in the fall of 2009, 
our final cohort of students.
 Hurwitz et al.
100
2. These statistics are calculated for our main 
analytic sample: U.S. students who graduated from 
high school between 2007 and 2009, sent at least 
1 SAT score, and last took the SAT in the spring of 
their junior year or fall of their senior year. Of stu-
dents who enrolled in 2-year colleges, 26% of students 
reported their SAT scores to their institution. Although 
2-year colleges are open enrollment, and therefore 
generally do not require admissions credentials such 
as SAT scores, there are at least two reasons why we 
would expect to observe students formally reporting 
SAT scores to 2-year institutions. First, some 2-year 
institutions allow for students to pass out of remedial 
course work based on their performance on the SATs. 
Second, some institutions that primarily host 2-year 
degree programs may have limited 4-year programs 
that do require certain admissions credentials such as 
SAT scores. Therefore, we do not consider this level of 
reporting to 2-year institutions to be unusual.
3. Digest of Education Statistics, Table 146 (https://
nces.ed.gov/programs/digest/d09/tables/dt09_146 
.asp).
4. Digest of Education Statistics, Table 226.40 
(https://nces.ed.gov/programs/digest/d13/tables/
dt13_226.40.asp). During the years that we consider in 
our analysis, only Maine had a universal, school-day 
SAT program through which virtually all public high 
school juniors took the SAT. Today, such programs at 
either the district or state level are more common.
5. For more information on the SAT score-send-
ing process, see https://sat.collegeboard.org/scores/
send-sat-scores. In addition to colleges, students may 
send SAT scores to scholarship organizations or to the 
National Collegiate Athletic Association (NCAA).
6. Cited from The College Board’s Draft Minutes 
from the June 21 to 22, 2007, Board of Trustees 
meeting.
7. 
For 
more 
information 
on 
Integrated 
Postsecondary Data System (IPEDS), see https://nces 
.ed.gov/ipeds/.
8. For more information on the National Center 
for Education Statistics (NCES)–Barron’s admissions 
competitiveness index, see http://ies.ed.gov/pubsearch/
pubsinfo.asp?pubid=2010331.
9. We utilize fixed effects based on when students 
last took the SAT, as SAT timing is the key measure of 
time that we have and is the measure that indicates the 
score-sending policy active at the time the student took 
the test. We note that our approach to use fixed effects 
does not result in combining students across gradua-
tion cohorts. This is because we limit our sample to 
students who took the SAT either in the spring of their 
junior year or in the fall of their senior year. Note that 
we do not include an indicator for postSpring2007 
in our model specification. This is because such an 
indicator is completely collinear with the fixed effects 
that we have included for SAT date.
10. In Appendix Figure A1, we additionally pro-
vide graphical evidence of the sudden convergence 
of score-sending behavior after the policy shock. In 
Appendix Table A1, we show descriptive statistics for 
the students who sent no scores to colleges or scholar-
ship organizations (available in the online version of 
the journal).
11. As a rule of thumb, one ACT point is the equiv-
alent of 40 SAT points on the 1,600 scale. See the 
concordance tables published by the College Board 
for more details (http://research.collegeboard.org/
sites/default/files/publications/2012/7/researchnote-
2009-40-act-sat-concordance-tables.pdf).
12. Results in Table 3 are based on linear prob-
ability models where the outcome is equal to 1 when 
the student sent any scores (column 1), exactly 1 to 7 
scores (columns 2–8), or 8 or more scores (column 9).
13. We obtain this figure by dividing the 1.7 per-
centage point estimate in Panel A, column 9 by the 2.0 
percentage point estimate in Panel A, column 2.
14. This figure is based on authors’ calculations 
using fee-waiver recipients from the 2007 through 
2009 cohorts.
15. The average completion percentages of 37% 
and 65% correspond to the outcomes of the average 
fee-waiver recipient in the top SAT fee-waiver tercile 
from the 2007 through 2009 cohorts who started at a 
4-year college on-time. We refer to this assumption as 
generous because the student on the margins of enroll-
ment likely had a lower SAT score than this group’s 
average of 1,050 and would also have other academic 
characteristics that would be expected to depress com-
pletion rates.
16. Among sampled fee-waiver students from the 
2007 through 2009 cohorts with SAT scores between 
880 and 1,290, 26% earned bachelor’s degrees within 
4 years of high school graduation and 49% earned 
bachelor’s degrees within 6 years. Among students 
with SAT scores of 1,300+, 58% and 76% earned 
bachelor’s degrees within 4 and 6 years of high school 
graduation, respectively.
17. Long classifies high unemployment growth 
states as those experiencing growth in unemployment 
of 5 percentage points or more between 2007Q1 and 
2009Q4. These states include AL, AZ, CA, FL, GA, 
ID, IL, IN, MI, NV, NJ, OH, OR, RI, SC, TN, UT, and 
WA. We classify remaining states as low unemploy-
ment growth states. Long (2015) does not classify AR, 
IA, MS, MT, NM, NC, ND, SD, VT, and WY due to 
state code data suppression. We also exclude students 
from these states in our sensitivity analyses.
18. Among students reporting parental incomes 
less than US$40,000, it seems likely that fee-waiver 
 Surprising Ripple Effects 
101
students are poorer than non-fee-waiver test takers. 
As the Pell expansion likely increased eligible funds 
to the wealthier students in this subcategory more 
than the poorer students, estimates on college-going 
and completion may actually be biased downwardly 
in the plausible scenario where Pell grant expansion 
impacted college-going or retention.
19. Panel B shows the results from the entire sample, 
including both the score senders and nonscore senders.
20. In these regressions, we omit students from the 
2008 high school cohort because some of these stu-
dents took the SAT under the new policy, whereas oth-
ers did not.
21. Based on the two right-hand columns of Table 4, 
we assume that 2% of the 150,000+ fee-waiver recipi-
ents from the 2009 cohort earned bachelor’s degrees as 
a direct result of the flexible score report policy. Prior 
to the policy change, we estimate from College Board 
administrative data that approximately 80% of scores 
sent by fee-waiver recipients in a given cohort were free 
“registration score sends.” Assuming a true cost of a fee-
waiver send of US$11.25, for a typical cohort of approx-
imately 150,000 fee-waiver recipients, the total cost 
of score sending for these students prior to the policy 
shift was 5.1 × US$11.25 × 150,000 = US$8,606,250. 
Of this amount, 80% (or US$6,885,000) was shouldered 
by the College Board, and the remaining US$1,721,250 
was collected from students. After the policy shift, we 
assume that, in the most extreme case (in terms of rev-
enue) for the College Board, that fee-waiver recipients 
did not shoulder any costs associated with score sending. 
Then, the total cost of score sending to the College Board 
was 5.5 × US$11.25 × 150,000 = US$9,281,250. In sum, 
we estimate the additional cost to the College Board of 
this policy shift to be (US$9,281,250 – US$6,885,000) 
or approximately US$2.4 million per cohort. In Board 
of Trustees meeting minutes, the associated revenue loss 
was estimated at a much lower US$1 million.
References
Avery, C., & Turner, S. (2010). Playing the college 
application game: Critical moves and the link to 
socio-economic circumstances (Working paper). 
Charlottesville: University of Virginia.
Avery, C. N., Howell, J. S., & Page, L. C. (2014). A 
review of the role of college applications in students’ 
postsecondary outcomes (College Board Research 
Brief). Retrieved from https://research.college 
board.org/sites/default/files/publications/2015/1/
college-board-research-brief-review-role-college-
applications-postsecondary-outcomes.pdf
Barr, A., & Turner, S. E. (2013). Expanding enroll-
ments and contracting state budgets: The effect 
of the Great Recession on higher education. The 
ANNALS of the American Academy of Political and 
Social Science, 650, 168–193.
Baum, S., Ma, J., & Payea, K. (2013). Education pays 
2013: The benefits of higher education for indi-
viduals and society. New York, NY: The College 
Board.
Baum, S., & Payea, K. (2013). Trends in student aid, 
2013. New York, NY: The College Board.
Baum, S., & Ma, J. (2014). Trends in college pricing, 
2014. New York, NY: The College Board.
Bettinger, E. P., Long, B. T., Oreopoulos, P., & 
Sanbonmatsu, L. (2012). The role of application 
assistance and information in college decisions: 
Results from the H&R Block FAFSA experiment. 
The Quarterly Journal of Economics, 127, 1205–
1242.
Bidwell, A. (2014). The $10,000 Community College 
B.A. (U.S. News and World Report). Retrieved from 
http://www.usnews.com/news/articles/2014/11/13/
average-student-loan-debt-hits-30-000
Bond, T. N., Bulman, G., Li, X., & Smith, J. (2016). 
Updating human capital decisions: Evidence 
from SAT score shocks and college applications 
(Working paper). Retrieved from http://papers 
.ssrn.com/sol3/papers.cfm?abstract_id=2738204
Card, D. (1999). The causal effect of education on 
earnings. Handbook of Labor Economics, 3, 1801–
1863.
Casey, B. J., Jones, R. M., & Somerville, L. H. (2011). 
Braking and accelerating of the adolescent brain. 
Journal of Research on Adolescence, 21, 21–33.
Castleman, B. L., Owen, L., & Page, L. C. (2015). Stay 
late or start early? Experimental evidence on the 
benefits of college matriculation support from high 
schools versus colleges. Economics of Education 
Review, 47, 168–179.
Castleman, B. L., & Page, L. C. (2014). Summer 
melt: Supporting low-income students through the 
transition to college. Cambridge, MA: Harvard 
Education Press.
Castleman, B. L., & Page, L. C. (2015). Summer 
nudging: Can personalized text messages and peer 
mentor outreach increase college going among 
low-income high school graduates? Journal of 
Economic Behavior & Organization, 115, 144–
160.
Castleman, B. L., & Page, L. C. (2016). Freshman year 
financial aid nudges: An experiment to increase 
FAFSA renewal and college persistence. Journal 
of Human Resources, 51, 389–415.
Castleman, B. L., Page, L. C., & Schooley, K. (2014). 
The forgotten summer: Mitigating summer attri-
tion among college-intending, low-income high 
school graduates. Journal of Policy Analysis and 
Management, 33, 320–344.
 Hurwitz et al.
102
Dillon, E. W., & Smith, J. A. (2013). The determi-
nants of mismatch between students and colleges 
(No. w19286). Cambridge, MA: National Bureau 
of Economic Research.
Dynarski, S. M., Hemelt, S. W., & Hyman, J. M. 
(2015). The missing manual using national stu-
dent clearinghouse data to track postsecondary 
outcomes. Educational Evaluation and Policy 
Analysis, 37(1 Suppl.), 53S–79S.
Goodman, J., Hurwitz, M., & Smith, J. (2015). College 
access, initial college choice and degree comple-
tion (No. w20996). Cambridge, MA: National 
Bureau of Economic Research.
Goodman, S. (2013). Learning from the test: Raising 
selective college enrollment by providing informa-
tion (Finance and Economics Discussion Series 
2013-69). Washington, DC: Board of Governors of 
the Federal Reserve System.
Howell, J. S., & Pender, M. (2016). The costs and ben-
efits of enrolling in an academically matched col-
lege. Economics of Education Review, 51, 152–168.
Hoxby, C., & Avery, C. (2013). The missing “one-
offs”: The hidden supply of high-achieving, low-
income students. Brookings Papers on Economic 
Activity, 2013(1), 1–65.
Hoxby, C., & Turner, S. (2013, March). Expanding col-
lege opportunities for high-achieving, low income 
students (Stanford Institute for Economic Policy 
Research Discussion Paper, 12-014). Retrieved 
from 
http://siepr.stanford.edu/sites/default/files/
publications/12-014paper_6.pdf
Hurwitz, M., & Howell, J. (2014). Estimating causal 
impacts of school counselors with regression 
discontinuity designs. Journal of Counseling & 
Development, 92, 316–327.
Hurwitz, M., Smith, J., Niu, S., & Howell, J. (2015). 
The Maine question: How is four-year college 
enrollment affected by mandatory college entrance 
exams? Educational Evaluation & Policy Analysis, 
37, 138–159.
Hyman, J. (2016). ACT for all: The effect of manda-
tory college entrance exams on postsecondary 
attainment and choice. Education Finance and 
Policy. Advance online publication. doi:10.1162/
EDFP_a_00206
Klasik, D. (2013). The ACT of enrollment: The 
college enrollment effects of required colleges 
entrance exam taking. Educational Researcher, 42, 
151–160.
Lavecchia, A. M., Liu, H., & Oreopoulos, P. (2014). 
Behavioral economics of education: Progress 
and possibilities (No. w20609). Cambridge, MA: 
National Bureau of Economic Research.
Long, B. T. (2004). How have college decisions changed 
over time? An application of the conditional logistic 
choice model. Journal of Econometrics, 121(1–2), 
271–296.
Long, B. T. (2015). The financial crisis and college 
enrollment: How have students and their families 
responded? In J. R. Brown & C. M. Hoxby (Eds.), 
How the great recession affected higher educa-
tion (pp. 209-234). Chicago, IL: The University of 
Chicago Press.
Manski, C., & Wise, D. (1983). College choice in 
America. Cambridge, MA: Harvard University 
Press.
Milkman, K. L., Beshears, J., Choi, J. J., Laibson, D., 
& Madrian, B. C. (2012). Following through on 
good intentions: The power of planning prompts 
(No. w17995). Cambridge, MA: National Bureau 
of Economic Research.
Page, L. C., & Scott-Clayton, J. (2016). Improving 
college access in the United States: Barriers and 
policy responses. Economics of Education Review, 
51, 4–22.
Pallais, A. (2015). Small differences that matter: 
Mistakes in applying to college. Journal of Labor 
Economics, 33, 493–520.
Radford, A. W. (2013). Top student, top school? How 
social class shapes where valedictorians go to 
college. Chicago, IL: The University of Chicago 
Press.
Radwin, D., & Horn, L. (2014). What is a commu-
nity college? (RTI International Completion Arch 
Research Brief). Retrieved from http://completion-
arch.org/uploads/What_is_Community_College.pdf
Ross, R., White, S., Wright, J., & Knapp, L. (2013). 
Using behavioral economics for postsecondary 
success. New York, NY: Ideas42.
Smith, J. (2013). The effect of college applications on 
enrollment. The BE Journal of Economic Analysis 
& Policy, 14, 151–188.
Smith, J. (2016). The sequential college application 
process (Working paper). Retrieved from https://
drive.google.com/file/d/0B5c–a90k-_zd2Q5YX-
JoMk1XcE0/view
Smith, J., Hurwitz, M., & Howell, J. (2015). Screening 
mechanisms and student responses in the college 
market. Economics of Education Review, 44, 17–28.
Smith, J., Pender, M., & Howell, J. (2013). The full 
extent of student-college academic undermatch. 
Economics of Education Review, 32, 247–261.
Thaler, R. H., & Mullainathan, S. (2008). Behavioral 
economics—The concise encyclopedia of eco-
nomics. Library of Economics and Liberty. 
Retrieved from http://www.econlib.org/library/
Enc/BehavioralEconomics.html
Thaler, R. H., & Sunstein, C. R. (2008). Nudge: 
Improving decisions about health, wealth, and 
happiness. New Haven, CT: Yale University Press.
 Surprising Ripple Effects 
103
Authors
MICHAEL HURWITZ is the senior director of Policy 
Research at the College Board. His current research 
focuses on college access and college completion.
PREEYA P. MBEKEANI is an advanced doctoral stu-
dent in Quantitative Policy Analysis in Education at the 
Harvard Graduate School of Education and a doctoral 
fellow in the Multidisciplinary Program in Inequality 
and Social Policy at the Harvard Kennedy School of 
Government. Her research focuses on education policy 
and inequality related to college access and success of 
first-generation and low-income students.
MARGARET M. NIPSON is a research manager at 
the Center for Education Policy Research at Harvard 
University. Her work focuses on helping education 
agencies leverage data for improved decision making. 
LINDSAY C. PAGE is an assistant professor of 
research methodology and a research scientist at the 
Learning Research and Development Center at the 
University of Pittsburgh. Her research focuses on 
quantitative methods and their application to ques-
tions regarding the effectiveness of educational poli-
cies and programs across the preschool to postsecond-
ary spectrum.
Manuscript received April 14, 2016
First revision received June 19, 2016
Second revision received July 23, 2016
Accepted July 29, 2016
