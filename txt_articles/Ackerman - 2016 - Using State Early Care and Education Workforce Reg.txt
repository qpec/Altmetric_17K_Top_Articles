  This Policy Information Report was written by:
Debra J. Ackerman
Educational Testing Service, Princeton, NJ
Policy Information Center
Mail Stop 19-R
Educational Testing Service
Rosedale Road Princeton, NJ 08541-0001
(609) 734-5212
pic@ets.org
Copies can be downloaded from:www.ets.org/research/pic
The views expressed in this report are those of the author and do not necessarily reflect the views of the officers and
trustees of Educational Testing Service.
About ETS
At ETS, we advance quality and equity in education for people worldwide by creating assessments based on rigorous
research. ETS serves individuals, educational institutions and government agencies by providing customized solutions
for teacher certification, English language learning, and elementary, secondary and postsecondary education, and by con-
ducting education research, analysis and policy studies. Founded as a nonprofit in 1947, ETS develops, administers and
scores more than 50 million tests annually — including the TOEFL® and TOEIC® tests, the GRE® tests and The Praxis
Series® assessments — in more than 180 countries, at over 9,000 locations worldwide.
 Policy Information Report and ETS Research Report Series ISSN 2330-8516
RESEARCH REPORT
Using State Early Care and Education Workforce Registry
Data to Inform Training-Related Questions: Issues to
Consider
Debra J. Ackerman
Educational Testing Service, Princeton, NJ
The current early care and education (ECE) policy context is bringing increased attention to the training completed by the child care
workforce and to the use of registries to track such training. Although ECE workforce registries are designed to record individuals’data,
aggregate registry data have the potential to shed light on the workforce’
s training needs. However, to date, registries have not been
tapped in this way, and there is limited research on the data collected across registries and the extent to which they are standardized. In
this report, I share the results of research on the training focus variables used across these databases and on the extent to which such
variables are comparable. Also explored is when registries began recording these data and whether enrollment is voluntary, incentivized,
or mandatory. The results of the study suggest that aggregate registry data have the potential to address questions related to the focus of
the training in which the ECE workforce participates. However, additional research on ECE workforce registries is needed to confirm
their usefulness as a source of data on child care training as well as the accessibility of these data.
Keywords Early care and education; training; workforce registries
doi:10.1002/ets2.12117
The current U.S. early care and education (ECE) policy landscape has a strong focus on improving the quality of child
care for children under the age of 5 years (Boller, Tarrant, & Schaack, 2014). These efforts have been both funded and
guided in large part by states’respective Child Care and Development Fund (CCDF) grants (see also the Child Care and
Development Block Grant Act of 2014) and Race to the Top–Early Learning Challenge (U.S. Department of Education,
2015) awards. Related contributors include state child care Quality Rating and Improvement System (QRIS) initiatives, the
majority of which are linked to Race to the Top–Early Learning Challenge and CCDF plans and are being implemented,
piloted, or planned in every state except Missouri (QRIS Compendium, 2016).
As part of these quality improvement efforts, policy makers are focusing on the child care workforce’
s capacity to
support young children’
s social, emotional, physical, and cognitive development. For example, states’biennial CCDF
plans must include ongoing workforce training and other professional development related to meeting the developmental
needs of participating children (Matthews, Schulman, Vogtman, Johnson-Staub, & Blank, 2015; Office of Child Care,
2015). Within QRIS initiatives, at least 38 states have elements focusing on staff education and training (National Center
on Child Care Quality Improvement, 2011a, 2011b; National Infant and Toddler Child Care Initiative, 2011). The focus
on training is particularly critical given that no state requires child care teachers to obtain a college degree prior to being
hired (Child Care Aware, 2012).
In addition, there is a growing state interest in using what are known as ECE workforce registries to track the child
care workforce’
s training, credit-bearing course work, and formal educational attainment (Prentice, 2013; The National
Registry Alliance [TNRA], 2009). These databases typically record a training’
s title and length in hours. Registries also
may document if enrollees meet licensing and/or QRIS requirements and, in states with what is known as a career ladder
or lattice, the criteria for a specific level. In addition, some registries record an enrollee’
s employment history. Finally,
the majority of registry Web sites allow individuals to search for trainings, trainers, and associated training organizations
(Kipnis & Whitebook, 2011; Prentice, 2013; U.S. Government Accountability Office, 2012; Wolfe, 2015).
Given the interrelated goals of this policy landscape, it would be helpful for ECE policy makers, program adminis-
trators, training providers, advocates, and researchers to have accurate information on the training needed by the child
Corresponding author: D. J. Ackerman, E-mail: dackerman@ets.org
2
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
 D. J. Ackerman
Using ECE Data to Inform Training
care workforce. Furthermore, although workforce registries are designed to record individuals’data, aggregate registry
data have the potential to provide such information (Early Learning Challenge Technical Assistance Program [ELC TA],
2015; National Center on Child Care Professional Development Systems and Workforce Initiatives [PDW Center], 2013).
Yet, to date, few researchers have tapped data from single registries (e.g., Douglass, Carter, Smith, & Gadhadharan, 2012;
Lipscomb, Schmitt, & Pratt, 2015; Weber & Grobe, 2014; Weber & Lipscomb, 2015). Even more importantly, there has
been limited research on the exact data collected across registries and the extent to which such data are standardized or
comparable (e.g., Mayfield, 2012; Prentice, 2013; TNRA, 2009).
In this report, I share the results of research on the variables ECE workforce registries use to denote the focus of
participants’training and on how long such data have been collected. Also shared is the extent to which registry enrollment
is voluntary, incentivized, or mandatory. To set the stage for the study, I take a closer look at the child care workforce
training policy context. I then review three issues to consider before using aggregate ECE workforce registry data to
inform training-related purposes. After discussing the study’
s results, I conclude the report with some future research to
be conducted on ECE workforce registries as a means for further informing their potential to be used as a source of data
on child care training.
Child Care Workforce Training Policy Context
Thinking about the knowledge and skills needed to effectively support student learning can be helpful for situating a dis-
cussion about the child care workforce training policy context, particularly when policies aim to enhance the workforce’
s
capacity to support young children’
s social, emotional, physical, and cognitive development. As Sykes and Wilson (2015)
described, high-quality teaching involves a complex set of instructional competencies, such as planning and preparing
for instruction (including determining what students already know and can do), developing relationships with students,
and managing the physical environment of the classroom. Teachers also must communicate effectively with other profes-
sionals and students’families. Furthermore, these competencies are not only dependent on the content of what is being
taught (e.g., literacy vs. mathematics) and student characteristics (e.g., age 3 vs. age 8; monolingual vs. dual language) but
also influenced by the settings in which teachers work and the support received in those settings.
In settings serving infants, toddlers, and preschoolers, it is especially critical for teachers to possess a strong founda-
tional knowledge about child development as well as the biological and environmental factors that can both enhance and
impede young children’
s behavior and learning. Such knowledge also is important for organizing the classroom learning
environment and using ongoing assessment data to inform instruction (Institute of Medicine [IOM] & National Research
Council [NRC], 2012). In addition, teachers’instructional interactions with their students may be particularly crucial to
effective teaching; that is, while teachers need to be caring, responsive, and mindful about children’
s health and safety,
also important are the activities and conversations that promote students’higher order thinking skills and early learning
outcomes (Burchinal et al., 2008; Cameron, 2012; Hamre et al., 2013; Pianta, 2011). In addition, effective teachers will
know when to use different learning approaches (e.g., one-on-one, small group, whole group, hands-on activities) and
“have a repertoire of content-specific instructional strategies that promote learning” (Hamre, 2014, p. 225).
Prehire Requirements
While effective teaching may require a specialized knowledge base and set of competencies, state policies require child
care teachers1 in licensed centers to attain minimal prehire qualifications. Simply put, no state requires center-based child
care teachers to have a college degree prior to being hired. Moreover, 20 states require attainment of only a high school
diploma or equivalent exam, and an additional 20 states have no minimum educational requirement. Just three states
require individuals to obtain a Child Development Associate (CDA) credential (Ackerman & Kingsley, 2015). To attain a
CDA, individuals must complete a variety of activities, including 120 clock hours of training across eight topics (Council
for Professional Recognition, 2015). The CDA also is a voluntary milestone step in the majority of states’child care career
ladders or lattices (Missouri Coordinating Board for Early Childhood, 2014).
In contrast, the 2007 Head Start reauthorization mandated 50% of teachers in the federally funded Head Start program
for preschoolers to have a minimum of a bachelor’
s degree in or related to early childhood by 2013 (Improving Head Start
for School Readiness Act of 2007). Among state-funded pre-K programs, which mostly target 4 year olds, 18 states require
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
3
 D. J. Ackerman
Using ECE Data to Inform Training
all teachers to have a minimum of a bachelor’
s degree. In an additional 33 states, teachers working in public school pre-K
settings must have a minimum of a 4-year degree (Schilder, 2016).
Perhaps not surprisingly, national studies suggest that just 25% of center-based child care teachers have a 4-year degree
(Bassok, Fitzpatrick, Loeb, & Paglayan, 2013; National Survey of Early Care and Education Project Team, 2012; U.S. Gov-
ernment Accountability Office, 2012). Conversely, an analysis of recent Head Start Program Information Report data
shows that 74% of its teachers are reported to have a bachelor’
s degree or higher (Office of Head Start, 2015).
Child Care and Development Fund
Several additional policy contexts drive the training completed by the child care workforce both initially and on an ongoing
basis. The first context stems from the CCDF program, which is part of the Child Care and Development Block Grant Act
and applies to staff in settings accepting CCDF family assistance vouchers. For example, new staff must receive training
on a variety of health- and safety-related topics, including recognizing symptoms of illness, preventing and controlling
infectious disease, administering medication, emergency procedures, and first aid and cardiopulmonary resuscitation
(National Center on Child Care Quality Improvement, 2015; Office of Child Care, 2015). States’biennial CCDF plans
also must provide an assurance that training will be conducted on an ongoing basis and reflect current research and best
practices related to the skills necessary to meet the developmental needs of participating children. Furthermore, states
must report via their CCDF Quality Performance Reports how many child care center-based teachers, family child care
providers, and legally exempt providers received training on the state’
s early learning guidelines (Matthews et al., 2015).
Child Care Licensing Regulations
A second key context is state child care licensing regulations, which, in addition to governing prehire qualifications, require
center-based child care teachers to complete posthire training. As might be expected given CCDF requirements, the major-
ity of states require initial training related to children’
s health and safety, emergency preparedness, licensing regulations,
and child abuse reporting. In addition, 48 states require staff in child care centers to undergo specific amounts of annual
training. These amounts vary widely, with 10 states requiring 11 or fewer hours, 24 states requiring between 12 and 17
hours, and the remaining states requiring 18 or more hours. While 40 states require annual training on health and safety
issues, in roughly two-thirds of states, varying amounts of annual training related to child development, child guidance
and behavior, and/or learning activities also are required (Child Care Aware, 2013; U.S. Government Accountability Office,
2012).
Quality Rating and Improvement System Initiatives
A third context driving child care training is QRIS initiatives, which are being implemented, piloted, or planned in 49
states and the District of Columbia (QRIS Compendium, 2016). These initiatives generally have two primary purposes,
both of which reflect the intent of the Child Care and Development Block Grant Act. The first purpose is to provide parents
and other consumers with information regarding the relative quality of any child care program. A second primary purpose
is to incentivize participating programs to maintain or improve their quality as a means for better supporting children’
s
development and early learning.
As mentioned in the introduction, at least 38 QRIS initiatives have quality improvement categories focusing on staff
education and training. Some of these states also require training related to specific topics, such as caring for infants
and toddlers or a state’
s early learning guidelines. And, as an incentive to the workforce to attain higher education levels
and/or participate in training, many QRIS offer scholarships (National Center on Child Care Quality Improvement, 2011a,
2011b; National Infant and Toddler Child Care Initiative, 2011). However, in most states, the participation of licensed
center-based child care programs is voluntary (QRIS Compendium, 2016).
Summary
In summary, owing to the minimal qualifications needed to enter he licensed child care workforce, individuals likely will
need training if they are to effectively contribute to efforts to improve child care quality and support young children’
s learn-
ing and development. At the same time, because of different regulatory policies, staff formal educational backgrounds, and
4
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
 D. J. Ackerman
Using ECE Data to Inform Training
the ages of the children served (e.g., infants and toddlers vs. preschoolers), the focus and amount of training needed are
not likely to be one size fits all (Wasik, Mattera, Lloyd, & Boller, 2013). It therefore could be helpful for ECE policy mak-
ers, program administrators, training providers, advocates, and researchers to have accurate information on the training
needs of staff within and across settings, programs, and states. Discussed next are some methodological considerations
when using aggregate data from ECE workforce registries for this purpose.
Issues in Using Aggregate Registry Data to Inform Training-Related Questions
ECE programs are increasingly using data from a variety of sources to satisfy reporting requirements related to licensing,
funding, ongoing monitoring, and accountability as well as part of quality improvement efforts (Jordan & King, 2015;
Kipnis, Stebbins, & Szekely, 2012; Riley-Ayers, Frede, Barnett, & Brenneman, 2011; The Early Childhood Data Collabora-
tive, 2014). These data also can be useful for performing needs assessments (e.g., Office of Child Care, 2013a), including
the training needed by the child care workforce to support state and federal regulations and quality initiatives. Secondary
analysis of aggregate data from one or a combination of ECE workforce registries has the potential to serve as another
source of data and thus inform the efforts of a wide array of stakeholders. However, at least three registry-specific issues
can undermine this potential: the data variables used, the years of data available, and the percentage of eligible individuals
enrolled.
Training Data Variables Used
The first issue to consider prior to potentially relying on aggregate registry data is the focus of the available data and, if
using multiple registries, the extent to which the variables used are standardized or at least comparable in terms of their
focus (Friese, King, & Tout, 2013; PDW Center, 2013). For example, some stakeholders may be interested in participation
in exact training titles as a means for documenting the number and percentage of individuals meeting specific licensing
requirements (e.g., the process for reporting potential child abuse). However, other stakeholders may wish to examine the
extent to which participants engage in training related to an overall content area (e.g., health and safety).
To address this latter issue, The National Registry Alliance (TNRA, 2013), a voluntary organization of ECE workforce
registry leaders, urges registries to use seven primary core knowledge/core content area categories to describe a training’
s
main focus. These categories are Administration and Management; Child Growth and Development; Early Childhood
Education Profession and Policy; Family and Community Relationships; Health, Safety, and Nutrition; Observing, Doc-
umenting, and Assessing; and Teaching and Learning.
The saliency of these topics within the larger ECE field is illustrated when considering the competencies to be displayed
by candidates for the CDA credential (Council for Professional Recognition, 2015) and the standards for early childhood
professional preparation program curriculum advocated by the National Association for the Education of Young Chil-
dren (NAEYC, 2009). As mentioned earlier, the CDA is the minimum qualification to be hired as a child care teacher
in licensed centers in three states and a voluntary milestone step in the majority of states’child care career ladders or
lattices. CDA candidates also must complete training across eight topics. NAEYC is considered to be the largest profes-
sional association for individuals working in settings for children ages birth through age 8 years in the United States and
also an important author of early childhood position statements, particularly related to high-quality ECE. And although
each organization has a different target audience, the CDA Competencies and NAEYC’
s professional preparation program
curriculum standards are acknowledged as comparable (NAEYC & Council for Professional Recognition, 2012).
To help demonstrate the relevancy of TNRA’
s category suggestions, Table 1 displays this organization’
s categories in
alphabetical order as well as their corresponding CDA Competencies and NAEYC standards. As can be seen, the titles
across the three organizations are not standardized. However, for the most part, they are comparable in terms of their
phrasing and focus. For example, the TNRA category of Child Growth and Development is comparable to the CDA
competency of Understanding Principles of Child Development and Learning and the NAEYC standard of Promoting
Child Development and Learning. In addition, the TNRA category of Family and Community Relationships is similar to
the CDA Competency of Building Productive Relationships with Families and NAEYC’
s Building Family and Community
Relationships standard.
Two exceptions to this comparability are seen in the TNRA categories of Administration and Management and Health,
Safety, and Nutrition. More specifically, both categories have a corresponding CDA competency, but neither of these topics
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
5
 D. J. Ackerman
Using ECE Data to Inform Training
Table 1 Comparability of The National Registry Alliance, Child Development Associate, and National Association for the Education
of Young Children Core Knowledge/Competency Standards Titles
TNRA Core Knowledge/Core Content
Areasa
CDA Competenciesb
NAEYC Standards for Early Childhood
Teacher Preparation Program Curriculumc
Administration and Management
Managing an Effective Program Operation
(No stand-alone equivalent)
Child Growth and Development
Understanding Principles of Child
Development and Learning
Promoting Child Development and
Learning
Early Childhood Profession and Policy
Maintaining a Commitment to
Professionalism
Becoming a Professional
Family and Community Relationships
Building Productive Relationships With
Families
Building Family and Community
Relationships
Health, Safety, and Nutrition
Planning a Safe and Healthy Learning
Environment
(No stand-alone equivalent)
Observing, Documenting, and Assessing
Observing and Recording Children’
s
Behavior
Observing, Documenting, and Assessing to
Support Young Children and Families
Teaching and Learning
Advancing Children’
s Physical and
Intellectual Competence; and
Supporting Children’
s Social and
Emotional Development
Using Content Knowledge to Build
Meaningful Curriculum; and
Using Developmentally Effective
Approaches to Connect With Children
and Families
Note. The three organizations’respective core knowledge/competency standards have been reordered to demonstrate their align-
ment with each other. CDA = Child Development Associate. NAEYC = National Association for the Education of Young Children.
TNRA = The National Registry Alliance.
aAvailable in The National Registry Alliance, Core Data Elements for Early Childhood and School-Age Registries, 2013, Washing-
ton, DC: Author. bRetrieved from http://www.cdacouncil.org/credentials/apply-for-cda/preschool. cRetrieved from https://www.naeyc
.org/files/naeyc/file/positions/ProfPrepStandards09.pdf
is a stand-alone category within the NAEYC standards. In addition, although the focus of the categories related to Teaching
and Learning is similar, the phrasing of the titles across the three organizations is very different.
Years of Data
A second potential challenge in using registry data for secondary analyses is for what length of time aggregate data
related to any variable are available. Such longitudinal data can be particularly useful when new policies or programs
are implemented and stakeholders are interested in measuring change over time (Kemper, Stringfield, & Teddlie, 2003).
For example, state administrators or researchers might wish to determine whether the carrot of access to free training
or eligibility for potential scholarships is correlated with an increase over several years in enrollment in a state’
s volun-
tary ECE workforce registry. Aggregate longitudinal data also might shed light on whether programs’participation in state
QRIS initiatives is correlated with greater workforce participation in specific training, such as supporting and/or assessing
children’
s learning.
Percentage of Eligible Individuals Enrolled
A third issue to bear in mind when considering the use of aggregate workforce registry data is the extent to which the data
have the potential to accurately inform specific policy or practice questions about specific populations of ECE teachers
(Mauzy, Tout, & Whitehead, 2014). For example, some training questions may focus on the ECE workforce as a whole,
whereas others may be designed to compare staff working in different auspices (e.g., licensed child care centers, state-
funded pre-K, Head Start) or at varying points in their careers (e.g., teachers at Step 1 or 2 in a state’
s ECE career lattice
vs. those at higher levels). In other cases, the training question may focus only on staff from a specific program or demo-
graphic.
A key ingredient in this potential is the extent to which registry enrollees are representative of the target population
in that jurisdiction (Gliklich, Dreyer, & Leavy, 2014). An example of a target population is infant, toddler, and preschool
6
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
 D. J. Ackerman
Using ECE Data to Inform Training
teachers working in licensed child care centers throughout a state. In an ideal research world, an ECE workforce reg-
istry’
s database will enroll all eligible individuals in the target population(s). One next best option is for the database to
be proportionally representative to the population of interest across the jurisdiction. As an example, suppose a researcher
wishes to compare rates of participation in health and safety training between infant/toddler teachers and preschool teach-
ers working in licensed centers within a single state. In this case, it would be helpful from a sampling perspective if the
proportion of registry enrollees reflected not only the teachers who do—and do not—attend health and safety training
but also the two child age groups. Conversely, if the percentage of teachers enrolled from either training category or age
group is significantly different than is actually the case—and the researcher does not take this difference into account—a
subsequent analysis of the data may result in an inaccurate estimation of each group’
s participation in health and safety
training (e.g., National Survey of Early Care and Education Project Team, 2016).
Assuming a workforce registry is open to everyone in a target population, determining its representativeness first
requires pinning down exactly how many individuals are eligible to enroll at any point in time. However, this can be
challenging due to the presence of multiple ECE auspices, variations in child care regulations regarding which settings
are required to be licensed, and the names of common job roles (e.g., child care worker vs. preschool teacher). Another
contributor is the nature of the child care field, which is dominated by low-waged hourly staff and high rates of turnover.
As a result, more than one individual may fill an otherwise full-time position within a single year (IOM & NRC, 2012;
Kipnis & Whitebook, 2011, 2012; National Survey of Early Care and Education Project Team, 2013; Whitebook, Phillips,
& Howes, 2014).
Even if the approximate number of eligible individuals can be determined, a database’
s representativeness typically
is conditional on there not being any reason to suspect that any subgroup of interest was less likely to enroll (e.g.,
infant/toddler teachers or teachers who attend very few trainings) and/or systematically excluded (Kadam & Bhalerao,
2010). Historically, the percentage of eligible individuals enrolled in states’respective ECE workforce registries has varied
widely (Bellm & Whitebook, 2004; Mayfield, 2012). This is not surprising given that another issue facing voluntary
registries of all types is recruitment and retention (Bishop, Tiro, Sanders, Craddock Lee, & Skinner, 2015).
To the best of my knowledge, no published research examines the percentage of eligible individuals enrolled in all
active ECE workforce registries. However, a review of states’Race to the Top—Early Learning Challenge performance
reports suggests that enrollment policies may serve as a proxy for registries being representative of the workforce in spe-
cific auspices. For example (and as is highlighted later in the report), Oregon mandates practitioners working in regulated
ECE settings to enroll in the state’
s registry. In turn, Oregon’
s 2013 Race to the Top Annual Performance Report (Ore-
gon Department of Education, 2013) noted that, “
as of 2012 … the online registry provides workforce data on 100%
of practitioners in regulated facilities” (p. 6). Georgia anticipates that enrollment in its newly implemented Professional
Development Registry also will be representative of its ECE workforce by the end of 2017 and when all programs are
required to be enrolled in the state’
s QRIS initiative (State of Georgia, 2015). Illinois began requiring all staff in licensed
child care settings to enroll in its registry in 2012, and as a result, membership more than doubled from 32,402 in 2013 to
80,769 in 2014 (State of Illinois, 2015).
Data from other sources also suggest that enrollment policies can shape registry enrollment. Nevada began requiring
the participation of staff working in licensed child care in 2009 and anticipated that full participation would be phased
in by the end of 2012 (The Nevada Registry, 2016). Oklahoma’
s registry reported a marked increase in enrollment after
regulations required the enrollment of staff in QRIS-participating settings (ELC TA, 2015). Wisconsin also requires the
enrollment of staff working in licensed programs (Wisconsin Department of Children and Families, 2009). In fact, based
on Bureau of Labor Statistics data on the size of the ECE workforce in both Oklahoma and Wisconsin, Mayfield (2015)
has estimated that these registries represent close to 100% enrollment. In sum, though not a guarantee, the experiences of
these registries suggest that a jurisdiction’
s requirements for participation are likely to be correlated with the percentage
of eligible individuals enrolled.
In the absence of mandatory enrollment, another way voluntary registries can mitigate recruitment and retention issues
is to reduce the cost of registry participation and increase the related rewards (Gliklich et al., 2014). Previous research has
suggested that ECE workforce registries also have implemented varying initial application or enrollment fees and partic-
ipation incentives (Bellm & Whitebook, 2004; Prentice, 2013). However, I could not identify any research exploring the
extent to which such fees and/or incentives play a role in boosting registry enrollment and, in turn, the representativeness
of any ECE subgroup.
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
7
 D. J. Ackerman
Using ECE Data to Inform Training
In conclusion, there are a variety of potential training-related uses for aggregate workforce registry data. At the
same time, the usefulness of these data may be dependent on a variety of registry-specific issues, including the type
of data recorded and the degree of comparability across registries, how long such data have been collected, and the
extent to which jurisdictions promote participation through their enrollment policies. These are important topics
given the larger policy focus on improving the quality of child care and the capacity of the workforce to support
children’
s learning and development, as well as ongoing state training requirements. Furthermore, because there
has been limited research on the exact data collected across ECE workforce registries, it is difficult to gauge the
potential for aggregate data from one or more registries to inform a variety of child care workforce training-related
questions.
To shed light on these issues, the research questions for this study are as follows:
1.
Which jurisdictions (e.g., states, District of Columbia, regions within single states) currently implement ECE work-
force registries?
2.
Which registries use variables to denote the focus of participants’training?
3.
To what extent do registries use a set of training focus variables that appear to be comparable based on variable title?
4.
In what year did registries begin recording the focus of participants’training?
5.
Across registries, to what extent is enrollment voluntary, incentivized, and/or mandatory for child care teachers in
licensed settings?
6.
What are the implications of the study for potentially using aggregate registry data to inform child care workforce
training-related questions?
The methodology for the study is discussed next.
Study Methodology
Data Collection and Sample
To address these questions, I engaged in three phases of data collection in fall 2015. The first phase consisted of iden-
tifying all potentially active ECE workforce registries in the 50 states and District of Columbia through a review of the
Office of Child Care Technical Assistance Network’
s State Profile information.2 In Phase 2, I identified and reviewed
registry Web sites not only to confirm that the database was active but also to determine if the jurisdiction uses core
knowledge or content categories to describe the main focus of noncredit training aimed at the child care workforce. If
so, these categories were noted as the potential registry noncredit training content variables for that state. This Web site
review also was used to identify which entity is responsible for administering the registry as well as a potential infor-
mant for the third phase of data collection. If no specific contacts were provided, to determine who might serve in that
role, I telephoned the registry’
s administrating entity, e-mailed the general information address, and/or reached out to
colleagues with knowledge about key early childhood stakeholders within that jurisdiction for advice regarding potential
informants.
Phase 3 involved e-mailing a jurisdiction-specific survey to each registry’
s identified informant in November 2015. The
survey questions were as follows:
1.
Does your registry include variables denoting the focus of participants’noncredit training?
2.
If so, do the following categories accurately reflect the variables used to denote the focus of participants’noncredit
training? (The categories were derived from a review of the registry’
s training Web site. Administrators also were
given the option of adding variables and noting any incorrect variables.)
3.
In what year did your registry begin using variables to denote the focus of noncredit training?
4.
To what extent is enrollment in your registry voluntary, incentivized, or mandated?
In the majority of cases, I followed up the survey invitations, as well as responses, with additional e-mails and/or phone
conversations in December 2015 and January 2016. Informants from 41 of the 44 identified registries participated in the
survey for a response rate of 93%. Because I could not confirm information for the three nonresponding registries,3 data
on their respective training focus variables, year in which such data were first recorded, or enrollment policies and/or
incentives are not included here.
8
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
 D. J. Ackerman
Using ECE Data to Inform Training
Data Analysis
To facilitate data analysis, I entered all of the study data into an Excel database. The analysis regarding which jurisdictions
currently implement ECE workforce registries (Research Question 1), use variables to denote the focus of participants’
training (Research Question 2), and in what year registries began recording the focus of participants’training (Research
Question 4) involved the generation of simple descriptive statistics via a count of the number of registries with responses
relevant to each of these questions.
To determine to what extent registries use a set of comparable variable titles to denote the focus of participants’training
(Research Question 3), I initially coded each registry’
s variables according to the seven categories suggested by TNRA
(2013) and noted earlier in Table 1. In analyzing these data, it became apparent that four additional categories were
required: Interactions, Guidance, and Social/Emotional Development; Diversity; Special Needs; and Other. In addition,
Teaching and Learning was expanded to reflect the CDA credential’
s and NAEYC’
s use of the words curriculum, learning
environment, and intellectual development to describe this knowledge area.
To illustrate this coding process, Table 2 displays the titles of Arizona’
s eight training focus variables in alphabetical
order as well as their corresponding TNRA categories. As can be seen, for seven of the eight Arizona variables, the coding
process was very straightforward, as the variable titles are either identical or very similar to TNRA’
s categories (e.g., Family
and Community Partnerships vs. Family and Community Relationships). The one instance in which the phrasing was not
similar was for the Arizona variable Curriculum and Learning Environment, which was coded as Teaching and Learning.
Table 2 Research Question 2 Coding Example
Arizona registry variable
Corresponding TNRA category
Child Growth and Development
Child Growth and Development
Child Observation and Assessment
Observing, Documenting, and Assessing Children
Curriculum and Learning Environment
Teaching and Learning
Effective Interactions
Interactions, Guidance, Social/Emotional Development
Family and Community Partnerships
Family and Community Relationships
Health, Safety, and Nutrition
Health, Safety, and Nutrition
Professionalism
Early Childhood Profession and Policy
Program Management
Administration and Management
Note. TNRA = the National Registry Alliance.
To determine to what extent enrollment is voluntary, incentivized, and/or mandatory (Research Question 5), data on
states’registry enrollment policies were coded as voluntary (and defined as individuals having the option to enroll in
the registry and with no incentive beyond tracking their training), conditional/incentivized (voluntary, but enrollment is
required to be eligible for or to access a financial or training incentive), or mandatory (required as part of employment
within a specific ECE setting). Finally, to determine the implications of the study for potentially using aggregate registry
data to inform child care training questions (Research Question 6), I juxtaposed two hypothetical research questions with
the results of the study’
s first five research questions. The results of all of these analyses are discussed next.
Results
States With Early Care and Education Workforce Registries
The study’
s first research question focused on which jurisdictions were implementing ECE workforce registries at the time
of the study. On the basis of a review of the Office of Child Care Technical Assistance Network’
s State Profile information,
registry Web sites, and/or registry administrator participation in the study’
s survey, there were 44 registries in some stage
of implementation in 42 states and the District of Columbia (see Figure 1).
Among these jurisdictions, Rhode Island and Michigan each had single, under-development registries, and Virginia’
s
sole registry was being piloted. In addition, I classified two states as having regional registries: California, which had
a single registry but with enrollment limited to individuals in specific regions, and Florida, which had two separately
administered, regional registries based in Palm Beach and Miami-Dade counties. There also were 38 jurisdiction-wide,
in-operation registries. These registries were located in the District of Columbia as well as the states of Alaska, Arizona,
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
9
 D. J. Ackerman
Using ECE Data to Inform Training
Figure 1 Jurisdictions with early care and education workforce registries.
Arkansas, Colorado, Connecticut, Georgia, Hawaii, Iowa, Idaho, Illinois, Kentucky, Louisiana, Maine, Massachusetts,
Maryland, Minnesota, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New York, North Dakota,
Ohio, Oklahoma, Oregon, Pennsylvania, South Carolina, Tennessee, Texas, Utah, Vermont, Washington, West Virginia,
Wisconsin, and Wyoming.
As can also be seen in Figure 1, eight states did not have an ECE workforce registry at the time of the study. These states
were Alabama, Delaware, Indiana, Kansas, Mississippi, New Mexico, North Carolina, and South Dakota. Among this
group, Alabama, Delaware, North Carolina, and South Dakota appear to have previously implemented registries (Bellm
& Whitebook, 2004; Kipnis & Whitebook, 2011; Southern Early Childhood Alliance, n.d.). However, I did not examine
why these registries were no longer operational as part of the study.
Registries Using Variables to Denote the Focus of Enrollees’ Training
Research Question 2 focused on which ECE workforce registries use variables to denote the content of training completed
by enrollees. This query was addressed through data from the study’
s 41 survey responses. In addition, training was defined
as being a noncredit class, course, or workshop that did not result in credits being awarded by a degree-granting institution.
Thirty-eight of the 41 registries participating in the study reported the use of variables to describe the focus of completed
training. These include 33 statewide registries; the registries in the District of Columbia, Palm Beach County (Florida),
and California; and the under-development databases in Rhode Island and Virginia. The single statewide registry that did
not have this capacity was Nebraska’
s, which was launched in 2014. This also was the case for the Miami-Dade County
regional registry in Florida and the registry under development in Michigan.
Across the 38 registries reporting use of variables to denote the focus of training, the quantity of variables ranged
from 6 to 20. However, 29 registries reported the use of 7 to 10 variables. Among the six registries using more than 10
individual variables, each registry separated out related variables which other registries combined. For example, instead
of having a single Health, Safety, and Nutrition variable, one registry reported four separate variables related to this topic.
Another registry separated out the single variable coded as Teaching and Learning into the two variables of Curriculum
and Learning Environments.
Comparability in Variable Titles
Research Question 3 focused on the extent to which registries’training variables are comparable and also was addressed
through analysis of survey data from the 38 registries with such variables. As mentioned earlier, the predetermined codes
10
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
 D. J. Ackerman
Using ECE Data to Inform Training
used to analyze these data were based on TNRA’
s (2013) suggested categories and included Administration and Manage-
ment; Child Growth and Development; Early Childhood Profession and Policy; Family and Community Relationships;
Health, Safety, and Nutrition; Observing, Documenting, and Assessing; and Teaching and Learning. I also added codes
for Interactions, Guidance, and Social/Emotional Development; Diversity; Special Needs; and Other.
As was the case with Arizona’
s registry and as highlighted in Table 2, the titles of most of the registries’training focus
variables were similar to TNRA’
s categories. For example, registry titles related to the category of Child Growth and
Development included Child Development; Child Development and Learning; and Child Growth, Development, and
Learning. Similarly, for the category of Family and Community Relationships, registry variable titles included Family and
Community Partnerships; Positive and Productive Relationships With Families; and Families, Schools, and Community
Collaboration.
However, the phrasing of the variable titles related to the topic of Teaching and Learning were not as similar. Instead,
these variables included the words curriculum, learning environment, and children’
s intellectual development and likely
reflect the phrasing used in the CDA Competencies (Council for Professional Recognition, 2015) or NAEYC’
s (2009)
Standards for Professional Preparation. As mentioned earlier, it was for this reason that the initial Teaching and Learning
code was expanded to include these specific words.
It also was interesting to note that within single jurisdictions, the exact phrasing used for their registry’
s variables
was not always exclusively aligned with the categories used by TNRA, the CDA, or NAEYC. For example, Pennsylvania’
s
Core Knowledge Competencies are reported as aligned with the NAEYC Standards for Professional Preparation Programs
(Pennsylvania Office of Child Development and Early Learning, 2013). Yet, the state’
s registry variable titles (Assessment;
Child Growth and Development; Communication; Curriculum and Learning Experiences; Families, Schools, and Com-
munity Collaboration; Health, Safety, and Nutrition; Professionalism and Leadership; and Program Organization and
Administration) are an amalgam of all three organizations’phrasing.
Comparability in the Set of Variables Used Across Registries
As mentioned earlier, though 38 registries reported using training focus variables, the number of variables used ranged
from 6 to 20. I therefore undertook further analysis of the study’
s data to determine to what extent registries contained a
comparable set of variables. As is displayed in Table 3, all 38 registries reporting the use of variables to denote the focus
of training also had variables related to Child Growth and Development; Health, Safety, and Nutrition; and Teaching and
Learning. Furthermore, nearly every registry included variables related to Early Childhood Education Profession and
Policy (N = 37); Family and Community Relationships (N = 37); and Observing, Documenting, and Assessing Children
(N = 35).
The variables least likely to be included across all 38 registries were Diversity (N = 12) and Special Needs (N = 9). At
first glance, it might appear that little training on these topics is completed within the jurisdictions that do not include
these variables. However, my study-related correspondence with their respective registry administrators suggests this is
not the case. Instead, these topics often are incorporated into training tagged with one of the other variable categories.
Year in Which Registries Began Recording Training Content
Research Question 4 focused on the year registries began recording the focus of participants’noncredit training as a
means for approximating the extent to which registries have longitudinal data related to this topic. This question also
was addressed through analysis of the survey data from the 38 states that reported using variables to record the focus of
participants’training.
As can be seen in Table 4, the dates in which registries first used such variables ranged from 1991 to 2016. In some states,
these dates also reflect the year in which states began implementing their respective ECE workforce registries (Bellm &
Whitebook, 2004). In other states, such variables were added at a subsequent date.
For example, Oklahoma and Missouri began implementing statewide registries in 1999 and 2000, respectively (Bellm
& Whitebook, 2004), but reported that training variables were incorporated in 2012. Anecdotal information gathered
during data collection suggests that these additions were sometimes due to the implementation of new iterations of the
registry database and/or database administration. Additional research suggests other contributors include states’Race to
the Top—Early Learning Challenge awards and its focus on A Great Early Childhood Education Workforce (Kipnis et al.,
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
11
 D. J. Ackerman
Using ECE Data to Inform Training
Table 3 Registry Variables Used to Denote the Focus of Noncredit Trainings
Registry
Administration
and
Management
Child
Growth and
Development
Diversity
Early Childhood
Education
Profession
and Policy
Family and
Community
Relationships
Health,
Safety, and
Nutrition
Interactions,
Guidance, and
Social/Emotional
Development
Observing,
Documenting,
and Assessing
Children
Special
Needs
Teaching
and
Learning
Other(s)
N
30
38
12
37
37
38
32
35
9
38
5
Alaska
X
X
X
X
X
X
X
X
X
Arizona
X
X
X
X
X
X
X
X
Arkansas
X
X
X
X
X
X
X
X
California
X
X
X
X
X
X
X
X
X
X
X
Colorado
X
X
X
X
X
X
X
X
District of Columbia
X
X
X
X
X
X
X
X
X
X
Palm Beach County FL
X
X
X
X
X
X
X
X
Georgia
X
X
X
X
X
X
X
Hawaii
X
X
X
X
X
X
X
X
X
Iowa
X
X
X
X
X
X
X
X
Idaho
X
X
X
X
X
X
X
X
X
X
Illinois
X
X
X
X
X
X
X
Kentucky
X
X
X
X
X
X
X
Maine
X
X
X
X
X
X
X
X
Maryland
X
X
X
X
X
X
Massachusetts
X
X
X
X
X
X
X
X
X
Minnesota
X
X
X
X
X
X
X
X
Missouri
X
X
X
X
X
X
X
X
Montana
X
X
X
X
X
X
X
X
X
X
Nevada
X
X
X
X
X
X
X
X
New Hampshire
X
X
X
X
X
X
X
X
X
X
New Jersey
X
X
X
X
X
X
X
X
X
New York
X
X
X
X
X
X
X
North Dakota
X
X
X
X
X
X
X
X
Ohio
X
X
X
X
X
X
Oklahoma
X
X
X
X
X
X
X
X
Oregon
X
X
X
X
X
X
X
X
X
X
Pennsylvania
X
X
X
X
X
X
X
X
Rhode Island
X
X
X
X
X
X
South Carolina
X
X
X
X
X
X
X
Tennessee
X
X
X
X
X
X
X
Texas
X
X
X
X
X
X
X
X
Utah
X
X
X
X
X
X
X
X
X
Virginia
X
X
X
X
X
X
X
X
Washington
X
X
X
X
X
X
X
X
West Virginia
X
X
X
X
X
X
X
Wisconsin
X
X
X
X
X
X
X
X
X
X
X
Wyoming
X
X
X
X
X
X
X
Note. N = 38.
12
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
 D. J. Ackerman
Using ECE Data to Inform Training
Table 4 Year Early Care and Education Workforce Registries Began Recording Focus of Training
Year
Registries
1991
Wisconsin
1993
Tennessee
1997
Oregon
1998
Montana
2000
Utah
2001
Arkansas, Hawaii, Maryland
2002
Maine, Pennsylvania, South Carolina
2003
Idaho, New Jersey, Texas
2004
Nevada
2005
West Virginia
2006
Alaska, Ohio
2007
Kentucky, Minnesota
2008
Palm Beach County, Florida
2009
Iowa, Illinois
2010
Massachusetts
2011
Washington
2012
District of Columbia, Missouri, New York, North Dakota, Oklahoma
2013
New Hampshire
2014
(No jurisdictions)
2015
Arizona, Colorado, Virginia
2016
California, Georgia, Rhode Island
Unknown
Wyoming
Note. N = 38.
2012; McDonald, 2013; PDW Center, 2013), as well as the more recent CCDF requirement regarding the submission of
states’annual Quality Performance Reports (Office of Child Care, 2013b).
Voluntary Versus Mandatory Participation
The study’
s fifth research question examined whether enrollment in registries is voluntary versus mandatory for child
care teachers working in licensed settings. The data for this research question were derived from participants’survey
responses as well as a review of registry Web sites. In addition, I coded these responses as voluntary (defined as individuals
having the option to enroll in the registry and with no incentive beyond tracking their training), conditional/incentivized
(voluntary, but enrollment is required to be eligible for or to attain access to an incentive), or mandatory (required as part
of employment within a specific ECE setting).
As can be seen in Table 5, policies regarding registry enrollment span all three of these categories, but 23 registries
have some type of mandatory policy. More specifically, no jurisdiction mandates all child care staff to enroll in its ECE
workforce registry. However, Oklahoma essentially requires this by mandating all staff in any child care setting to enroll in
the state’
s Professional Development Ladder, which is only accessible via the state’
s registry. In 22 additional jurisdictions,
enrollment is mandatory if child care staff work in a specific program/setting. Examples of settings include all licensed
ECE programs (N = 10) and programs that participate in a state’
s QRIS (N = 10). In addition, while enrollment may be
mandatory for some or all child care staff, some of these jurisdictions also offer potential incentives, including eligibility
for CDA fees and scholarships.
As also can be seen in Table 5, registry enrollment is voluntary in the remaining 15 jurisdictions. However, in eight
states, enrollment is necessary to gain access to and/or to be eligible for a variety of incentives. Five of these eight states
offer financial incentives, including partial or full payment of the CDA application or renewal fee, college scholarships,
and one-time bonuses for achieving certain levels within a state’
s career ladder or lattice. Two of the states in this group
offer access to training that is not otherwise available to the general public. Enrollment in Tennessee’
s registry offers access
to both financial and training incentives.
In the final group of seven jurisdictions, registry enrollment also is voluntary, but aside from keeping track of an individ-
ual’
s data, there do not appear to be any extra incentives to enroll. I did not focus on why this might be the case. However,
at the time of the study, Rhode Island’
s registry was under development and Virginia’
s registry was being piloted, with
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
13
 D. J. Ackerman
Using ECE Data to Inform Training
Table 5 Registry Enrollment Requirements for Licensed Child Care Staff
Enrollment requirement
Jurisdictions
Mandatory/setting dependenta
Staff in licensed ECE programs
Arkansas, Hawaii, Illinois, Kentucky, Nevada, Oklahoma,b
Oregon, South Carolina, Washington, Wisconsin, Wyoming
Staff in QRIS programs
Arizona, Colorado, Palm Beach County, Florida, Georgia,
Maine, Minnesota, Montana, New Jersey, Ohio, Pennsylvania
Staff in licensed ECE programs and nonlicensed QRIS programs
Massachusetts
Staff in city- or state-contracted programs
California
Conditional/incentivizedc
Financial incentive
Alaska, Idaho, Maryland, North Dakota, Texas
Training incentive
Iowa, Utah
Training and financial incentive
Tennessee
Voluntary/no extra incentivesd
District of Columbia, Missouri, New Hampshire, New York,
Rhode Island,e Virginia,f West Virginia
Note. N = 38. Includes the registries that participated in the study and use variables to denote the focus of training. ECE = early care
and education. QRIS = Quality Rating and Improvement System.
an = 23. bOklahoma requires all child care staff to enroll in the state’
s Professional Development Ladder, which is accessible only through
the state’
s registry. cn = 8. dn = 7. eEnrollment is required for state’
s publicly funded pre-K teachers only. fEnrollment is required for
state’
s publicly funded Virginia Preschool Initiative pre-K teachers only.
both states planning to initially require teachers working in their respective state-funded pre-K programs to enroll. Also,
the registries in the District of Columbia, New Hampshire, and New York were in operation for just a few years. Finally,
although Missouri’
s registry had been operational for roughly 15 years (Bellm & Whitebook, 2004), the state cannot
implement a QRIS without legislative action. And West Virginia was planning a QRIS (Quality Rating and Improvement
System National Learning Network, 2016). So, while this is conjecture only, the voluntary/no incentive status of some of
these registries may reflect their respective internal capacities and/or lack of an active role in the larger child care quality
improvement process (i.e., QRIS) at the time of the study.
Interestingly, a review of registry Web sites suggested that no jurisdictions were charging an enrollment fee at the
time of the study. Oregon mandated all staff in licensed programs to enroll and also had an enrollment fee of $10, but
only if an individual met the qualifications for Steps 3–12 on the state’
s career ladder, and this fee was “waived until
further notice … to support the Race to the Top Early Learning Challenge Grant project goals” (Oregon Center for
Career Development, 2016). Based on earlier research conducted by Bellm and Whitebook (2004) and Prentice (2013),
the current no-fee approach appears to represent a turnaround in registry policies. However, the study’
s survey did not
include a question about past and current fees, and thus this information was not verified.
Implications of the Study for the Potential Usefulness of Aggregate Registry Data
The study’
s final research question focused on the implications of these findings for potentially using aggregate work-
force registry data to inform child care training-related questions. To address this issue, I juxtaposed two hypothetical
research questions with the results of Research Questions 1–5 as a means for illustrating how researchers and other ECE
stakeholders might determine which aggregate registries’data sets could serve as a source of useful data.
More specifically, assume a researcher is interested in conducting secondary analyses of aggregate registry data to
compare hours of training focused on Child Growth and Development and Health, Safety, and Nutrition. In addition, one
study will focus solely on 2015, and a second study will examine trends in participation from 2010 to 2015. Finally, the
focus will be on individuals working in licensed child care settings across the United States.
Recalling the three key issues to consider when contemplating the use of aggregate registry data, based on the results
of the current study, we know 38 registries use variables focusing on training content, and all 38 have variables related to
these two training topics. Accordingly, aggregate data for all of these registries initially can be considered for inclusion.
However, one study focuses on 2015 alone, and the second focuses on 2010–2015 participation rates, and both studies
wish to focus on ECE staff in licensed child care centers. Therefore we also need to consider which data sets to exclude
based on these parameters.
14
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
 D. J. Ackerman
Using ECE Data to Inform Training
Table 6 Registries’Initial Use of Training Focus Variables and Current Enrollment Policies
Enrollment policies
Initial year
using training
focus variables
Registry
Voluntary/no
extra incentive
Conditional/
incentivized
Mandatory
QRIS
densitya (%)
1991
Wisconsin
Licensed ECE program staff
1993
Tennessee
X
1997
Oregon
Licensed ECE program staff
1998
Montana
Staff in QRIS programs
21
2000
Utah
X
2001
Arkansas
Licensed ECE program staff
Hawaii
Licensed ECE program staff
Maryland
X
2002
Maine
Staff in QRIS programs
41
Pennsylvania
Staff in QRIS programs
64
South Carolina
Licensed ECE program staff
2003
Idaho
X
New Jersey
Staff in QRIS programs
Not reported
Texas
X
2004
Nevada
Licensed ECE program staff
2005
West Virginia
X
2006
Alaska
X
Ohio
Staff in QRIS programs
32
2007
Kentucky
Licensed ECE program staff
Minnesota
Staff in QRIS programs
36
2008
Palm Beach County,
Florida
Staff in QRIS programs
27
2009
Iowa
X
Illinois
Licensed ECE program staff
2010
Massachusetts
Licensed ECE and
nonlicensed QRIS program
staff
2011
Washington
Licensed ECE program staff
2012
District of Columbia
X
Missouri
X
New York
X
North Dakota
X
Oklahoma
Staff in all child care
programs
2013
New Hampshire
X
2015
Arizona
Staff in QRIS programs
19
Colorado
Staff in QRIS programs
52
Virginia
X
2016
Georgia
Staff in QRIS programs
50
Rhode Island
X
California
Staff in city-contracted or
state-funded programs
Note. N = 37. Wyoming is not included due to the survey respondent not knowing the year in which the state’
s registry began including
training content variables. ECE = early care and education. QRIS = Quality Rating and Improvement System.
aDerived from data generated through the QRIS Compendium’
s Create a Report feature at http://qriscompendium.org/create-a-report
To help explore the ramifications of these study design conditions, Table 6 combines the year in which registries began
including variables to denote the focus of training (and previously displayed in Table 4) and their respective current
enrollment policies (and previously displayed in Table 5). The registries also are listed chronologically according to the
year in which training focus variables were first used.
For the 2015 study, Table 6 suggests that registry data from Arizona, Colorado, Virginia, Georgia, Rhode Island, and
California likely will not be useful, as each reported that training variables were first used sometime in 2015 or 2016.
There may be additional reasons why data from any of these six registries are not suitable (e.g., being in the pilot phase or
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
15
 D. J. Ackerman
Using ECE Data to Inform Training
regional), but given the focus on 2015, they can be excluded on the basis of date alone. Similarly, for the 2010–2015 study,
the researcher also would exclude from consideration the registry data sets from Massachusetts, Washington, the District
of Columbia, Missouri, New York, North Dakota, Oklahoma, and New Hampshire, as each did not begin recording the
focus of training until 2010 or later.
Once the years-of-data issue is taken into account, the next issue to consider is the extent to which a registry enrolls
a representative sample of the child care workforce working in licensed settings. As mentioned previously, to the best
of my knowledge, no published research has examined this specific issue across registries. However, based on Oregon’
s
experience, regulations regarding the mandatory enrollment of staff in licensed child care settings might serve as a suitable
proxy. Therefore the best candidates for the 2015 study (in alphabetical order) appear to be the aggregate registry data sets
from the 11 states of Arkansas, Hawaii, Illinois, Kentucky, Massachusetts, Nevada, Oklahoma, Oregon, South Carolina,
Washington, and Wisconsin. However, because of the years-of-data issue, this group of 11 registries would need to be
whittled down to eight registries for the study focusing on 2010–2015 training.
For either study, the researcher also may consider using aggregate data from the year-relevant registries mandating the
enrollment of staff in programs participating in their respective jurisdiction’
s QRIS initiatives. However, based on data
from the QRIS Compendium’
s (2016) Create a Report tool, although a few QRIS initiatives have enrolled 100% of their
respective licensed ECE centers, none of these is located in the nine states that mandate staff in QRIS programs to enroll
in their respective workforce registries. Instead, the density rates of participation in these states’QRIS range from 19% to
64% (see Table 6).
Summary
In summary, in this study, I determined that 44 ECE workforce registries were in some stage of operation in 43 juris-
dictions. In 37 states and the District of Columbia, a single, jurisdiction-wide registry was up and running. In addition,
California limited participation in its single registry based on region, and Florida had two separately administered regional
registries. Finally, three states were piloting or developing their registries.
Furthermore, among the 41 registries participating in the study, 38 reported the use of variables denoting the focus
of training completed by enrollees. Although these training focus variables are not standardized, the results of the study
suggest that registries use variables that can be considered comparable due to their similar focus. In addition, the majority
of registries have variables related to the topics of Child Growth and Development; Teaching and Learning; Document-
ing and Assessing Children’
s Knowledge, Skills, and Abilities; Early Childhood Education and Profession; Family and
Community Relationships; and Health, Safety, and Nutrition.
At the same time, registries vary widely in the year in which they began including variables to denote the focus of
participants’noncredit training. And although 23 variable-using registries require staff in specific child care settings to
enroll, this is not the case for 15 registries. These findings have implications not only for the potential to use aggregate
registry data to inform an array of child care training questions but also for the additional registry-related research that
needs to be conducted.
Research Implications
In this report, I aimed to address a gap in the literature base on ECE workforce registries by sharing the results of a study of
the training focus variables these databases use and of the extent to which these variables are comparable across states. Also
of interest was when registries began using these variables and whether enrollment in a registry is voluntary, incentivized,
or mandatory. The study is especially salient given the increasing reliance on registries to track the training completed by
the child care workforce as well as the ongoing need for valid data to inform a wide variety of ECE-related programmatic,
policy making, and research. Moreover, highlighting the potential use of aggregate registry data to address a variety of
child care training-related questions is timely given the larger policy emphasis on improving child care quality and the
capacity of the child care workforce to support children’
s development and learning.
Of course, a key limitation of this study is its partial reliance on data from a self-reported survey and a policy context
that is in continual flux. As a result, although every attempt was made to link survey data with online information regarding
training content variables and other relevant policies, the information reported here may not continue to be accurate in
the short or long term. The study also is limited in that only a select group of aggregate registry data issues were examined.
16
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
 D. J. Ackerman
Using ECE Data to Inform Training
Therefore, to further inform the process by which researchers and other ECE stakeholders determine which registries
might serve as useful data sources to address questions related to child care training, several other topics should be inves-
tigated through future research. In addition to the percentage of eligible individuals enrolled in all active ECE workforce
registries, these topics include the comparability of the training related to any focus variable, the policies on and processes
for entering and verifying training data, and the extent to which access to aggregate registry data is available.
Comparability of Training
Although it is helpful to know which registries include specific variables related to the focus of the training completed by
its enrollees, I did not investigate the extent to which the training related to any of these variables is comparable between
different jurisdictions. That is, though the variable titles appear to be aligned, the variable alone does not indicate the types
of training events recorded (e.g., workshop vs. technical assistance); a training’
s objectives; whether it was considered to
be at a beginner, intermediate, or advanced level; and if it was targeted at teachers who work with infants and toddlers
versus preschoolers. It therefore would be helpful for future research to investigate the other training-related variables
used across registries and whether these variables also are comparable across individual ECE workforce registries.
Entering and Verifying Training Data
Another key question that I did not address through the study reported on here is to what extent registry data accurately
reflect the training completed by any individual. More specifically, even if all eligible individuals within a target popu-
lation are enrolled, does their respective registry data reflect all of the training the database was designed to record, or
only some of it? For example, in Douglass, Carter, Smith, and Killins’(2015) study of the extent to which ECE staff train
together, the research team had to merge the state’
s workforce registry identification numbers with professional develop-
ment attendance data because the training of interest was not consistently reported in the registry. Data completeness has
been a challenge in using child welfare administrative data to help inform program development and policy decisions, as
well (Permanency Innovations Initiative Evaluation Team, 2016).
To determine if this is potentially an issue in other ECE workforce registries, it would be helpful for future research
to survey registry administrators regarding who is responsible for adding new training-related data into an individual’
s
registry record (e.g., enrollee vs. trainer), by what means (if any) new information is verified, and whether adding the
information is optional versus mandatory. Having such information will shed further light on which registry data sets are
likely to provide useful information to address specific child care workforce training questions.
Access to Aggregate Early Care and Education Workforce Registry Data
When combined with the results of this current study, future research on these topics has the potential to help a wide array
of ECE stakeholders determine which aggregate registry data sets might inform a training-related question of interest.
However, another important issue to be researched is whether it is possible for a stakeholder to gain access to any relevant
registry’
s aggregate data. This also is a two-pronged issue.
First, a key issue is the capacity of both the software platform a registry uses and its data analysis staff to generate
deidentified, aggregate data related to a defined set of variables (Friese, Tout, & Kirby, 2014; Kreader & Schneider, 2011;
Weber & Iruka, 2014). Again, to the best of my knowledge, no published research is available that examines this capacity
across ECE workforce registries. This study’
s review of ECE workforce registry Web sites showed that registries are admin-
istered by a range of entities, including 2- and 4-year colleges and universities, child care resource and referral agencies,
nonprofit organizations, and state departments, so such an inquiry is particularly relevant.
Second, a review of each registry’
s Web site demonstrated that none of these data sets is in the public domain. This is not
surprising given their main purpose, which is to record and maintain the confidentiality of information about individuals,
including their names, addresses, where they are/have been employed, and how much they earn. In fact, the inclusion of
such personally identifiable information is precisely why some researchers and other stakeholders might seek access to
aggregate data.
However, given the range of entities administering and governing these registries, a related question to be explored
via future research is the extent to which registry administrators are permitted to share aggregate data sets and, if so, the
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
17
 D. J. Ackerman
Using ECE Data to Inform Training
process researchers and other stakeholders need to undergo to request such access (Hotz, Goerge, Balzekas, & Margolin,
1998; The Early Childhood Data Collaborative, 2014). For example, does a researcher merely ask for the dataset via an
e-mail request, or is a formal Freedom of Information Act request required? What other departments or agencies, if any,
must also provide approval prior to releasing aggregate data? In short, knowing that an aggregate data set has the potential
to inform research related to ECE workforce training is of little use if the data ultimately are not available.
Summary
Given the ongoing policy attention paid to improving child care quality and the capacity of the child care workforce, ECE
workforce registries play an important role in documenting the training completed by individuals who play a key role in
supporting the learning and development of our nation’
s young children. This study suggests that registry data also have
the potential to shed light on the focus of the training completed by the workforce both within and across jurisdictions
and, in so doing, inform a wide variety of ECE stakeholders. To build on this study, additional research should focus on the
extent to which registries enroll all eligible members of the child care workforce and on the comparability of the training
related to any focus variable, the policies on and processes for entering and verifying training data, and the capacity of
registries to produce and share aggregate data with researchers and other ECE stakeholders.
Acknowledgments
The author wishes to thank the workforce registry administrators who graciously agreed to participate in this study, as well
as the manuscript’
s external reviewers. The views expressed in this report are those of the author and do not necessarily
reflect the view of the officers and trustees of Educational Testing Service.
Notes
1 Teacher is defined here as the adult who is primarily responsible for the care and/or education of the children in a specific
classroom and counted in that classroom’
s staff-child ratio.
2 https://childcareta.acf.hhs.gov/data#tab-pds-state-profiles
3 The three nonresponding ECE workforce registries were those in Connecticut, Louisiana, and Vermont.
References
Ackerman, D. J., & Kingsley, E. (2015). Variations in the pre-hire education requirements for early care and education teachers. Unpub-
lished manuscript.
Bassok, D., Fitzpatrick, M., Loeb, S., & Paglayan, A. S. (2013). The early childhood care and education workforce in the United States:
Understanding changes from 1990 through 2010. Education Finance and Policy, 8, 581–601.
Bellm, D., & Whitebook, M. (2004). State registries of the early care and education workforce: A review of current models and options for
California. Berkeley, CA: University of California, Berkeley, Center for the Study of Child Care Employment.
Bishop, W. P., Tiro, J. A., Sanders, J. M., Craddock Lee, S. J., & Skinner C. S. (2015). Effectiveness of a community research registry to
recruit minority and underserved adults for health research. Clinical and Translational Science, 8, 82–84.
Boller, K., Tarrant, K., & Schaack, D. D. (2014). Early care and education quality improvement: A typology of intervention approaches
(OPRE Research Report No. 2014–36). Washington, DC: U.S. Department of Health and Human Services, Office of Planning,
Research, and Evaluation.
Burchinal, M. R., Howes, C., Pianta, R., Bryant, D., Early, D., Clifford, R., & Barbarin, O. (2008). Predicting child outcomes at the end
of kindergarten from the quality of pre-kindergarten teacher–child interactions and instruction. Applied Developmental Science, 12,
140–153.
Cameron, C. E. (2012). A transactional model of effective teaching and learning in the early childhood classroom. In R. C. Pianta, W. S.
Barnett, S. M. Sheridan, & L. M. Justice (Eds.), Handbook of early childhood education (pp. 278–296). New York, NY: Guilford Press.
Child Care and Development Block Grant Act, Pub. L. No.113-186, 120 Stat. 1971 (2014).
Child Care Aware. (2012). Minimum education requirements for family child care home providers [Excel spreadsheet]. Retrieved from
http://www.naccrra.org/about-child-care/state-child-care-licensing/training-requirements
Child Care Aware. (2013). We can do better: Child Care Aware of America’
s ranking of state child care center regulations and oversight.
Arlington, VA: Author.
18
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
 D. J. Ackerman
Using ECE Data to Inform Training
Council for Professional Recognition. (2015). Steps to earn your preschool CDA credential. Retrieved from http://www.cdacouncil.org/
credentials/apply-for-cda/preschool
Douglass, A., Carter, A., Smith, F., & Gadhadharan, B. K. (2012). The Massachusetts early education and care professional development
study 2012. Boston, MA: University of Massachusetts.
Douglass, A., Carter, A., Smith, F., & Killins, S. (2015). Training together: State policy and collective participation in early educator
professional development. New England Journal of Public Policy, 27(1), Article 5.
Early Learning Challenge Technical Assistance Program. (2015). Early childhood workforce data: Collection practices and possibilities.
Washington, DC: Author.
Friese, S., King, C., & Tout, K. (2013). INQUIRE data toolkit (OPRE Report No. 2013–58). Washington, DC: U.S. Department of Health
and Human Services, Office of Planning, Research, and Evaluation.
Friese, S., Tout, K., & Kirby, G. (2014). Best practices in ensuring data quality in quality rating and improvement systems (QRIS) (OPRE
Research Brief No. 2014–47). Washington, DC: U.S. Department of Health and Human Services, Office of Planning, Research, and
Evaluation.
Gliklich, R. E., Dreyer, N. A., & Leavy, M. B. (Eds.). (2014). Registries for evaluating patient outcomes: A user’
s guide (3rd ed.). Rockville,
MD: Agency for Healthcare Research and Quality.
Hamre, B. K. (2014). Teachers’daily interactions with children: An essential ingredient in effective early childhood programs. Child
Development Perspectives, 8, 223–230.
Hamre, B. K., Pianta, R. C., Downer, J. T., DeCoster, J., Mashburn, A. J., Jones, S. M., … Hamagami, A. (2013). Teaching through
interactions: Testing a developmental framework of teachers effectiveness in over 4,000 classrooms. The Elementary School Journal,
113, 461–487.
Hotz, V. J., Goerge, R., Balzekas, J., & Margolin, F. (Eds.). (1998). Administrative data for policy-relevant research: Assessment of current
utility and recommendations for development. A report of the Advisory Panel on Research Uses of Administrative Data of the Northwest-
ern University/University of Chicago Joint Center for Poverty Research. Chicago, IL: Northwestern University/University of Chicago
Joint Center for Poverty Research.
Improving Head Start for School Readiness Act, Pub. L. No. 110–134, 121 Stat. 1363 (2007).
Institute of Medicine & National Research Council. (2012). The early childhood care and education workforce: Challenges and opportu-
nities: A workshop report. Washington, DC: National Academies Press.
Jordan, E., & King, C. (2015). Stacking the blocks: A look at integrated data strategies. In H. Dichter (Ed.), Rising to the challenge:
Building effective systems for young children and families, a BUILD e-book (pp. 1–19). Retrieved from http://www.buildinitiative.org/
OurWork/StateandLocal/EarlyLearningChallenge.aspx
Kadam, P., & Bhalerao, S. (2010). Sample size calculation. International Journal of Ayerveda Research, 1(1), 55–57.
Kemper, E. A., Stringfield, S., & Teddlie, C. (2003). Mixed methods sampling strategies in social science research. In A. Tashakkori &
C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (pp. 273–296). Thousand Oaks, CA: Sage.
Kipnis, F., Stebbins, H., & Szekely, A. (2012). Developing coordinated longitudinal early childhood data systems: Trends and opportunities
in Race to the Top Early Learning Challenge applications. Washington, DC: The Early Childhood Data Collaborative.
Kipnis, F., & Whitebook, M. (2011). Workforce information: A critical component of coordinated state early care and education data
systems. Berkeley, CA: University of California, Berkeley, Center for the Study of Child Care Employment.
Kipnis, F., & Whitebook, M. (2012). Mapping current professional preparation and professional development opportunities for New Jer-
sey’
s early learning workforce—Final report. Berkeley, CA: University of California, Berkeley, Center for the Study of Child Care
Employment.
Kreader, J. L., & Schneider, W. J. (2011). Putting the pieces together: New York early learning program data systems. New York, NY:
Columbia University, National Center for Children in Poverty.
Lipscomb, S. T., Schmitt, S. A., & Pratt, M. E. (2015). Professional development scholarships increase qualifications of diverse providers.
Journal of Early Childhood Teacher Education, 36, 232–249.
Matthews, H., Schulman, K., Vogtman, J., Johnson-Staub, C., & Blank, H. (2015). Implementing the Child Care and Development Block
Grant reauthorization: A guide for states. Washington, DC: National Women’
s Law Center and CLASP.
Mauzy, D., Tout, K., & Whitehead, J. (2014). New workforce data [PowerPoint presentation]. Retrieved from http://www.naeyc.org/
files/naeyc/Denise%20Mauzy%20Workforce%20Data%20Panel%20Final.pdf
Mayfield, W. (2012). The National Registry Alliance 2012 workforce dataset: A review of workforce trends. Washington, DC: The National
Registry Alliance.
Mayfield, W. (2015, September). 2015 Dataset findings and implications for policy and practice. Presentation at the National Workforce
Registry Alliance 2015 Conference, Orlando, FL.
McDonald, D. (2013). Workforce initiatives in Race to the Top—Early Learning Challenge program annual performance reports. Wash-
ington, DC: U.S. Department of Health and Human Services, Administration for Children and Families.
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
19
 D. J. Ackerman
Using ECE Data to Inform Training
Missouri Coordinating Board for Early Childhood. (2014). “Career lattice” paper: Early childhood state charts describing steps for
advancement. Jefferson City, MO: Author. Retrieved from http://dss.mo.gov/cbec/pdf/career-lattice-paper.pdf
National Association for the Education of Young Children. (2009). NAEYC standards for early childhood professional preparation pro-
grams: Position statement approved by the NAEYC Governing Board July 2009. Washington, DC: Author.
National Association for the Education of Young Children & Council for Professional Recognition. (2012). Suggested relationship
between the CDA formal child care education requirement and NAEYC standards for professional preparation programs. Washington,
DC: Authors.
National Center on Child Care Professional Development Systems and Workforce Initiatives. (2013). Workforce data planning and
implementation guide. Washington, DC: Author.
National Center on Child Care Quality Improvement. (2011a). Common categories of QRIS quality standards. Fairfax, VA: Author.
National Center on Child Care Quality Improvement. (2011b). Early learning guidelines in QRIS standards. Fairfax, VA: Author.
National Center on Child Care Quality Improvement. (2015). CCDF health and safety requirements fact sheet: Health and safety training.
Fairfax, VA: Author.
National Infant and Toddler Child Care Initiative. (2011). Quality rating and improvement systems (QRIS): Inclusion of infant/toddler
quality indicators. Washington, DC: U.S. Department of Health and Human Services, Office of Child Care.
National Survey of Early Care and Education Project Team. (2012). National Survey of Early Care and Education Workforce file. Retrieved
from http://www.icpsr.umich.edu/icpsrweb/ICPSR/studies/35519
National Survey of Early Care and Education Project Team. (2013). Number and characteristics of early care and education (ECE) teach-
ers and caregivers: Initial findings from the National Survey of Early Care and Education (NSECE) (OPRE Report No. 2013–38).
Washington, DC: U.S. Department of Health and Human Services, Office of Planning, Research, and Evaluation.
National Survey of Early Care and Education Project Team. (2016). Examining child care subsidy receipt: An analysis of matched NSECE
and Illinois administrative data (OPRE Report No. 2016–12). Washington DC: U.S. Department of Health and Human Services,
Office of Planning, Research, and Evaluation.
Office of Child Care. (2013a). A tool to assess the alignment of state/territory PDS and QRIS. Washington, DC: U.S. Department of Health
and Human Services, Administration for Children and Families.
Office of Child Care. (2013b). Working toward a qualified and professional early childhood and school-age workforce. Washington, DC:
U.S. Department of Health and Human Services, Administration for Children and Families.
Office of Child Care. (2015). CCDF plans. Retrieved from http://www.acf.hhs.gov/programs/occ/plans
Office of Head Start. (2015). 2014–2015 Head Start program information report (PIR): Staff qualifications report—2015—National level.
Retrieved from https://hses.ohs.acf.hhs.gov/pir/reports
Oregon Center for Career Development. (2016). Frequently asked questions: Oregon registry. Retrieved from https://www.pdx.edu/occd/
frequently-asked-questions-oregon-registry#question%204
Oregon Department of Education. (2013). Race to the Top Early Learning Challenge annual performance report. Salem, OR: Author.
Pennsylvania Office of Child Development and Early Learning. (2013). Crosswalk between Pennsylvania core knowledge competen-
cies, NAEYC Standards for Professional Preparation Programs, NBPTS early childhood generalist standards, and Charlotte Daniel-
son’
s Framework for Teaching. Retrieved from http://www.pakeys.org/uploadedContent/Docs/Early%20Learning%20Programs/Pre-
K%20Counts/Crosswalk%20NAEYC%20NBPTS%20CKC%20Danielson%203.pdf
Permanency Innovations Initiative Evaluation Team. (2016). Using child welfare administrative data in the Permanency Innovations Ini-
tiative Evaluation (OPRE Report No. 2016–47).Washington, DC: U.S. Department of Health and Human Services, Office of Planning,
Research, and Evaluation
Pianta, R. C. (2011, September). Individualized and effective professional development supports in early care and education settings.
Zero to Three, pp. 4–10.
Prentice, C. (2013). State of registries survey 2012: A survey of the nation’
s early childhood and school-age registries. Washington, DC: The
National Registry Alliance.
QRIS Compendium. (2016). Create a report. Retrieved from http://qriscompendium.org/create-a-report
Quality Rating and Improvement System National Learning Network. (2016). QRIS state contacts and map. Retrieved from http://
qrisnetwork.org/qris-state-contacts-map
Riley-Ayers, S., Frede, E., Barnett, W. S., & Brenneman, K. (2011). Improving early education programs through data-based decision
making. New Brunswick, NJ: National Institute for Early Education Research.
Schilder, D. (2016). Early childhood teacher education policies: Research review and state trends. New Brunswick, NJ: Center on Enhanc-
ing Early Learning Outcomes.
Southern Early Childhood Alliance. (n.d). Professional development systems and the SECA states. Retrieved from http://www
.southernearlychildhood.org/upload/file/Public%20Policy%20Docs/Professional%20Development%20Systems%20and%20the
%20SECA%20States.ppt
State of Georgia. (2015). Race to the Top—Early Learning Challenge 2014 annual performance report. Atlanta, GA: Author.
20
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
 D. J. Ackerman
Using ECE Data to Inform Training
State of Illinois. (2015). Race to the Top—Early Learning Challenge 2014 annual performance report. Springfield, IL: Author.
Sykes, G., & Wilson, S. (2015). How teachers teach: Mapping the terrain of practice. Princeton, NJ: ETS.
The Early Childhood Data Collaborative. (2014). 2013 State of states’early childhood data systems. Washington, DC: Child Trends.
The National Registry Alliance. (2009). State of early childhood and school-age workforce registries. Washington, DC: Author.
The National Registry Alliance. (2013). Core data elements for early childhood and school-age registries. Washington, DC: Author.
The Nevada Registry. (2016). Program statistics. Retrieved from http://www.nevadaregistry.org/about/program-statistics/program-
statistics.html
U.S. Department of Education. (2015). Race to the Top—Early Learning Challenge awards. Retrieved from http://www2.ed.gov/
programs/racetothetop-earlylearningchallenge/awards.html
U.S. Government Accountability Office. (2012). Early child care and education: HHS and Education are taking steps to improve workforce
data and enhance worker quality (Report No. GAO-12–248). Washington, DC: Author.
Wasik, B. A., Mattera, S. K., Lloyd, C. M., & Boller, K. (2013). Intervention dosage in early childhood care and education: It’
s complicated
(OPRE Research Brief No. 2013–15). Washington, DC: U.S. Department of Health and Human Services, Office of Planning, Research,
and Evaluation.
Weber, R. G., & Grobe, D. (2014). Betty Gray early childhood training and certification scholarship program evaluation. Corvallis, OR:
Oregon State University.
Weber, R., & Iruka, I. U. (2014). Best practices in data governance and management for early care and education: Supporting effective
quality rating and improvement systems (OPRE Research Brief No. 2014–35). Washington, DC: U.S. Department of Health and
Human Services, Office of Planning, Research, and Evaluation.
Weber, R. B., & Lipscomb, S. T. (2015). Supporting the professional development of Oregon’
s early learning workforce: A foundation for
kindergarten readiness of Oregon’
s children. Corvallis, OR: Oregon State University.
Whitebook, M., Phillips, D., & Howes, C. (2014). Worthy work, still unlivable wages: The early childhood workforce 25 years after the
National Child Care Staffing Study. Berkeley, CA: University of California, Berkeley, Center for the Study of Child Care Employment.
Wisconsin Department of Children and Families, Division of Early Care and Education. (2009). DCF 251 Licensing rules for group child
care centers. Madison, WI: Author.
Wolfe, R. B. (2015). Trends and innovations in early childhood education workforce development. In H. Dichter (Ed.), Rising to the
challenge: Building effective systems for young children and families, a BUILD e-book (pp. 4-1–4-16). Retrieved from http://www
.buildinitiative.org/OurWork/StateandLocal/EarlyLearningChallenge.aspx
Suggested citation:
Ackerman, D. J. (2016). Using state early care and education workforce registry data to inform training-related questions: Issues to consider
(ETS Research Report No. RR-16-31). Princeton, NJ: Educational Testing Service. http://dx.doi.org/10.1002/ets2.12117
Action Editor: James Carlson
Reviewers: Danielle Guzman-Orth and Sam Rikoon
ETS and the ETS logo areregistered trademarks of Educational Testing Service (ETS). MEASURINGTHE POWER OF LEARNING is a
trademark of ETS. All other trademarks are property of their respective owners.
Find other ETS-published reports by searching the ETS ReSEARCHER database at http://search.ets.org/researcher/
Policy Information Report and ETS Research Report Series No. RR-16-31. © 2016 Educational Testing Service
21
